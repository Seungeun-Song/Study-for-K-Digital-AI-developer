{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dVtiJzby2dln",
        "outputId": "afb42fc7-5d72-4d18-9f47-031410ba4ec5"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9lemUDV2iz1"
      },
      "source": [
        "from keras.datasets import boston_housing\n",
        " \n",
        "(X_train, y_train),(X_test, y_test) = boston_housing.load_data()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPJ2SOTf2zlj",
        "outputId": "62efddaf-376b-45ce-df36-f5d2a01dacfc"
      },
      "source": [
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(404, 13) (404,) (102, 13) (102,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "It3Y5NTQ23me",
        "outputId": "9f948f9a-066a-4a02-d130-d1dafa7912dc"
      },
      "source": [
        "print(X_train[:10],'\\n',y_train[:10])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.23247e+00 0.00000e+00 8.14000e+00 0.00000e+00 5.38000e-01 6.14200e+00\n",
            "  9.17000e+01 3.97690e+00 4.00000e+00 3.07000e+02 2.10000e+01 3.96900e+02\n",
            "  1.87200e+01]\n",
            " [2.17700e-02 8.25000e+01 2.03000e+00 0.00000e+00 4.15000e-01 7.61000e+00\n",
            "  1.57000e+01 6.27000e+00 2.00000e+00 3.48000e+02 1.47000e+01 3.95380e+02\n",
            "  3.11000e+00]\n",
            " [4.89822e+00 0.00000e+00 1.81000e+01 0.00000e+00 6.31000e-01 4.97000e+00\n",
            "  1.00000e+02 1.33250e+00 2.40000e+01 6.66000e+02 2.02000e+01 3.75520e+02\n",
            "  3.26000e+00]\n",
            " [3.96100e-02 0.00000e+00 5.19000e+00 0.00000e+00 5.15000e-01 6.03700e+00\n",
            "  3.45000e+01 5.98530e+00 5.00000e+00 2.24000e+02 2.02000e+01 3.96900e+02\n",
            "  8.01000e+00]\n",
            " [3.69311e+00 0.00000e+00 1.81000e+01 0.00000e+00 7.13000e-01 6.37600e+00\n",
            "  8.84000e+01 2.56710e+00 2.40000e+01 6.66000e+02 2.02000e+01 3.91430e+02\n",
            "  1.46500e+01]\n",
            " [2.83920e-01 0.00000e+00 7.38000e+00 0.00000e+00 4.93000e-01 5.70800e+00\n",
            "  7.43000e+01 4.72110e+00 5.00000e+00 2.87000e+02 1.96000e+01 3.91130e+02\n",
            "  1.17400e+01]\n",
            " [9.18702e+00 0.00000e+00 1.81000e+01 0.00000e+00 7.00000e-01 5.53600e+00\n",
            "  1.00000e+02 1.58040e+00 2.40000e+01 6.66000e+02 2.02000e+01 3.96900e+02\n",
            "  2.36000e+01]\n",
            " [4.09740e+00 0.00000e+00 1.95800e+01 0.00000e+00 8.71000e-01 5.46800e+00\n",
            "  1.00000e+02 1.41180e+00 5.00000e+00 4.03000e+02 1.47000e+01 3.96900e+02\n",
            "  2.64200e+01]\n",
            " [2.15505e+00 0.00000e+00 1.95800e+01 0.00000e+00 8.71000e-01 5.62800e+00\n",
            "  1.00000e+02 1.51660e+00 5.00000e+00 4.03000e+02 1.47000e+01 1.69270e+02\n",
            "  1.66500e+01]\n",
            " [1.62864e+00 0.00000e+00 2.18900e+01 0.00000e+00 6.24000e-01 5.01900e+00\n",
            "  1.00000e+02 1.43940e+00 4.00000e+00 4.37000e+02 2.12000e+01 3.96900e+02\n",
            "  3.44100e+01]] \n",
            " [15.2 42.3 50.  21.1 17.7 18.5 11.3 15.6 15.6 14.4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNmRNSu_2_cd"
      },
      "source": [
        "mean = X_train.mean(axis=0)\n",
        "std = X_train.std(axis=0)\n",
        "\n",
        "X_train = X_train - mean\n",
        "X_train = X_train / std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZfehxvf6VI3"
      },
      "source": [
        "X_test = X_test - mean\n",
        "X_test = X_test / std"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_VGQCLy34PQ",
        "outputId": "06bbf85d-179a-45a7-e950-cc1ae86891f6"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=2045)\n",
        "\n",
        "X_train.shape, X_valid.shape, y_train.shape, y_valid.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((323, 13), (81, 13), (323,), (81,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-Cpuf0c4kRe",
        "outputId": "8f4dd042-d858-4a11-992e-df25923114dc"
      },
      "source": [
        "from keras import models, layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(65, activation = 'relu', input_shape=(13,)))\n",
        "model.add(layers.Dense(35, activation='relu'))\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 65)                910       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 35)                2310      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 36        \n",
            "=================================================================\n",
            "Total params: 3,256\n",
            "Trainable params: 3,256\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l41fjMKl486s",
        "outputId": "45ca6b00-ea97-4181-cd51-711b33222e54"
      },
      "source": [
        "model.compile(loss='mse', optimizer='Adam', metrics=['mae'])\n",
        "Hist = model.fit(X_train, y_train, epochs=500, batch_size=1, validation_data=(X_valid, y_valid))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 9.8016 - mae: 2.1329 - val_loss: 11.0091 - val_mae: 2.2572\n",
            "Epoch 2/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 6.9795 - mae: 1.9965 - val_loss: 11.6466 - val_mae: 2.6577\n",
            "Epoch 3/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 6.3481 - mae: 1.8896 - val_loss: 10.4823 - val_mae: 2.2387\n",
            "Epoch 4/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 7.7837 - mae: 2.0711 - val_loss: 11.8060 - val_mae: 2.3142\n",
            "Epoch 5/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 7.0676 - mae: 1.9875 - val_loss: 10.7700 - val_mae: 2.2978\n",
            "Epoch 6/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 7.4631 - mae: 2.0133 - val_loss: 10.6557 - val_mae: 2.2253\n",
            "Epoch 7/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 6.3195 - mae: 1.8303 - val_loss: 11.4391 - val_mae: 2.2389\n",
            "Epoch 8/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 6.1236 - mae: 1.7646 - val_loss: 10.9536 - val_mae: 2.2665\n",
            "Epoch 9/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 7.6379 - mae: 1.9606 - val_loss: 11.0505 - val_mae: 2.3385\n",
            "Epoch 10/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 6.2875 - mae: 1.7672 - val_loss: 10.1033 - val_mae: 2.1513\n",
            "Epoch 11/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 6.7930 - mae: 1.8439 - val_loss: 9.3577 - val_mae: 2.1574\n",
            "Epoch 12/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 6.6924 - mae: 1.9228 - val_loss: 11.4580 - val_mae: 2.3987\n",
            "Epoch 13/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 7.9737 - mae: 1.9859 - val_loss: 11.8718 - val_mae: 2.2728\n",
            "Epoch 14/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 5.6994 - mae: 1.7325 - val_loss: 9.2816 - val_mae: 2.2271\n",
            "Epoch 15/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 6.0012 - mae: 1.8010 - val_loss: 9.6856 - val_mae: 2.1872\n",
            "Epoch 16/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 5.1583 - mae: 1.7246 - val_loss: 8.6717 - val_mae: 2.2799\n",
            "Epoch 17/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 5.5750 - mae: 1.7308 - val_loss: 9.5278 - val_mae: 2.4478\n",
            "Epoch 18/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 6.1334 - mae: 1.8084 - val_loss: 9.0565 - val_mae: 2.1255\n",
            "Epoch 19/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 4.6888 - mae: 1.6092 - val_loss: 8.8818 - val_mae: 2.1983\n",
            "Epoch 20/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 5.6206 - mae: 1.7552 - val_loss: 9.1328 - val_mae: 2.2167\n",
            "Epoch 21/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 5.8100 - mae: 1.7392 - val_loss: 11.2550 - val_mae: 2.4141\n",
            "Epoch 22/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 6.3696 - mae: 1.8380 - val_loss: 8.7241 - val_mae: 2.1592\n",
            "Epoch 23/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 5.4451 - mae: 1.6815 - val_loss: 8.7855 - val_mae: 2.1600\n",
            "Epoch 24/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 4.9182 - mae: 1.5789 - val_loss: 9.1898 - val_mae: 2.1808\n",
            "Epoch 25/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 5.6330 - mae: 1.7403 - val_loss: 10.4644 - val_mae: 2.2000\n",
            "Epoch 26/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 5.3551 - mae: 1.7557 - val_loss: 8.7199 - val_mae: 2.1627\n",
            "Epoch 27/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 4.2370 - mae: 1.5588 - val_loss: 7.9665 - val_mae: 2.1002\n",
            "Epoch 28/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 4.9445 - mae: 1.6555 - val_loss: 8.8263 - val_mae: 1.9851\n",
            "Epoch 29/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 4.8881 - mae: 1.6514 - val_loss: 8.9341 - val_mae: 2.1659\n",
            "Epoch 30/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 5.4353 - mae: 1.7097 - val_loss: 9.1702 - val_mae: 2.1886\n",
            "Epoch 31/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 5.0538 - mae: 1.6791 - val_loss: 7.7135 - val_mae: 2.0885\n",
            "Epoch 32/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 4.9140 - mae: 1.6659 - val_loss: 8.2062 - val_mae: 2.0646\n",
            "Epoch 33/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 4.8398 - mae: 1.5754 - val_loss: 8.9456 - val_mae: 2.2227\n",
            "Epoch 34/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 5.8301 - mae: 1.7934 - val_loss: 8.6011 - val_mae: 2.1915\n",
            "Epoch 35/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 4.3098 - mae: 1.5053 - val_loss: 7.8408 - val_mae: 2.0229\n",
            "Epoch 36/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 4.5899 - mae: 1.6753 - val_loss: 7.8694 - val_mae: 2.0001\n",
            "Epoch 37/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 4.9724 - mae: 1.6462 - val_loss: 7.9498 - val_mae: 2.0724\n",
            "Epoch 38/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 4.0405 - mae: 1.5052 - val_loss: 8.1001 - val_mae: 2.0403\n",
            "Epoch 39/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 5.3530 - mae: 1.5690 - val_loss: 7.9036 - val_mae: 1.9779\n",
            "Epoch 40/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 5.6298 - mae: 1.7656 - val_loss: 9.8733 - val_mae: 2.2343\n",
            "Epoch 41/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 4.7086 - mae: 1.6484 - val_loss: 7.9716 - val_mae: 1.9924\n",
            "Epoch 42/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 4.5577 - mae: 1.5903 - val_loss: 10.9725 - val_mae: 2.3013\n",
            "Epoch 43/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 4.7967 - mae: 1.5686 - val_loss: 8.6431 - val_mae: 2.1085\n",
            "Epoch 44/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 4.5762 - mae: 1.5648 - val_loss: 8.6887 - val_mae: 2.1211\n",
            "Epoch 45/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 3.7961 - mae: 1.5395 - val_loss: 9.4758 - val_mae: 2.2884\n",
            "Epoch 46/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 4.8083 - mae: 1.6595 - val_loss: 8.2665 - val_mae: 2.0338\n",
            "Epoch 47/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 3.3738 - mae: 1.3479 - val_loss: 8.0877 - val_mae: 2.1116\n",
            "Epoch 48/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 4.7335 - mae: 1.5494 - val_loss: 8.5551 - val_mae: 2.0139\n",
            "Epoch 49/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 3.8461 - mae: 1.3995 - val_loss: 9.6250 - val_mae: 2.1615\n",
            "Epoch 50/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 3.9484 - mae: 1.4294 - val_loss: 8.0147 - val_mae: 2.0181\n",
            "Epoch 51/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 3.5092 - mae: 1.4030 - val_loss: 7.8381 - val_mae: 1.9943\n",
            "Epoch 52/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 3.6257 - mae: 1.4531 - val_loss: 9.4541 - val_mae: 2.0181\n",
            "Epoch 53/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 3.9571 - mae: 1.4863 - val_loss: 8.7192 - val_mae: 2.1351\n",
            "Epoch 54/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 3.9212 - mae: 1.4266 - val_loss: 8.4050 - val_mae: 2.1443\n",
            "Epoch 55/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 4.5400 - mae: 1.5313 - val_loss: 8.8363 - val_mae: 2.0365\n",
            "Epoch 56/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 4.4764 - mae: 1.6100 - val_loss: 7.8619 - val_mae: 1.9895\n",
            "Epoch 57/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 3.9063 - mae: 1.5150 - val_loss: 8.7365 - val_mae: 2.1031\n",
            "Epoch 58/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 4.0673 - mae: 1.4498 - val_loss: 7.9027 - val_mae: 2.0474\n",
            "Epoch 59/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 3.4557 - mae: 1.2919 - val_loss: 8.5859 - val_mae: 2.1008\n",
            "Epoch 60/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 4.3598 - mae: 1.5425 - val_loss: 8.6605 - val_mae: 2.0911\n",
            "Epoch 61/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 4.3569 - mae: 1.4674 - val_loss: 8.5192 - val_mae: 2.0177\n",
            "Epoch 62/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 2.7144 - mae: 1.2174 - val_loss: 8.1829 - val_mae: 2.1150\n",
            "Epoch 63/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 3.6269 - mae: 1.3616 - val_loss: 8.6575 - val_mae: 2.0898\n",
            "Epoch 64/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 3.7407 - mae: 1.4220 - val_loss: 7.8047 - val_mae: 1.9566\n",
            "Epoch 65/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 3.6499 - mae: 1.3731 - val_loss: 8.0560 - val_mae: 2.0352\n",
            "Epoch 66/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 3.1045 - mae: 1.2943 - val_loss: 8.7657 - val_mae: 2.0649\n",
            "Epoch 67/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 3.0074 - mae: 1.3558 - val_loss: 9.0065 - val_mae: 2.2323\n",
            "Epoch 68/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 2.8805 - mae: 1.2581 - val_loss: 8.4197 - val_mae: 2.0492\n",
            "Epoch 69/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 3.1945 - mae: 1.3670 - val_loss: 8.7156 - val_mae: 2.1897\n",
            "Epoch 70/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 3.3478 - mae: 1.3805 - val_loss: 7.3182 - val_mae: 2.0398\n",
            "Epoch 71/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 3.5757 - mae: 1.4387 - val_loss: 6.8229 - val_mae: 1.9559\n",
            "Epoch 72/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 3.0857 - mae: 1.3272 - val_loss: 8.1102 - val_mae: 2.1158\n",
            "Epoch 73/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 3.3834 - mae: 1.3989 - val_loss: 8.2720 - val_mae: 2.1206\n",
            "Epoch 74/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 3.2430 - mae: 1.4287 - val_loss: 7.8243 - val_mae: 2.0623\n",
            "Epoch 75/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 3.2745 - mae: 1.2712 - val_loss: 8.1344 - val_mae: 1.9838\n",
            "Epoch 76/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 2.7280 - mae: 1.2471 - val_loss: 8.5683 - val_mae: 2.1136\n",
            "Epoch 77/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 2.6279 - mae: 1.1902 - val_loss: 8.4235 - val_mae: 2.0926\n",
            "Epoch 78/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 2.9594 - mae: 1.3597 - val_loss: 8.9790 - val_mae: 2.2782\n",
            "Epoch 79/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 3.5190 - mae: 1.4247 - val_loss: 11.1757 - val_mae: 2.3632\n",
            "Epoch 80/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 3.2009 - mae: 1.3011 - val_loss: 7.9169 - val_mae: 1.9815\n",
            "Epoch 81/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 2.4521 - mae: 1.1326 - val_loss: 7.7357 - val_mae: 1.9798\n",
            "Epoch 82/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 2.9535 - mae: 1.2540 - val_loss: 8.0270 - val_mae: 2.0544\n",
            "Epoch 83/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 2.4067 - mae: 1.1588 - val_loss: 7.4216 - val_mae: 2.0032\n",
            "Epoch 84/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 3.0817 - mae: 1.3367 - val_loss: 8.1008 - val_mae: 2.1346\n",
            "Epoch 85/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 3.2206 - mae: 1.3580 - val_loss: 8.6832 - val_mae: 2.0507\n",
            "Epoch 86/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 2.7900 - mae: 1.1966 - val_loss: 9.6225 - val_mae: 2.1233\n",
            "Epoch 87/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 3.2602 - mae: 1.3718 - val_loss: 8.5432 - val_mae: 2.2268\n",
            "Epoch 88/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 3.5054 - mae: 1.4172 - val_loss: 9.3617 - val_mae: 2.2208\n",
            "Epoch 89/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 2.6400 - mae: 1.2012 - val_loss: 6.8995 - val_mae: 1.9784\n",
            "Epoch 90/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 2.8001 - mae: 1.2298 - val_loss: 8.1501 - val_mae: 2.1323\n",
            "Epoch 91/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 3.2854 - mae: 1.4064 - val_loss: 8.6294 - val_mae: 2.1740\n",
            "Epoch 92/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 2.3929 - mae: 1.1352 - val_loss: 7.5307 - val_mae: 2.0278\n",
            "Epoch 93/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 2.6920 - mae: 1.2338 - val_loss: 8.8912 - val_mae: 2.2902\n",
            "Epoch 94/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 2.5756 - mae: 1.1866 - val_loss: 7.9584 - val_mae: 1.9920\n",
            "Epoch 95/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 2.6764 - mae: 1.2103 - val_loss: 9.0729 - val_mae: 2.1046\n",
            "Epoch 96/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 2.4124 - mae: 1.1598 - val_loss: 7.4627 - val_mae: 2.0796\n",
            "Epoch 97/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 3.1812 - mae: 1.3232 - val_loss: 7.6408 - val_mae: 2.0188\n",
            "Epoch 98/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 2.0355 - mae: 1.1003 - val_loss: 8.7339 - val_mae: 2.1680\n",
            "Epoch 99/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 3.0164 - mae: 1.2712 - val_loss: 8.0634 - val_mae: 2.0989\n",
            "Epoch 100/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 2.3112 - mae: 1.1302 - val_loss: 8.5844 - val_mae: 2.1594\n",
            "Epoch 101/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 2.9523 - mae: 1.2506 - val_loss: 7.8915 - val_mae: 2.0720\n",
            "Epoch 102/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 2.3120 - mae: 1.0805 - val_loss: 8.9430 - val_mae: 2.1229\n",
            "Epoch 103/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.7526 - mae: 0.9941 - val_loss: 8.7394 - val_mae: 2.1949\n",
            "Epoch 104/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 2.5404 - mae: 1.1989 - val_loss: 7.9142 - val_mae: 2.0765\n",
            "Epoch 105/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 2.7816 - mae: 1.2685 - val_loss: 7.5576 - val_mae: 2.0559\n",
            "Epoch 106/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 2.6186 - mae: 1.2378 - val_loss: 8.6653 - val_mae: 2.0177\n",
            "Epoch 107/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 2.4974 - mae: 1.1752 - val_loss: 9.5258 - val_mae: 2.2919\n",
            "Epoch 108/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 2.4595 - mae: 1.1196 - val_loss: 7.8332 - val_mae: 2.0085\n",
            "Epoch 109/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 2.5682 - mae: 1.2133 - val_loss: 7.5219 - val_mae: 2.0012\n",
            "Epoch 110/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 2.4160 - mae: 1.1991 - val_loss: 7.8946 - val_mae: 2.1128\n",
            "Epoch 111/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 2.0591 - mae: 1.1101 - val_loss: 8.2814 - val_mae: 2.0900\n",
            "Epoch 112/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 3.2426 - mae: 1.3321 - val_loss: 8.0317 - val_mae: 2.0309\n",
            "Epoch 113/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 2.2080 - mae: 1.1151 - val_loss: 7.2494 - val_mae: 2.0293\n",
            "Epoch 114/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 2.5821 - mae: 1.2111 - val_loss: 8.3736 - val_mae: 2.1189\n",
            "Epoch 115/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 2.4615 - mae: 1.1638 - val_loss: 7.8805 - val_mae: 2.0115\n",
            "Epoch 116/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 2.4501 - mae: 1.1541 - val_loss: 7.9819 - val_mae: 2.0905\n",
            "Epoch 117/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 2.5398 - mae: 1.1356 - val_loss: 7.5262 - val_mae: 2.0444\n",
            "Epoch 118/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.8976 - mae: 1.0045 - val_loss: 9.8081 - val_mae: 2.3515\n",
            "Epoch 119/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 2.7792 - mae: 1.2672 - val_loss: 7.9208 - val_mae: 2.0894\n",
            "Epoch 120/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 2.3452 - mae: 1.1513 - val_loss: 7.4183 - val_mae: 2.0383\n",
            "Epoch 121/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 2.5308 - mae: 1.2039 - val_loss: 8.3224 - val_mae: 2.0715\n",
            "Epoch 122/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.8871 - mae: 1.0135 - val_loss: 7.1543 - val_mae: 1.9693\n",
            "Epoch 123/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 2.0214 - mae: 1.0345 - val_loss: 9.4357 - val_mae: 2.2236\n",
            "Epoch 124/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 2.5351 - mae: 1.2270 - val_loss: 8.3557 - val_mae: 2.1225\n",
            "Epoch 125/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.9983 - mae: 1.0791 - val_loss: 8.3214 - val_mae: 2.0350\n",
            "Epoch 126/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.9113 - mae: 1.0491 - val_loss: 7.8308 - val_mae: 1.9933\n",
            "Epoch 127/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.9680 - mae: 1.0524 - val_loss: 7.6375 - val_mae: 2.0364\n",
            "Epoch 128/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 2.2405 - mae: 1.1084 - val_loss: 8.8861 - val_mae: 2.2335\n",
            "Epoch 129/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 2.0473 - mae: 1.0915 - val_loss: 8.6710 - val_mae: 2.0965\n",
            "Epoch 130/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.9842 - mae: 1.0417 - val_loss: 7.7196 - val_mae: 2.0581\n",
            "Epoch 131/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 2.0747 - mae: 1.0891 - val_loss: 8.3380 - val_mae: 2.0695\n",
            "Epoch 132/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 2.6076 - mae: 1.1940 - val_loss: 8.8756 - val_mae: 2.1843\n",
            "Epoch 133/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.8800 - mae: 1.0571 - val_loss: 8.2347 - val_mae: 2.0219\n",
            "Epoch 134/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.7460 - mae: 0.9788 - val_loss: 8.2118 - val_mae: 2.0409\n",
            "Epoch 135/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.6629 - mae: 0.9638 - val_loss: 10.0344 - val_mae: 2.2549\n",
            "Epoch 136/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.9704 - mae: 1.0988 - val_loss: 9.2704 - val_mae: 2.2874\n",
            "Epoch 137/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 2.2392 - mae: 1.1370 - val_loss: 8.1534 - val_mae: 2.0630\n",
            "Epoch 138/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.9426 - mae: 1.0538 - val_loss: 8.8941 - val_mae: 2.1306\n",
            "Epoch 139/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.4259 - mae: 0.9274 - val_loss: 7.7949 - val_mae: 2.0044\n",
            "Epoch 140/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 2.1100 - mae: 1.0780 - val_loss: 9.6093 - val_mae: 2.2317\n",
            "Epoch 141/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.8774 - mae: 0.9913 - val_loss: 8.1630 - val_mae: 2.1107\n",
            "Epoch 142/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.7229 - mae: 0.9463 - val_loss: 9.5593 - val_mae: 2.2088\n",
            "Epoch 143/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.6479 - mae: 0.9753 - val_loss: 9.3148 - val_mae: 2.2537\n",
            "Epoch 144/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.6819 - mae: 0.9375 - val_loss: 8.6194 - val_mae: 2.0975\n",
            "Epoch 145/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.5184 - mae: 0.9203 - val_loss: 9.7429 - val_mae: 2.2940\n",
            "Epoch 146/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.7992 - mae: 0.9960 - val_loss: 9.0454 - val_mae: 2.2505\n",
            "Epoch 147/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 2.0741 - mae: 1.0975 - val_loss: 8.7406 - val_mae: 2.1241\n",
            "Epoch 148/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 2.0396 - mae: 1.0979 - val_loss: 9.4234 - val_mae: 2.2289\n",
            "Epoch 149/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.7353 - mae: 0.9811 - val_loss: 8.5027 - val_mae: 2.2045\n",
            "Epoch 150/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.7581 - mae: 1.0129 - val_loss: 8.6592 - val_mae: 2.1714\n",
            "Epoch 151/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.5759 - mae: 0.9221 - val_loss: 8.7937 - val_mae: 2.1843\n",
            "Epoch 152/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.5858 - mae: 0.9782 - val_loss: 9.8931 - val_mae: 2.3116\n",
            "Epoch 153/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.8490 - mae: 1.0036 - val_loss: 8.9053 - val_mae: 2.1697\n",
            "Epoch 154/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.8229 - mae: 1.0138 - val_loss: 8.1870 - val_mae: 2.0873\n",
            "Epoch 155/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.7020 - mae: 0.9873 - val_loss: 8.4920 - val_mae: 2.1924\n",
            "Epoch 156/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.4442 - mae: 0.9347 - val_loss: 9.0064 - val_mae: 2.1562\n",
            "Epoch 157/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.4719 - mae: 0.9188 - val_loss: 9.3643 - val_mae: 2.1833\n",
            "Epoch 158/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.8652 - mae: 1.0175 - val_loss: 8.1563 - val_mae: 2.1041\n",
            "Epoch 159/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.5578 - mae: 0.9114 - val_loss: 8.9921 - val_mae: 2.2549\n",
            "Epoch 160/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.3874 - mae: 0.8650 - val_loss: 8.1152 - val_mae: 2.1460\n",
            "Epoch 161/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.2433 - mae: 0.8004 - val_loss: 8.9639 - val_mae: 2.1652\n",
            "Epoch 162/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.5439 - mae: 0.9274 - val_loss: 8.9445 - val_mae: 2.1748\n",
            "Epoch 163/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.2680 - mae: 0.8614 - val_loss: 9.8007 - val_mae: 2.2841\n",
            "Epoch 164/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.6526 - mae: 0.9435 - val_loss: 9.3468 - val_mae: 2.1565\n",
            "Epoch 165/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 2.0766 - mae: 1.1006 - val_loss: 8.6098 - val_mae: 2.1514\n",
            "Epoch 166/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.9343 - mae: 1.0484 - val_loss: 8.6999 - val_mae: 2.1432\n",
            "Epoch 167/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.8191 - mae: 1.0285 - val_loss: 8.4001 - val_mae: 2.1157\n",
            "Epoch 168/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.0410 - mae: 0.7785 - val_loss: 8.8510 - val_mae: 2.1098\n",
            "Epoch 169/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 2.2533 - mae: 1.0762 - val_loss: 8.4310 - val_mae: 2.1980\n",
            "Epoch 170/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.7899 - mae: 1.0414 - val_loss: 9.1831 - val_mae: 2.1281\n",
            "Epoch 171/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.5783 - mae: 0.9169 - val_loss: 8.4550 - val_mae: 2.2125\n",
            "Epoch 172/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.3336 - mae: 0.8877 - val_loss: 9.4587 - val_mae: 2.2470\n",
            "Epoch 173/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.4958 - mae: 0.9091 - val_loss: 8.3584 - val_mae: 2.2170\n",
            "Epoch 174/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.6966 - mae: 0.9521 - val_loss: 8.9792 - val_mae: 2.2014\n",
            "Epoch 175/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.5767 - mae: 0.9620 - val_loss: 9.1985 - val_mae: 2.1872\n",
            "Epoch 176/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.2492 - mae: 0.8352 - val_loss: 8.6839 - val_mae: 2.2756\n",
            "Epoch 177/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.2838 - mae: 0.8747 - val_loss: 9.2163 - val_mae: 2.2208\n",
            "Epoch 178/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.5520 - mae: 0.9544 - val_loss: 9.1421 - val_mae: 2.1550\n",
            "Epoch 179/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.3079 - mae: 0.8130 - val_loss: 8.4549 - val_mae: 2.1419\n",
            "Epoch 180/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.3528 - mae: 0.8485 - val_loss: 9.5498 - val_mae: 2.2593\n",
            "Epoch 181/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.2888 - mae: 0.8640 - val_loss: 9.1884 - val_mae: 2.2523\n",
            "Epoch 182/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.4729 - mae: 0.8985 - val_loss: 8.6984 - val_mae: 2.2000\n",
            "Epoch 183/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.7824 - mae: 1.0183 - val_loss: 8.9719 - val_mae: 2.2333\n",
            "Epoch 184/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.3498 - mae: 0.8608 - val_loss: 8.1134 - val_mae: 2.1571\n",
            "Epoch 185/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.4680 - mae: 0.8713 - val_loss: 9.3287 - val_mae: 2.2417\n",
            "Epoch 186/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.3247 - mae: 0.8674 - val_loss: 8.8541 - val_mae: 2.2019\n",
            "Epoch 187/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.6129 - mae: 0.9691 - val_loss: 8.4433 - val_mae: 2.2058\n",
            "Epoch 188/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.7441 - mae: 0.9963 - val_loss: 9.5437 - val_mae: 2.3110\n",
            "Epoch 189/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.7467 - mae: 0.9655 - val_loss: 8.6073 - val_mae: 2.1719\n",
            "Epoch 190/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.2830 - mae: 0.7965 - val_loss: 9.1674 - val_mae: 2.2185\n",
            "Epoch 191/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.0333 - mae: 0.7584 - val_loss: 8.8045 - val_mae: 2.2220\n",
            "Epoch 192/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.4515 - mae: 0.9153 - val_loss: 9.3918 - val_mae: 2.2598\n",
            "Epoch 193/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.4427 - mae: 0.9159 - val_loss: 8.5049 - val_mae: 2.1351\n",
            "Epoch 194/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.5259 - mae: 0.9294 - val_loss: 8.6964 - val_mae: 2.2412\n",
            "Epoch 195/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.7445 - mae: 0.9665 - val_loss: 10.0775 - val_mae: 2.3135\n",
            "Epoch 196/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.1380 - mae: 0.8444 - val_loss: 9.4339 - val_mae: 2.3013\n",
            "Epoch 197/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.3163 - mae: 0.8326 - val_loss: 8.6279 - val_mae: 2.0891\n",
            "Epoch 198/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.4569 - mae: 0.8880 - val_loss: 10.7156 - val_mae: 2.3455\n",
            "Epoch 199/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 2.4677 - mae: 1.1213 - val_loss: 9.4050 - val_mae: 2.2068\n",
            "Epoch 200/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.2349 - mae: 0.8044 - val_loss: 10.0154 - val_mae: 2.2688\n",
            "Epoch 201/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.3440 - mae: 0.8680 - val_loss: 9.2353 - val_mae: 2.3234\n",
            "Epoch 202/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.2531 - mae: 0.8496 - val_loss: 9.3614 - val_mae: 2.2196\n",
            "Epoch 203/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.2790 - mae: 0.8409 - val_loss: 9.1048 - val_mae: 2.2009\n",
            "Epoch 204/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.2913 - mae: 0.8481 - val_loss: 10.0878 - val_mae: 2.3310\n",
            "Epoch 205/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.6357 - mae: 0.9649 - val_loss: 9.7763 - val_mae: 2.2710\n",
            "Epoch 206/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.2313 - mae: 0.8553 - val_loss: 9.6811 - val_mae: 2.2287\n",
            "Epoch 207/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.0876 - mae: 0.7747 - val_loss: 9.8659 - val_mae: 2.3141\n",
            "Epoch 208/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.0428 - mae: 0.7608 - val_loss: 10.0200 - val_mae: 2.3670\n",
            "Epoch 209/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.4186 - mae: 0.8940 - val_loss: 9.6394 - val_mae: 2.2367\n",
            "Epoch 210/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 0.9043 - mae: 0.7376 - val_loss: 10.6458 - val_mae: 2.3747\n",
            "Epoch 211/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.4132 - mae: 0.9026 - val_loss: 9.1705 - val_mae: 2.2132\n",
            "Epoch 212/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.0217 - mae: 0.7676 - val_loss: 10.5517 - val_mae: 2.3842\n",
            "Epoch 213/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.2964 - mae: 0.8502 - val_loss: 10.1669 - val_mae: 2.4034\n",
            "Epoch 214/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.9872 - mae: 1.0864 - val_loss: 11.7171 - val_mae: 2.5285\n",
            "Epoch 215/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.1964 - mae: 0.8649 - val_loss: 9.6871 - val_mae: 2.2847\n",
            "Epoch 216/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.6434 - mae: 1.0070 - val_loss: 9.9643 - val_mae: 2.3387\n",
            "Epoch 217/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.0597 - mae: 0.7607 - val_loss: 10.5018 - val_mae: 2.3408\n",
            "Epoch 218/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.1013 - mae: 0.7934 - val_loss: 9.8106 - val_mae: 2.2351\n",
            "Epoch 219/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.3947 - mae: 0.8733 - val_loss: 9.5283 - val_mae: 2.2337\n",
            "Epoch 220/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 0.8506 - mae: 0.7062 - val_loss: 10.1856 - val_mae: 2.2846\n",
            "Epoch 221/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.4413 - mae: 0.8970 - val_loss: 10.5938 - val_mae: 2.2978\n",
            "Epoch 222/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.4922 - mae: 0.9332 - val_loss: 10.1553 - val_mae: 2.2583\n",
            "Epoch 223/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 0.9285 - mae: 0.7411 - val_loss: 9.6423 - val_mae: 2.3040\n",
            "Epoch 224/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.5136 - mae: 0.9282 - val_loss: 10.3811 - val_mae: 2.3447\n",
            "Epoch 225/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.2254 - mae: 0.7902 - val_loss: 10.3685 - val_mae: 2.2463\n",
            "Epoch 226/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.4317 - mae: 0.9012 - val_loss: 10.0220 - val_mae: 2.2616\n",
            "Epoch 227/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.4580 - mae: 0.8683 - val_loss: 10.8574 - val_mae: 2.4286\n",
            "Epoch 228/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.4221 - mae: 0.8653 - val_loss: 9.1753 - val_mae: 2.1912\n",
            "Epoch 229/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.0449 - mae: 0.7423 - val_loss: 11.2791 - val_mae: 2.4984\n",
            "Epoch 230/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.4309 - mae: 0.8581 - val_loss: 9.6851 - val_mae: 2.2490\n",
            "Epoch 231/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.2083 - mae: 0.7756 - val_loss: 11.3913 - val_mae: 2.5472\n",
            "Epoch 232/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.0110 - mae: 0.7180 - val_loss: 9.9153 - val_mae: 2.2259\n",
            "Epoch 233/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.4883 - mae: 0.8944 - val_loss: 10.7324 - val_mae: 2.4182\n",
            "Epoch 234/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.3874 - mae: 0.8410 - val_loss: 10.6881 - val_mae: 2.3991\n",
            "Epoch 235/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 0.9189 - mae: 0.7095 - val_loss: 10.2121 - val_mae: 2.3359\n",
            "Epoch 236/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.0111 - mae: 0.7971 - val_loss: 9.7673 - val_mae: 2.2532\n",
            "Epoch 237/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 0.9989 - mae: 0.7664 - val_loss: 9.1775 - val_mae: 2.2078\n",
            "Epoch 238/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.3447 - mae: 0.8408 - val_loss: 11.4379 - val_mae: 2.3760\n",
            "Epoch 239/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.2466 - mae: 0.8276 - val_loss: 9.3991 - val_mae: 2.2028\n",
            "Epoch 240/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 0.9461 - mae: 0.6920 - val_loss: 8.6338 - val_mae: 2.1699\n",
            "Epoch 241/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.1754 - mae: 0.8208 - val_loss: 8.7507 - val_mae: 2.1652\n",
            "Epoch 242/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.1894 - mae: 0.8195 - val_loss: 9.0829 - val_mae: 2.2230\n",
            "Epoch 243/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.0862 - mae: 0.7880 - val_loss: 10.0621 - val_mae: 2.2383\n",
            "Epoch 244/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.2865 - mae: 0.8531 - val_loss: 9.0435 - val_mae: 2.1361\n",
            "Epoch 245/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.3029 - mae: 0.8824 - val_loss: 8.1433 - val_mae: 2.1196\n",
            "Epoch 246/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.0674 - mae: 0.7857 - val_loss: 9.4483 - val_mae: 2.2043\n",
            "Epoch 247/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8861 - mae: 0.7084 - val_loss: 9.8552 - val_mae: 2.2700\n",
            "Epoch 248/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.2105 - mae: 0.8222 - val_loss: 10.1669 - val_mae: 2.3459\n",
            "Epoch 249/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.4932 - mae: 0.8576 - val_loss: 8.8659 - val_mae: 2.1722\n",
            "Epoch 250/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 0.9579 - mae: 0.7370 - val_loss: 9.4451 - val_mae: 2.3300\n",
            "Epoch 251/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.3205 - mae: 0.8673 - val_loss: 9.7122 - val_mae: 2.2683\n",
            "Epoch 252/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 0.7756 - mae: 0.6648 - val_loss: 9.3202 - val_mae: 2.2990\n",
            "Epoch 253/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.1363 - mae: 0.8172 - val_loss: 8.7752 - val_mae: 2.1417\n",
            "Epoch 254/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.0629 - mae: 0.7540 - val_loss: 9.5042 - val_mae: 2.2396\n",
            "Epoch 255/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.3892 - mae: 0.8923 - val_loss: 8.3747 - val_mae: 2.1361\n",
            "Epoch 256/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 0.9252 - mae: 0.7105 - val_loss: 10.3753 - val_mae: 2.3321\n",
            "Epoch 257/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.2693 - mae: 0.8568 - val_loss: 10.0272 - val_mae: 2.1892\n",
            "Epoch 258/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.1681 - mae: 0.8037 - val_loss: 9.1095 - val_mae: 2.2028\n",
            "Epoch 259/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.9877 - mae: 0.6938 - val_loss: 10.2212 - val_mae: 2.2385\n",
            "Epoch 260/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 0.9214 - mae: 0.7392 - val_loss: 9.9333 - val_mae: 2.3124\n",
            "Epoch 261/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.9608 - mae: 0.7036 - val_loss: 9.0806 - val_mae: 2.1617\n",
            "Epoch 262/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.1996 - mae: 0.7539 - val_loss: 10.3223 - val_mae: 2.3208\n",
            "Epoch 263/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.3852 - mae: 0.8681 - val_loss: 9.0925 - val_mae: 2.2083\n",
            "Epoch 264/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.5865 - mae: 0.9667 - val_loss: 10.2268 - val_mae: 2.3353\n",
            "Epoch 265/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.0747 - mae: 0.8036 - val_loss: 10.9970 - val_mae: 2.3059\n",
            "Epoch 266/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.4093 - mae: 0.8995 - val_loss: 8.9168 - val_mae: 2.2059\n",
            "Epoch 267/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.4411 - mae: 0.8851 - val_loss: 9.0251 - val_mae: 2.1672\n",
            "Epoch 268/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.1017 - mae: 0.7598 - val_loss: 9.4970 - val_mae: 2.2889\n",
            "Epoch 269/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.1618 - mae: 0.7826 - val_loss: 10.7321 - val_mae: 2.4251\n",
            "Epoch 270/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.0918 - mae: 0.7992 - val_loss: 9.5572 - val_mae: 2.2675\n",
            "Epoch 271/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.0334 - mae: 0.7811 - val_loss: 9.7693 - val_mae: 2.2278\n",
            "Epoch 272/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 0.8331 - mae: 0.6741 - val_loss: 9.2995 - val_mae: 2.2720\n",
            "Epoch 273/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.9987 - mae: 0.7630 - val_loss: 10.2706 - val_mae: 2.2570\n",
            "Epoch 274/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.1688 - mae: 0.8274 - val_loss: 9.9940 - val_mae: 2.3267\n",
            "Epoch 275/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.2319 - mae: 0.8197 - val_loss: 9.7667 - val_mae: 2.2204\n",
            "Epoch 276/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 0.9815 - mae: 0.7365 - val_loss: 9.1510 - val_mae: 2.2350\n",
            "Epoch 277/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.2806 - mae: 0.8151 - val_loss: 10.2298 - val_mae: 2.3200\n",
            "Epoch 278/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.2348 - mae: 0.8272 - val_loss: 9.0991 - val_mae: 2.1844\n",
            "Epoch 279/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.2223 - mae: 0.7938 - val_loss: 9.4187 - val_mae: 2.2589\n",
            "Epoch 280/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 0.7826 - mae: 0.6528 - val_loss: 8.8690 - val_mae: 2.2100\n",
            "Epoch 281/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 1.1851 - mae: 0.8102 - val_loss: 10.6641 - val_mae: 2.2740\n",
            "Epoch 282/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.0218 - mae: 0.7265 - val_loss: 10.0456 - val_mae: 2.3243\n",
            "Epoch 283/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 0.9504 - mae: 0.6837 - val_loss: 9.4475 - val_mae: 2.2205\n",
            "Epoch 284/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 0.8333 - mae: 0.6786 - val_loss: 10.0011 - val_mae: 2.2841\n",
            "Epoch 285/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.9228 - mae: 0.7005 - val_loss: 9.3727 - val_mae: 2.2450\n",
            "Epoch 286/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.3345 - mae: 0.8989 - val_loss: 10.3976 - val_mae: 2.3580\n",
            "Epoch 287/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 0.8990 - mae: 0.7204 - val_loss: 10.5247 - val_mae: 2.3262\n",
            "Epoch 288/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.0548 - mae: 0.7812 - val_loss: 9.8881 - val_mae: 2.2753\n",
            "Epoch 289/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 0.9340 - mae: 0.7247 - val_loss: 9.7739 - val_mae: 2.2246\n",
            "Epoch 290/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.0324 - mae: 0.7888 - val_loss: 9.6382 - val_mae: 2.2786\n",
            "Epoch 291/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.9375 - mae: 0.7180 - val_loss: 9.8157 - val_mae: 2.2104\n",
            "Epoch 292/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8362 - mae: 0.7070 - val_loss: 10.3287 - val_mae: 2.2641\n",
            "Epoch 293/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 0.8983 - mae: 0.7116 - val_loss: 9.8721 - val_mae: 2.2513\n",
            "Epoch 294/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.1328 - mae: 0.8282 - val_loss: 8.6023 - val_mae: 2.2010\n",
            "Epoch 295/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.2193 - mae: 0.8366 - val_loss: 8.9546 - val_mae: 2.2026\n",
            "Epoch 296/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7157 - mae: 0.6056 - val_loss: 9.8649 - val_mae: 2.3002\n",
            "Epoch 297/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.9404 - mae: 0.7349 - val_loss: 10.4372 - val_mae: 2.3499\n",
            "Epoch 298/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 0.9984 - mae: 0.7548 - val_loss: 9.9379 - val_mae: 2.2803\n",
            "Epoch 299/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.9192 - mae: 0.7203 - val_loss: 8.6201 - val_mae: 2.1364\n",
            "Epoch 300/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.1190 - mae: 0.8091 - val_loss: 9.7012 - val_mae: 2.2736\n",
            "Epoch 301/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.0841 - mae: 0.7641 - val_loss: 9.8969 - val_mae: 2.3482\n",
            "Epoch 302/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.2483 - mae: 0.8285 - val_loss: 9.3515 - val_mae: 2.1766\n",
            "Epoch 303/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.6924 - mae: 0.6250 - val_loss: 9.4305 - val_mae: 2.2707\n",
            "Epoch 304/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.6264 - mae: 0.5968 - val_loss: 9.3886 - val_mae: 2.1254\n",
            "Epoch 305/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7346 - mae: 0.6347 - val_loss: 9.1202 - val_mae: 2.2475\n",
            "Epoch 306/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.0120 - mae: 0.7827 - val_loss: 9.8449 - val_mae: 2.2522\n",
            "Epoch 307/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.9635 - mae: 0.7055 - val_loss: 8.8802 - val_mae: 2.2572\n",
            "Epoch 308/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 0.9259 - mae: 0.7204 - val_loss: 9.0228 - val_mae: 2.2087\n",
            "Epoch 309/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.0071 - mae: 0.7343 - val_loss: 8.5703 - val_mae: 2.1272\n",
            "Epoch 310/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8774 - mae: 0.6775 - val_loss: 9.3158 - val_mae: 2.3140\n",
            "Epoch 311/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.6580 - mae: 0.5934 - val_loss: 9.1672 - val_mae: 2.2257\n",
            "Epoch 312/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7259 - mae: 0.6619 - val_loss: 10.3218 - val_mae: 2.2533\n",
            "Epoch 313/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8987 - mae: 0.7293 - val_loss: 10.3345 - val_mae: 2.3021\n",
            "Epoch 314/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7858 - mae: 0.6579 - val_loss: 9.5230 - val_mae: 2.2846\n",
            "Epoch 315/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.1261 - mae: 0.7904 - val_loss: 9.7118 - val_mae: 2.2713\n",
            "Epoch 316/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.9895 - mae: 0.7227 - val_loss: 8.5861 - val_mae: 2.1892\n",
            "Epoch 317/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.1007 - mae: 0.7734 - val_loss: 9.6575 - val_mae: 2.2548\n",
            "Epoch 318/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.5818 - mae: 0.9079 - val_loss: 9.8132 - val_mae: 2.3243\n",
            "Epoch 319/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.0267 - mae: 0.7415 - val_loss: 10.0244 - val_mae: 2.2798\n",
            "Epoch 320/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.9837 - mae: 0.7270 - val_loss: 9.9171 - val_mae: 2.3054\n",
            "Epoch 321/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.6858 - mae: 0.6131 - val_loss: 9.5800 - val_mae: 2.2154\n",
            "Epoch 322/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 0.7349 - mae: 0.6518 - val_loss: 9.2206 - val_mae: 2.2208\n",
            "Epoch 323/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7182 - mae: 0.6602 - val_loss: 9.6342 - val_mae: 2.2719\n",
            "Epoch 324/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.9245 - mae: 0.7193 - val_loss: 11.3687 - val_mae: 2.4820\n",
            "Epoch 325/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 0.8437 - mae: 0.6826 - val_loss: 10.1061 - val_mae: 2.3291\n",
            "Epoch 326/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.0820 - mae: 0.7347 - val_loss: 10.0366 - val_mae: 2.2414\n",
            "Epoch 327/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8129 - mae: 0.6654 - val_loss: 10.0197 - val_mae: 2.3433\n",
            "Epoch 328/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.0147 - mae: 0.7672 - val_loss: 9.9710 - val_mae: 2.2943\n",
            "Epoch 329/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7950 - mae: 0.6887 - val_loss: 9.8195 - val_mae: 2.2635\n",
            "Epoch 330/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8060 - mae: 0.6865 - val_loss: 10.0159 - val_mae: 2.3047\n",
            "Epoch 331/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 0.9025 - mae: 0.6800 - val_loss: 9.3761 - val_mae: 2.1900\n",
            "Epoch 332/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.5165 - mae: 0.5496 - val_loss: 9.9603 - val_mae: 2.3394\n",
            "Epoch 333/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 0.7887 - mae: 0.6797 - val_loss: 9.8634 - val_mae: 2.2276\n",
            "Epoch 334/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.2756 - mae: 0.8672 - val_loss: 10.4303 - val_mae: 2.2382\n",
            "Epoch 335/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.0880 - mae: 0.7819 - val_loss: 9.9265 - val_mae: 2.2544\n",
            "Epoch 336/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8844 - mae: 0.6955 - val_loss: 9.4714 - val_mae: 2.1900\n",
            "Epoch 337/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.2122 - mae: 0.7713 - val_loss: 9.6396 - val_mae: 2.1507\n",
            "Epoch 338/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.9746 - mae: 0.7359 - val_loss: 9.5681 - val_mae: 2.1691\n",
            "Epoch 339/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7407 - mae: 0.6512 - val_loss: 10.1078 - val_mae: 2.2899\n",
            "Epoch 340/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.9260 - mae: 0.6781 - val_loss: 11.0743 - val_mae: 2.3435\n",
            "Epoch 341/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8504 - mae: 0.6739 - val_loss: 9.9524 - val_mae: 2.1925\n",
            "Epoch 342/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8423 - mae: 0.6563 - val_loss: 9.3487 - val_mae: 2.2009\n",
            "Epoch 343/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.6669 - mae: 0.6400 - val_loss: 11.0702 - val_mae: 2.2818\n",
            "Epoch 344/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.9359 - mae: 0.7390 - val_loss: 9.8770 - val_mae: 2.2019\n",
            "Epoch 345/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.9638 - mae: 0.7613 - val_loss: 10.0486 - val_mae: 2.2501\n",
            "Epoch 346/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8439 - mae: 0.6059 - val_loss: 10.6443 - val_mae: 2.2575\n",
            "Epoch 347/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7812 - mae: 0.6564 - val_loss: 10.5720 - val_mae: 2.2332\n",
            "Epoch 348/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8627 - mae: 0.7000 - val_loss: 11.0024 - val_mae: 2.3115\n",
            "Epoch 349/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8949 - mae: 0.7191 - val_loss: 9.7614 - val_mae: 2.2143\n",
            "Epoch 350/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8331 - mae: 0.7175 - val_loss: 10.7195 - val_mae: 2.2879\n",
            "Epoch 351/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8961 - mae: 0.6879 - val_loss: 10.8435 - val_mae: 2.3191\n",
            "Epoch 352/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.4532 - mae: 0.8427 - val_loss: 10.1594 - val_mae: 2.2918\n",
            "Epoch 353/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7537 - mae: 0.6388 - val_loss: 9.8968 - val_mae: 2.2197\n",
            "Epoch 354/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7325 - mae: 0.6323 - val_loss: 9.2767 - val_mae: 2.2101\n",
            "Epoch 355/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8185 - mae: 0.6751 - val_loss: 10.4170 - val_mae: 2.2767\n",
            "Epoch 356/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.6489 - mae: 0.6030 - val_loss: 10.0570 - val_mae: 2.2503\n",
            "Epoch 357/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7272 - mae: 0.6489 - val_loss: 10.1436 - val_mae: 2.2827\n",
            "Epoch 358/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.1003 - mae: 0.7869 - val_loss: 10.0132 - val_mae: 2.2637\n",
            "Epoch 359/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.0803 - mae: 0.7512 - val_loss: 10.6078 - val_mae: 2.2787\n",
            "Epoch 360/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7256 - mae: 0.6342 - val_loss: 9.6099 - val_mae: 2.2473\n",
            "Epoch 361/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.1184 - mae: 0.6874 - val_loss: 10.0746 - val_mae: 2.3456\n",
            "Epoch 362/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8065 - mae: 0.7043 - val_loss: 10.8689 - val_mae: 2.3370\n",
            "Epoch 363/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.0247 - mae: 0.7928 - val_loss: 9.5718 - val_mae: 2.2508\n",
            "Epoch 364/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7982 - mae: 0.6112 - val_loss: 9.4607 - val_mae: 2.2118\n",
            "Epoch 365/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 0.9016 - mae: 0.6646 - val_loss: 10.4096 - val_mae: 2.2644\n",
            "Epoch 366/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7871 - mae: 0.6261 - val_loss: 9.8807 - val_mae: 2.2594\n",
            "Epoch 367/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7414 - mae: 0.6647 - val_loss: 9.6658 - val_mae: 2.2699\n",
            "Epoch 368/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7896 - mae: 0.6636 - val_loss: 9.8380 - val_mae: 2.2484\n",
            "Epoch 369/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.9486 - mae: 0.7253 - val_loss: 9.8468 - val_mae: 2.2837\n",
            "Epoch 370/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.9553 - mae: 0.7004 - val_loss: 11.0212 - val_mae: 2.3965\n",
            "Epoch 371/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.1914 - mae: 0.7752 - val_loss: 10.3001 - val_mae: 2.4269\n",
            "Epoch 372/500\n",
            "323/323 [==============================] - 0s 1ms/step - loss: 0.8083 - mae: 0.6858 - val_loss: 10.2296 - val_mae: 2.2934\n",
            "Epoch 373/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.0473 - mae: 0.7433 - val_loss: 10.5023 - val_mae: 2.3301\n",
            "Epoch 374/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7298 - mae: 0.6491 - val_loss: 10.3316 - val_mae: 2.2892\n",
            "Epoch 375/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8418 - mae: 0.6840 - val_loss: 9.8031 - val_mae: 2.2539\n",
            "Epoch 376/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8761 - mae: 0.7128 - val_loss: 9.9182 - val_mae: 2.2837\n",
            "Epoch 377/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8456 - mae: 0.7191 - val_loss: 11.3598 - val_mae: 2.3091\n",
            "Epoch 378/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8113 - mae: 0.6894 - val_loss: 9.2547 - val_mae: 2.2458\n",
            "Epoch 379/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7044 - mae: 0.6349 - val_loss: 9.3897 - val_mae: 2.1942\n",
            "Epoch 380/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.5784 - mae: 0.5826 - val_loss: 9.3625 - val_mae: 2.2314\n",
            "Epoch 381/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8012 - mae: 0.6574 - val_loss: 10.1496 - val_mae: 2.3015\n",
            "Epoch 382/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.0089 - mae: 0.7436 - val_loss: 9.4746 - val_mae: 2.2207\n",
            "Epoch 383/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.1472 - mae: 0.7947 - val_loss: 10.1310 - val_mae: 2.2841\n",
            "Epoch 384/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.9336 - mae: 0.7007 - val_loss: 9.6340 - val_mae: 2.2940\n",
            "Epoch 385/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8912 - mae: 0.7118 - val_loss: 10.3358 - val_mae: 2.3235\n",
            "Epoch 386/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7880 - mae: 0.6284 - val_loss: 9.9993 - val_mae: 2.2675\n",
            "Epoch 387/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7718 - mae: 0.6354 - val_loss: 9.6810 - val_mae: 2.2775\n",
            "Epoch 388/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.4697 - mae: 0.5167 - val_loss: 9.4232 - val_mae: 2.2857\n",
            "Epoch 389/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.5518 - mae: 0.5889 - val_loss: 10.3119 - val_mae: 2.2562\n",
            "Epoch 390/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.9821 - mae: 0.7344 - val_loss: 10.3865 - val_mae: 2.2756\n",
            "Epoch 391/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 1.1705 - mae: 0.7710 - val_loss: 9.8623 - val_mae: 2.2799\n",
            "Epoch 392/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.0765 - mae: 0.7656 - val_loss: 11.5635 - val_mae: 2.4835\n",
            "Epoch 393/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7871 - mae: 0.6777 - val_loss: 9.6879 - val_mae: 2.2654\n",
            "Epoch 394/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7815 - mae: 0.6630 - val_loss: 10.1388 - val_mae: 2.3383\n",
            "Epoch 395/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7019 - mae: 0.6391 - val_loss: 10.4185 - val_mae: 2.2400\n",
            "Epoch 396/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.9973 - mae: 0.7376 - val_loss: 11.6348 - val_mae: 2.4377\n",
            "Epoch 397/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.0675 - mae: 0.7174 - val_loss: 10.0202 - val_mae: 2.3329\n",
            "Epoch 398/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.9358 - mae: 0.6615 - val_loss: 9.7966 - val_mae: 2.2592\n",
            "Epoch 399/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.6222 - mae: 0.5838 - val_loss: 9.7435 - val_mae: 2.3060\n",
            "Epoch 400/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.0999 - mae: 0.7380 - val_loss: 10.0341 - val_mae: 2.2900\n",
            "Epoch 401/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7287 - mae: 0.6505 - val_loss: 10.3944 - val_mae: 2.3062\n",
            "Epoch 402/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7175 - mae: 0.6369 - val_loss: 9.7476 - val_mae: 2.2507\n",
            "Epoch 403/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.5819 - mae: 0.5602 - val_loss: 11.0362 - val_mae: 2.3125\n",
            "Epoch 404/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.5765 - mae: 0.5787 - val_loss: 9.1874 - val_mae: 2.1997\n",
            "Epoch 405/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.6753 - mae: 0.6047 - val_loss: 10.2464 - val_mae: 2.3353\n",
            "Epoch 406/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.1376 - mae: 0.8039 - val_loss: 10.6401 - val_mae: 2.3362\n",
            "Epoch 407/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.6139 - mae: 0.5779 - val_loss: 9.5008 - val_mae: 2.2793\n",
            "Epoch 408/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7985 - mae: 0.6578 - val_loss: 10.3349 - val_mae: 2.3053\n",
            "Epoch 409/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7592 - mae: 0.6173 - val_loss: 9.7468 - val_mae: 2.2511\n",
            "Epoch 410/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.6561 - mae: 0.5981 - val_loss: 10.4636 - val_mae: 2.3693\n",
            "Epoch 411/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8866 - mae: 0.6850 - val_loss: 9.4975 - val_mae: 2.1797\n",
            "Epoch 412/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7915 - mae: 0.6699 - val_loss: 10.0337 - val_mae: 2.3723\n",
            "Epoch 413/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7932 - mae: 0.6692 - val_loss: 9.6949 - val_mae: 2.2585\n",
            "Epoch 414/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.0237 - mae: 0.7461 - val_loss: 9.2184 - val_mae: 2.1962\n",
            "Epoch 415/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.4879 - mae: 0.5389 - val_loss: 10.2494 - val_mae: 2.2679\n",
            "Epoch 416/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.6242 - mae: 0.5992 - val_loss: 11.0668 - val_mae: 2.4680\n",
            "Epoch 417/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8794 - mae: 0.7322 - val_loss: 9.5309 - val_mae: 2.1996\n",
            "Epoch 418/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7683 - mae: 0.6334 - val_loss: 10.9470 - val_mae: 2.3290\n",
            "Epoch 419/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8709 - mae: 0.6868 - val_loss: 9.4133 - val_mae: 2.2511\n",
            "Epoch 420/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8027 - mae: 0.6845 - val_loss: 9.7006 - val_mae: 2.2413\n",
            "Epoch 421/500\n",
            "323/323 [==============================] - 0s 2ms/step - loss: 0.5707 - mae: 0.5575 - val_loss: 8.8277 - val_mae: 2.2173\n",
            "Epoch 422/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8244 - mae: 0.6902 - val_loss: 10.8219 - val_mae: 2.3601\n",
            "Epoch 423/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.9955 - mae: 0.7416 - val_loss: 9.5963 - val_mae: 2.2470\n",
            "Epoch 424/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.6810 - mae: 0.6377 - val_loss: 9.4107 - val_mae: 2.2505\n",
            "Epoch 425/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.0915 - mae: 0.7768 - val_loss: 10.1425 - val_mae: 2.3493\n",
            "Epoch 426/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7300 - mae: 0.6494 - val_loss: 9.5915 - val_mae: 2.2437\n",
            "Epoch 427/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8126 - mae: 0.6658 - val_loss: 9.9830 - val_mae: 2.2761\n",
            "Epoch 428/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8217 - mae: 0.6220 - val_loss: 10.6198 - val_mae: 2.2650\n",
            "Epoch 429/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7385 - mae: 0.6085 - val_loss: 9.3649 - val_mae: 2.1773\n",
            "Epoch 430/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7227 - mae: 0.6358 - val_loss: 10.1134 - val_mae: 2.2382\n",
            "Epoch 431/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.6203 - mae: 0.5909 - val_loss: 10.5026 - val_mae: 2.2766\n",
            "Epoch 432/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8284 - mae: 0.7085 - val_loss: 9.8085 - val_mae: 2.2492\n",
            "Epoch 433/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.6677 - mae: 0.5946 - val_loss: 10.6693 - val_mae: 2.2487\n",
            "Epoch 434/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.6351 - mae: 0.5922 - val_loss: 9.5044 - val_mae: 2.2015\n",
            "Epoch 435/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.6181 - mae: 0.5640 - val_loss: 8.6787 - val_mae: 2.2399\n",
            "Epoch 436/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8399 - mae: 0.6682 - val_loss: 10.5886 - val_mae: 2.2604\n",
            "Epoch 437/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.6680 - mae: 0.6081 - val_loss: 10.4216 - val_mae: 2.4243\n",
            "Epoch 438/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8731 - mae: 0.7050 - val_loss: 10.3293 - val_mae: 2.3007\n",
            "Epoch 439/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.4661 - mae: 0.5142 - val_loss: 10.3098 - val_mae: 2.2748\n",
            "Epoch 440/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.4534 - mae: 0.4733 - val_loss: 10.9614 - val_mae: 2.3697\n",
            "Epoch 441/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8150 - mae: 0.6654 - val_loss: 9.9530 - val_mae: 2.2782\n",
            "Epoch 442/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.6664 - mae: 0.6120 - val_loss: 10.4878 - val_mae: 2.2975\n",
            "Epoch 443/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.5128 - mae: 0.5380 - val_loss: 10.0528 - val_mae: 2.2407\n",
            "Epoch 444/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7577 - mae: 0.6256 - val_loss: 10.1825 - val_mae: 2.2747\n",
            "Epoch 445/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7677 - mae: 0.6450 - val_loss: 10.0698 - val_mae: 2.3102\n",
            "Epoch 446/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7442 - mae: 0.6383 - val_loss: 11.5104 - val_mae: 2.2885\n",
            "Epoch 447/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.6320 - mae: 0.5640 - val_loss: 9.9737 - val_mae: 2.3276\n",
            "Epoch 448/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.5675 - mae: 0.5525 - val_loss: 9.8126 - val_mae: 2.1827\n",
            "Epoch 449/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7214 - mae: 0.6541 - val_loss: 10.2434 - val_mae: 2.2945\n",
            "Epoch 450/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.6050 - mae: 0.5924 - val_loss: 10.1939 - val_mae: 2.2758\n",
            "Epoch 451/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.6321 - mae: 0.5974 - val_loss: 9.9931 - val_mae: 2.2330\n",
            "Epoch 452/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7687 - mae: 0.6288 - val_loss: 10.3321 - val_mae: 2.3516\n",
            "Epoch 453/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.5156 - mae: 0.5609 - val_loss: 9.9580 - val_mae: 2.1925\n",
            "Epoch 454/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.6518 - mae: 0.5973 - val_loss: 9.8620 - val_mae: 2.2928\n",
            "Epoch 455/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.5301 - mae: 0.5000 - val_loss: 9.6775 - val_mae: 2.1962\n",
            "Epoch 456/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7819 - mae: 0.6217 - val_loss: 11.1224 - val_mae: 2.3837\n",
            "Epoch 457/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.9964 - mae: 0.7689 - val_loss: 9.9408 - val_mae: 2.2998\n",
            "Epoch 458/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.6995 - mae: 0.5917 - val_loss: 9.1683 - val_mae: 2.2500\n",
            "Epoch 459/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.0808 - mae: 0.7269 - val_loss: 10.9215 - val_mae: 2.3890\n",
            "Epoch 460/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7954 - mae: 0.6608 - val_loss: 10.2232 - val_mae: 2.3069\n",
            "Epoch 461/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.6506 - mae: 0.5876 - val_loss: 10.3520 - val_mae: 2.3308\n",
            "Epoch 462/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.6663 - mae: 0.6190 - val_loss: 11.2301 - val_mae: 2.3599\n",
            "Epoch 463/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.0077 - mae: 0.7566 - val_loss: 9.5676 - val_mae: 2.2530\n",
            "Epoch 464/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.6206 - mae: 0.5525 - val_loss: 10.3871 - val_mae: 2.2363\n",
            "Epoch 465/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.5978 - mae: 0.5808 - val_loss: 9.8827 - val_mae: 2.2942\n",
            "Epoch 466/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8351 - mae: 0.6412 - val_loss: 10.8393 - val_mae: 2.3586\n",
            "Epoch 467/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7077 - mae: 0.6013 - val_loss: 10.4453 - val_mae: 2.3268\n",
            "Epoch 468/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.5047 - mae: 0.5051 - val_loss: 10.2452 - val_mae: 2.2401\n",
            "Epoch 469/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.5119 - mae: 0.5188 - val_loss: 10.5557 - val_mae: 2.3115\n",
            "Epoch 470/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7033 - mae: 0.6309 - val_loss: 10.7409 - val_mae: 2.3461\n",
            "Epoch 471/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7417 - mae: 0.6531 - val_loss: 10.0767 - val_mae: 2.3104\n",
            "Epoch 472/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7278 - mae: 0.6279 - val_loss: 9.7667 - val_mae: 2.2831\n",
            "Epoch 473/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.6107 - mae: 0.5618 - val_loss: 10.5477 - val_mae: 2.3037\n",
            "Epoch 474/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8558 - mae: 0.6664 - val_loss: 10.4170 - val_mae: 2.3213\n",
            "Epoch 475/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.6967 - mae: 0.6085 - val_loss: 11.8595 - val_mae: 2.3761\n",
            "Epoch 476/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.6913 - mae: 0.6007 - val_loss: 10.6104 - val_mae: 2.3458\n",
            "Epoch 477/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.5058 - mae: 0.5216 - val_loss: 11.0703 - val_mae: 2.3443\n",
            "Epoch 478/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.0249 - mae: 0.7816 - val_loss: 9.6456 - val_mae: 2.2418\n",
            "Epoch 479/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8020 - mae: 0.6838 - val_loss: 9.8633 - val_mae: 2.1851\n",
            "Epoch 480/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.4969 - mae: 0.5151 - val_loss: 10.1903 - val_mae: 2.2544\n",
            "Epoch 481/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8689 - mae: 0.6832 - val_loss: 9.7509 - val_mae: 2.2243\n",
            "Epoch 482/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.6193 - mae: 0.5609 - val_loss: 9.6661 - val_mae: 2.1936\n",
            "Epoch 483/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.5430 - mae: 0.5335 - val_loss: 9.7934 - val_mae: 2.2641\n",
            "Epoch 484/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.6146 - mae: 0.6086 - val_loss: 11.1462 - val_mae: 2.4089\n",
            "Epoch 485/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.5957 - mae: 0.5231 - val_loss: 10.8696 - val_mae: 2.3113\n",
            "Epoch 486/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.6350 - mae: 0.6013 - val_loss: 10.4971 - val_mae: 2.3209\n",
            "Epoch 487/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8568 - mae: 0.6290 - val_loss: 10.7462 - val_mae: 2.3034\n",
            "Epoch 488/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7073 - mae: 0.5659 - val_loss: 10.2096 - val_mae: 2.3144\n",
            "Epoch 489/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.0713 - mae: 0.7668 - val_loss: 10.4068 - val_mae: 2.3500\n",
            "Epoch 490/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7700 - mae: 0.6478 - val_loss: 10.5968 - val_mae: 2.2964\n",
            "Epoch 491/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.8210 - mae: 0.6422 - val_loss: 9.9656 - val_mae: 2.2979\n",
            "Epoch 492/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7025 - mae: 0.5823 - val_loss: 10.0221 - val_mae: 2.2077\n",
            "Epoch 493/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.5778 - mae: 0.5418 - val_loss: 10.4362 - val_mae: 2.2676\n",
            "Epoch 494/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.4761 - mae: 0.5068 - val_loss: 10.0962 - val_mae: 2.3633\n",
            "Epoch 495/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.9902 - mae: 0.6983 - val_loss: 10.8823 - val_mae: 2.3154\n",
            "Epoch 496/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.7886 - mae: 0.6317 - val_loss: 10.2623 - val_mae: 2.2839\n",
            "Epoch 497/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.6424 - mae: 0.5800 - val_loss: 10.8599 - val_mae: 2.4149\n",
            "Epoch 498/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 1.3571 - mae: 0.8200 - val_loss: 11.8414 - val_mae: 2.3991\n",
            "Epoch 499/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.5621 - mae: 0.5714 - val_loss: 10.2902 - val_mae: 2.3519\n",
            "Epoch 500/500\n",
            "323/323 [==============================] - 1s 2ms/step - loss: 0.5204 - mae: 0.5190 - val_loss: 10.2589 - val_mae: 2.2838\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "tLf06mpM5S-U",
        "outputId": "947d574b-7174-466a-a22a-e0787907c076"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = range(1, len(Hist.history['loss'])+1)\n",
        "\n",
        "plt.plot(epochs, Hist.history['loss'])\n",
        "plt.plot(epochs, Hist.history['val_loss'])\n",
        "plt.legend(['loss','val_loss'])\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epochs, Hist.history['mae'])\n",
        "plt.plot(epochs, Hist.history['val_mae'])\n",
        "plt.legend(['mae','val_mae'])\n",
        "plt.show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ul8vpuWi69JA",
        "outputId": "4a14831e-c767-4243-81b0-cb6b1addd264"
      },
      "source": [
        "test_mse_score, test_mae_score = model.evaluate(X_test, y_test)\n",
        "test_mse_score, test_mae_score"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 3ms/step - loss: 19.0327 - mae: 2.7464\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19.03273582458496, 2.7464263439178467)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgrnjypm5qWN",
        "outputId": "f843c2c1-0506-4b84-c314-41f9ccd68aba"
      },
      "source": [
        "test_mse_score, test_mae_score = model.evaluate(X_test, y_test)\n",
        "test_mse_score, test_mae_score"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 3ms/step - loss: 20.9809 - mae: 2.8144\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20.98088836669922, 2.814417600631714)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "yoA62Fsn552q",
        "outputId": "2a031d98-35df-4d37-db3c-2b8f203ece6a"
      },
      "source": [
        "def smooth_curve(points, factor = 0.9):\n",
        "  smoothed_points=[]\n",
        "  for point in points:\n",
        "    if smoothed_points:\n",
        "      previous=smoothed_points[-1]\n",
        "      smoothed_points.append(previous*factor + point * (1-factor))\n",
        "    else:\n",
        "      smoothed_points.append(point)\n",
        "  return smoothed_points\n",
        "\n",
        "mae_history = Hist.history['val_mae']\n",
        "mae_history = smooth_curve(mae_history[5:])\n",
        "\n",
        "plt.figure(figsize = (9, 6))\n",
        "plt.plot(range(1, len(mae_history) + 1), mae_history)\n",
        "plt.title('Validation MAE')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Mean Absolute Error')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 648x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "626fSMOd8Il_"
      },
      "source": [
        "from keras import backend as K\n",
        "K.clear_session()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79juNh5-8UAQ"
      },
      "source": [
        "\n",
        "model = models.Sequential(name = 'EarlyStopping')\n",
        "model.add(layers.Dense(65, activation='relu', input_shape=(13,)))\n",
        "model.add(layers.Dense(35, activation='relu'))\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(loss='mse', optimizer='Adam', metrics=['mae'])"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oDH63XW9F5P"
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "es = EarlyStopping(monitor = 'val_mae', mode='min', patience=100, verbose=1)\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "mc = ModelCheckpoint('best_boston.h5', monitor='val_mae', mode='min', save_best_only=True, verbose=1)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppJLbZ9V8veI",
        "outputId": "fa20ab96-8c8d-4d24-c141-f45310ec173d"
      },
      "source": [
        "%%time\n",
        "Hist = model.fit(X_train, y_train, epochs=500, batch_size=200, \n",
        "                 callbacks=[es,mc], verbose=1,\n",
        "                 validation_data=(X_test, y_test))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "2/2 [==============================] - 0s 100ms/step - loss: 17.3011 - mae: 3.0012 - val_loss: 25.2014 - val_mae: 3.8115\n",
            "\n",
            "Epoch 00001: val_mae improved from inf to 3.81153, saving model to best_boston.h5\n",
            "Epoch 2/500\n",
            "2/2 [==============================] - 0s 98ms/step - loss: 17.1972 - mae: 2.9924 - val_loss: 25.1160 - val_mae: 3.8028\n",
            "\n",
            "Epoch 00002: val_mae improved from 3.81153 to 3.80280, saving model to best_boston.h5\n",
            "Epoch 3/500\n",
            "2/2 [==============================] - 0s 105ms/step - loss: 17.1081 - mae: 2.9850 - val_loss: 25.0254 - val_mae: 3.7937\n",
            "\n",
            "Epoch 00003: val_mae improved from 3.80280 to 3.79369, saving model to best_boston.h5\n",
            "Epoch 4/500\n",
            "2/2 [==============================] - 0s 98ms/step - loss: 17.0154 - mae: 2.9764 - val_loss: 24.9444 - val_mae: 3.7848\n",
            "\n",
            "Epoch 00004: val_mae improved from 3.79369 to 3.78484, saving model to best_boston.h5\n",
            "Epoch 5/500\n",
            "2/2 [==============================] - 0s 95ms/step - loss: 16.9197 - mae: 2.9672 - val_loss: 24.8460 - val_mae: 3.7744\n",
            "\n",
            "Epoch 00005: val_mae improved from 3.78484 to 3.77437, saving model to best_boston.h5\n",
            "Epoch 6/500\n",
            "2/2 [==============================] - 0s 123ms/step - loss: 16.8233 - mae: 2.9582 - val_loss: 24.7641 - val_mae: 3.7653\n",
            "\n",
            "Epoch 00006: val_mae improved from 3.77437 to 3.76535, saving model to best_boston.h5\n",
            "Epoch 7/500\n",
            "2/2 [==============================] - 0s 104ms/step - loss: 16.7373 - mae: 2.9499 - val_loss: 24.6858 - val_mae: 3.7566\n",
            "\n",
            "Epoch 00007: val_mae improved from 3.76535 to 3.75658, saving model to best_boston.h5\n",
            "Epoch 8/500\n",
            "2/2 [==============================] - 0s 103ms/step - loss: 16.6529 - mae: 2.9426 - val_loss: 24.6136 - val_mae: 3.7484\n",
            "\n",
            "Epoch 00008: val_mae improved from 3.75658 to 3.74844, saving model to best_boston.h5\n",
            "Epoch 9/500\n",
            "2/2 [==============================] - 0s 103ms/step - loss: 16.5552 - mae: 2.9339 - val_loss: 24.5617 - val_mae: 3.7413\n",
            "\n",
            "Epoch 00009: val_mae improved from 3.74844 to 3.74125, saving model to best_boston.h5\n",
            "Epoch 10/500\n",
            "2/2 [==============================] - 0s 92ms/step - loss: 16.4692 - mae: 2.9271 - val_loss: 24.5161 - val_mae: 3.7339\n",
            "\n",
            "Epoch 00010: val_mae improved from 3.74125 to 3.73386, saving model to best_boston.h5\n",
            "Epoch 11/500\n",
            "2/2 [==============================] - 0s 101ms/step - loss: 16.3819 - mae: 2.9193 - val_loss: 24.4714 - val_mae: 3.7266\n",
            "\n",
            "Epoch 00011: val_mae improved from 3.73386 to 3.72659, saving model to best_boston.h5\n",
            "Epoch 12/500\n",
            "2/2 [==============================] - 0s 91ms/step - loss: 16.3025 - mae: 2.9122 - val_loss: 24.4128 - val_mae: 3.7192\n",
            "\n",
            "Epoch 00012: val_mae improved from 3.72659 to 3.71920, saving model to best_boston.h5\n",
            "Epoch 13/500\n",
            "2/2 [==============================] - 0s 96ms/step - loss: 16.2070 - mae: 2.9042 - val_loss: 24.3416 - val_mae: 3.7109\n",
            "\n",
            "Epoch 00013: val_mae improved from 3.71920 to 3.71086, saving model to best_boston.h5\n",
            "Epoch 14/500\n",
            "2/2 [==============================] - 0s 98ms/step - loss: 16.1269 - mae: 2.8966 - val_loss: 24.2818 - val_mae: 3.7037\n",
            "\n",
            "Epoch 00014: val_mae improved from 3.71086 to 3.70372, saving model to best_boston.h5\n",
            "Epoch 15/500\n",
            "2/2 [==============================] - 0s 99ms/step - loss: 16.0370 - mae: 2.8879 - val_loss: 24.2204 - val_mae: 3.6957\n",
            "\n",
            "Epoch 00015: val_mae improved from 3.70372 to 3.69575, saving model to best_boston.h5\n",
            "Epoch 16/500\n",
            "2/2 [==============================] - 0s 99ms/step - loss: 15.9529 - mae: 2.8793 - val_loss: 24.1732 - val_mae: 3.6893\n",
            "\n",
            "Epoch 00016: val_mae improved from 3.69575 to 3.68926, saving model to best_boston.h5\n",
            "Epoch 17/500\n",
            "2/2 [==============================] - 0s 96ms/step - loss: 15.8762 - mae: 2.8718 - val_loss: 24.1209 - val_mae: 3.6823\n",
            "\n",
            "Epoch 00017: val_mae improved from 3.68926 to 3.68227, saving model to best_boston.h5\n",
            "Epoch 18/500\n",
            "2/2 [==============================] - 0s 93ms/step - loss: 15.7942 - mae: 2.8634 - val_loss: 24.0663 - val_mae: 3.6751\n",
            "\n",
            "Epoch 00018: val_mae improved from 3.68227 to 3.67508, saving model to best_boston.h5\n",
            "Epoch 19/500\n",
            "2/2 [==============================] - 0s 105ms/step - loss: 15.7130 - mae: 2.8557 - val_loss: 24.0092 - val_mae: 3.6685\n",
            "\n",
            "Epoch 00019: val_mae improved from 3.67508 to 3.66850, saving model to best_boston.h5\n",
            "Epoch 20/500\n",
            "2/2 [==============================] - 0s 99ms/step - loss: 15.6310 - mae: 2.8482 - val_loss: 23.9530 - val_mae: 3.6621\n",
            "\n",
            "Epoch 00020: val_mae improved from 3.66850 to 3.66211, saving model to best_boston.h5\n",
            "Epoch 21/500\n",
            "2/2 [==============================] - 0s 103ms/step - loss: 15.5592 - mae: 2.8405 - val_loss: 23.8909 - val_mae: 3.6548\n",
            "\n",
            "Epoch 00021: val_mae improved from 3.66211 to 3.65479, saving model to best_boston.h5\n",
            "Epoch 22/500\n",
            "2/2 [==============================] - 0s 93ms/step - loss: 15.4762 - mae: 2.8325 - val_loss: 23.8144 - val_mae: 3.6460\n",
            "\n",
            "Epoch 00022: val_mae improved from 3.65479 to 3.64601, saving model to best_boston.h5\n",
            "Epoch 23/500\n",
            "2/2 [==============================] - 0s 101ms/step - loss: 15.4056 - mae: 2.8257 - val_loss: 23.7693 - val_mae: 3.6402\n",
            "\n",
            "Epoch 00023: val_mae improved from 3.64601 to 3.64018, saving model to best_boston.h5\n",
            "Epoch 24/500\n",
            "2/2 [==============================] - 0s 116ms/step - loss: 15.3237 - mae: 2.8178 - val_loss: 23.7196 - val_mae: 3.6335\n",
            "\n",
            "Epoch 00024: val_mae improved from 3.64018 to 3.63355, saving model to best_boston.h5\n",
            "Epoch 25/500\n",
            "2/2 [==============================] - 0s 103ms/step - loss: 15.2463 - mae: 2.8106 - val_loss: 23.6833 - val_mae: 3.6281\n",
            "\n",
            "Epoch 00025: val_mae improved from 3.63355 to 3.62805, saving model to best_boston.h5\n",
            "Epoch 26/500\n",
            "2/2 [==============================] - 0s 103ms/step - loss: 15.1760 - mae: 2.8037 - val_loss: 23.6546 - val_mae: 3.6236\n",
            "\n",
            "Epoch 00026: val_mae improved from 3.62805 to 3.62361, saving model to best_boston.h5\n",
            "Epoch 27/500\n",
            "2/2 [==============================] - 0s 106ms/step - loss: 15.1028 - mae: 2.7983 - val_loss: 23.6299 - val_mae: 3.6196\n",
            "\n",
            "Epoch 00027: val_mae improved from 3.62361 to 3.61955, saving model to best_boston.h5\n",
            "Epoch 28/500\n",
            "2/2 [==============================] - 0s 98ms/step - loss: 15.0400 - mae: 2.7925 - val_loss: 23.6160 - val_mae: 3.6162\n",
            "\n",
            "Epoch 00028: val_mae improved from 3.61955 to 3.61620, saving model to best_boston.h5\n",
            "Epoch 29/500\n",
            "2/2 [==============================] - 0s 98ms/step - loss: 14.9618 - mae: 2.7865 - val_loss: 23.5697 - val_mae: 3.6105\n",
            "\n",
            "Epoch 00029: val_mae improved from 3.61620 to 3.61053, saving model to best_boston.h5\n",
            "Epoch 30/500\n",
            "2/2 [==============================] - 0s 104ms/step - loss: 14.8910 - mae: 2.7799 - val_loss: 23.5135 - val_mae: 3.6034\n",
            "\n",
            "Epoch 00030: val_mae improved from 3.61053 to 3.60343, saving model to best_boston.h5\n",
            "Epoch 31/500\n",
            "2/2 [==============================] - 0s 101ms/step - loss: 14.8233 - mae: 2.7737 - val_loss: 23.4737 - val_mae: 3.5976\n",
            "\n",
            "Epoch 00031: val_mae improved from 3.60343 to 3.59764, saving model to best_boston.h5\n",
            "Epoch 32/500\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 14.7547 - mae: 2.7678 - val_loss: 23.4327 - val_mae: 3.5916\n",
            "\n",
            "Epoch 00032: val_mae improved from 3.59764 to 3.59160, saving model to best_boston.h5\n",
            "Epoch 33/500\n",
            "2/2 [==============================] - 0s 97ms/step - loss: 14.6904 - mae: 2.7617 - val_loss: 23.3912 - val_mae: 3.5848\n",
            "\n",
            "Epoch 00033: val_mae improved from 3.59160 to 3.58480, saving model to best_boston.h5\n",
            "Epoch 34/500\n",
            "2/2 [==============================] - 0s 106ms/step - loss: 14.6283 - mae: 2.7562 - val_loss: 23.3487 - val_mae: 3.5791\n",
            "\n",
            "Epoch 00034: val_mae improved from 3.58480 to 3.57911, saving model to best_boston.h5\n",
            "Epoch 35/500\n",
            "2/2 [==============================] - 0s 93ms/step - loss: 14.5586 - mae: 2.7493 - val_loss: 23.2913 - val_mae: 3.5717\n",
            "\n",
            "Epoch 00035: val_mae improved from 3.57911 to 3.57174, saving model to best_boston.h5\n",
            "Epoch 36/500\n",
            "2/2 [==============================] - 0s 104ms/step - loss: 14.4946 - mae: 2.7421 - val_loss: 23.2318 - val_mae: 3.5640\n",
            "\n",
            "Epoch 00036: val_mae improved from 3.57174 to 3.56403, saving model to best_boston.h5\n",
            "Epoch 37/500\n",
            "2/2 [==============================] - 0s 96ms/step - loss: 14.4278 - mae: 2.7353 - val_loss: 23.1729 - val_mae: 3.5568\n",
            "\n",
            "Epoch 00037: val_mae improved from 3.56403 to 3.55682, saving model to best_boston.h5\n",
            "Epoch 38/500\n",
            "2/2 [==============================] - 0s 102ms/step - loss: 14.3728 - mae: 2.7306 - val_loss: 23.1143 - val_mae: 3.5504\n",
            "\n",
            "Epoch 00038: val_mae improved from 3.55682 to 3.55038, saving model to best_boston.h5\n",
            "Epoch 39/500\n",
            "2/2 [==============================] - 0s 101ms/step - loss: 14.3078 - mae: 2.7259 - val_loss: 23.0515 - val_mae: 3.5431\n",
            "\n",
            "Epoch 00039: val_mae improved from 3.55038 to 3.54307, saving model to best_boston.h5\n",
            "Epoch 40/500\n",
            "2/2 [==============================] - 0s 109ms/step - loss: 14.2541 - mae: 2.7210 - val_loss: 22.9889 - val_mae: 3.5362\n",
            "\n",
            "Epoch 00040: val_mae improved from 3.54307 to 3.53619, saving model to best_boston.h5\n",
            "Epoch 41/500\n",
            "2/2 [==============================] - 0s 98ms/step - loss: 14.1861 - mae: 2.7157 - val_loss: 22.9240 - val_mae: 3.5285\n",
            "\n",
            "Epoch 00041: val_mae improved from 3.53619 to 3.52852, saving model to best_boston.h5\n",
            "Epoch 42/500\n",
            "2/2 [==============================] - 0s 98ms/step - loss: 14.1257 - mae: 2.7095 - val_loss: 22.8553 - val_mae: 3.5203\n",
            "\n",
            "Epoch 00042: val_mae improved from 3.52852 to 3.52030, saving model to best_boston.h5\n",
            "Epoch 43/500\n",
            "2/2 [==============================] - 0s 100ms/step - loss: 14.0744 - mae: 2.7047 - val_loss: 22.7968 - val_mae: 3.5136\n",
            "\n",
            "Epoch 00043: val_mae improved from 3.52030 to 3.51356, saving model to best_boston.h5\n",
            "Epoch 44/500\n",
            "2/2 [==============================] - 0s 108ms/step - loss: 14.0127 - mae: 2.6989 - val_loss: 22.7280 - val_mae: 3.5056\n",
            "\n",
            "Epoch 00044: val_mae improved from 3.51356 to 3.50556, saving model to best_boston.h5\n",
            "Epoch 45/500\n",
            "2/2 [==============================] - 0s 108ms/step - loss: 13.9661 - mae: 2.6941 - val_loss: 22.6789 - val_mae: 3.4993\n",
            "\n",
            "Epoch 00045: val_mae improved from 3.50556 to 3.49934, saving model to best_boston.h5\n",
            "Epoch 46/500\n",
            "2/2 [==============================] - 0s 99ms/step - loss: 13.9059 - mae: 2.6882 - val_loss: 22.6305 - val_mae: 3.4928\n",
            "\n",
            "Epoch 00046: val_mae improved from 3.49934 to 3.49276, saving model to best_boston.h5\n",
            "Epoch 47/500\n",
            "2/2 [==============================] - 0s 107ms/step - loss: 13.8498 - mae: 2.6824 - val_loss: 22.5868 - val_mae: 3.4868\n",
            "\n",
            "Epoch 00047: val_mae improved from 3.49276 to 3.48680, saving model to best_boston.h5\n",
            "Epoch 48/500\n",
            "2/2 [==============================] - 0s 114ms/step - loss: 13.7993 - mae: 2.6775 - val_loss: 22.5726 - val_mae: 3.4838\n",
            "\n",
            "Epoch 00048: val_mae improved from 3.48680 to 3.48384, saving model to best_boston.h5\n",
            "Epoch 49/500\n",
            "2/2 [==============================] - 0s 106ms/step - loss: 13.7436 - mae: 2.6719 - val_loss: 22.5417 - val_mae: 3.4795\n",
            "\n",
            "Epoch 00049: val_mae improved from 3.48384 to 3.47954, saving model to best_boston.h5\n",
            "Epoch 50/500\n",
            "2/2 [==============================] - 0s 106ms/step - loss: 13.6951 - mae: 2.6678 - val_loss: 22.5296 - val_mae: 3.4772\n",
            "\n",
            "Epoch 00050: val_mae improved from 3.47954 to 3.47725, saving model to best_boston.h5\n",
            "Epoch 51/500\n",
            "2/2 [==============================] - 0s 111ms/step - loss: 13.6325 - mae: 2.6630 - val_loss: 22.5011 - val_mae: 3.4734\n",
            "\n",
            "Epoch 00051: val_mae improved from 3.47725 to 3.47341, saving model to best_boston.h5\n",
            "Epoch 52/500\n",
            "2/2 [==============================] - 0s 109ms/step - loss: 13.5831 - mae: 2.6600 - val_loss: 22.4818 - val_mae: 3.4707\n",
            "\n",
            "Epoch 00052: val_mae improved from 3.47341 to 3.47068, saving model to best_boston.h5\n",
            "Epoch 53/500\n",
            "2/2 [==============================] - 0s 101ms/step - loss: 13.5362 - mae: 2.6564 - val_loss: 22.4683 - val_mae: 3.4683\n",
            "\n",
            "Epoch 00053: val_mae improved from 3.47068 to 3.46830, saving model to best_boston.h5\n",
            "Epoch 54/500\n",
            "2/2 [==============================] - 0s 102ms/step - loss: 13.4713 - mae: 2.6516 - val_loss: 22.4297 - val_mae: 3.4637\n",
            "\n",
            "Epoch 00054: val_mae improved from 3.46830 to 3.46369, saving model to best_boston.h5\n",
            "Epoch 55/500\n",
            "2/2 [==============================] - 0s 106ms/step - loss: 13.4244 - mae: 2.6479 - val_loss: 22.4020 - val_mae: 3.4599\n",
            "\n",
            "Epoch 00055: val_mae improved from 3.46369 to 3.45991, saving model to best_boston.h5\n",
            "Epoch 56/500\n",
            "2/2 [==============================] - 0s 105ms/step - loss: 13.3753 - mae: 2.6445 - val_loss: 22.3902 - val_mae: 3.4574\n",
            "\n",
            "Epoch 00056: val_mae improved from 3.45991 to 3.45743, saving model to best_boston.h5\n",
            "Epoch 57/500\n",
            "2/2 [==============================] - 0s 103ms/step - loss: 13.3252 - mae: 2.6405 - val_loss: 22.3610 - val_mae: 3.4534\n",
            "\n",
            "Epoch 00057: val_mae improved from 3.45743 to 3.45339, saving model to best_boston.h5\n",
            "Epoch 58/500\n",
            "2/2 [==============================] - 0s 102ms/step - loss: 13.2729 - mae: 2.6365 - val_loss: 22.3219 - val_mae: 3.4479\n",
            "\n",
            "Epoch 00058: val_mae improved from 3.45339 to 3.44790, saving model to best_boston.h5\n",
            "Epoch 59/500\n",
            "2/2 [==============================] - 0s 106ms/step - loss: 13.2325 - mae: 2.6330 - val_loss: 22.2763 - val_mae: 3.4425\n",
            "\n",
            "Epoch 00059: val_mae improved from 3.44790 to 3.44248, saving model to best_boston.h5\n",
            "Epoch 60/500\n",
            "2/2 [==============================] - 0s 108ms/step - loss: 13.1792 - mae: 2.6289 - val_loss: 22.2342 - val_mae: 3.4372\n",
            "\n",
            "Epoch 00060: val_mae improved from 3.44248 to 3.43715, saving model to best_boston.h5\n",
            "Epoch 61/500\n",
            "2/2 [==============================] - 0s 108ms/step - loss: 13.1338 - mae: 2.6241 - val_loss: 22.2133 - val_mae: 3.4338\n",
            "\n",
            "Epoch 00061: val_mae improved from 3.43715 to 3.43381, saving model to best_boston.h5\n",
            "Epoch 62/500\n",
            "2/2 [==============================] - 0s 97ms/step - loss: 13.0833 - mae: 2.6192 - val_loss: 22.1895 - val_mae: 3.4302\n",
            "\n",
            "Epoch 00062: val_mae improved from 3.43381 to 3.43025, saving model to best_boston.h5\n",
            "Epoch 63/500\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 13.0465 - mae: 2.6148 - val_loss: 22.1846 - val_mae: 3.4282\n",
            "\n",
            "Epoch 00063: val_mae improved from 3.43025 to 3.42822, saving model to best_boston.h5\n",
            "Epoch 64/500\n",
            "2/2 [==============================] - 0s 98ms/step - loss: 12.9945 - mae: 2.6100 - val_loss: 22.1455 - val_mae: 3.4234\n",
            "\n",
            "Epoch 00064: val_mae improved from 3.42822 to 3.42338, saving model to best_boston.h5\n",
            "Epoch 65/500\n",
            "2/2 [==============================] - 0s 96ms/step - loss: 12.9446 - mae: 2.6049 - val_loss: 22.0942 - val_mae: 3.4173\n",
            "\n",
            "Epoch 00065: val_mae improved from 3.42338 to 3.41731, saving model to best_boston.h5\n",
            "Epoch 66/500\n",
            "2/2 [==============================] - 0s 99ms/step - loss: 12.9026 - mae: 2.5999 - val_loss: 22.0645 - val_mae: 3.4134\n",
            "\n",
            "Epoch 00066: val_mae improved from 3.41731 to 3.41342, saving model to best_boston.h5\n",
            "Epoch 67/500\n",
            "2/2 [==============================] - 0s 103ms/step - loss: 12.8558 - mae: 2.5951 - val_loss: 22.0416 - val_mae: 3.4096\n",
            "\n",
            "Epoch 00067: val_mae improved from 3.41342 to 3.40964, saving model to best_boston.h5\n",
            "Epoch 68/500\n",
            "2/2 [==============================] - 0s 102ms/step - loss: 12.8101 - mae: 2.5897 - val_loss: 22.0175 - val_mae: 3.4060\n",
            "\n",
            "Epoch 00068: val_mae improved from 3.40964 to 3.40599, saving model to best_boston.h5\n",
            "Epoch 69/500\n",
            "2/2 [==============================] - 0s 100ms/step - loss: 12.7658 - mae: 2.5855 - val_loss: 22.0063 - val_mae: 3.4031\n",
            "\n",
            "Epoch 00069: val_mae improved from 3.40599 to 3.40308, saving model to best_boston.h5\n",
            "Epoch 70/500\n",
            "2/2 [==============================] - 0s 99ms/step - loss: 12.7237 - mae: 2.5812 - val_loss: 22.0010 - val_mae: 3.4008\n",
            "\n",
            "Epoch 00070: val_mae improved from 3.40308 to 3.40084, saving model to best_boston.h5\n",
            "Epoch 71/500\n",
            "2/2 [==============================] - 0s 105ms/step - loss: 12.6741 - mae: 2.5763 - val_loss: 21.9807 - val_mae: 3.3973\n",
            "\n",
            "Epoch 00071: val_mae improved from 3.40084 to 3.39730, saving model to best_boston.h5\n",
            "Epoch 72/500\n",
            "2/2 [==============================] - 0s 95ms/step - loss: 12.6351 - mae: 2.5728 - val_loss: 21.9585 - val_mae: 3.3936\n",
            "\n",
            "Epoch 00072: val_mae improved from 3.39730 to 3.39361, saving model to best_boston.h5\n",
            "Epoch 73/500\n",
            "2/2 [==============================] - 0s 103ms/step - loss: 12.5916 - mae: 2.5681 - val_loss: 21.9332 - val_mae: 3.3897\n",
            "\n",
            "Epoch 00073: val_mae improved from 3.39361 to 3.38972, saving model to best_boston.h5\n",
            "Epoch 74/500\n",
            "2/2 [==============================] - 0s 106ms/step - loss: 12.5462 - mae: 2.5627 - val_loss: 21.9372 - val_mae: 3.3884\n",
            "\n",
            "Epoch 00074: val_mae improved from 3.38972 to 3.38836, saving model to best_boston.h5\n",
            "Epoch 75/500\n",
            "2/2 [==============================] - 0s 101ms/step - loss: 12.5071 - mae: 2.5579 - val_loss: 21.9575 - val_mae: 3.3882\n",
            "\n",
            "Epoch 00075: val_mae improved from 3.38836 to 3.38815, saving model to best_boston.h5\n",
            "Epoch 76/500\n",
            "2/2 [==============================] - 0s 114ms/step - loss: 12.4583 - mae: 2.5532 - val_loss: 21.9580 - val_mae: 3.3863\n",
            "\n",
            "Epoch 00076: val_mae improved from 3.38815 to 3.38630, saving model to best_boston.h5\n",
            "Epoch 77/500\n",
            "2/2 [==============================] - 0s 106ms/step - loss: 12.4174 - mae: 2.5485 - val_loss: 21.9573 - val_mae: 3.3847\n",
            "\n",
            "Epoch 00077: val_mae improved from 3.38630 to 3.38470, saving model to best_boston.h5\n",
            "Epoch 78/500\n",
            "2/2 [==============================] - 0s 94ms/step - loss: 12.3733 - mae: 2.5441 - val_loss: 21.9839 - val_mae: 3.3855\n",
            "\n",
            "Epoch 00078: val_mae did not improve from 3.38470\n",
            "Epoch 79/500\n",
            "2/2 [==============================] - 0s 103ms/step - loss: 12.3413 - mae: 2.5410 - val_loss: 21.9974 - val_mae: 3.3855\n",
            "\n",
            "Epoch 00079: val_mae did not improve from 3.38470\n",
            "Epoch 80/500\n",
            "2/2 [==============================] - 0s 98ms/step - loss: 12.2946 - mae: 2.5368 - val_loss: 21.9837 - val_mae: 3.3825\n",
            "\n",
            "Epoch 00080: val_mae improved from 3.38470 to 3.38248, saving model to best_boston.h5\n",
            "Epoch 81/500\n",
            "2/2 [==============================] - 0s 96ms/step - loss: 12.2465 - mae: 2.5316 - val_loss: 21.9410 - val_mae: 3.3764\n",
            "\n",
            "Epoch 00081: val_mae improved from 3.38248 to 3.37641, saving model to best_boston.h5\n",
            "Epoch 82/500\n",
            "2/2 [==============================] - 0s 102ms/step - loss: 12.2094 - mae: 2.5276 - val_loss: 21.8874 - val_mae: 3.3686\n",
            "\n",
            "Epoch 00082: val_mae improved from 3.37641 to 3.36865, saving model to best_boston.h5\n",
            "Epoch 83/500\n",
            "2/2 [==============================] - 0s 99ms/step - loss: 12.1616 - mae: 2.5221 - val_loss: 21.8449 - val_mae: 3.3620\n",
            "\n",
            "Epoch 00083: val_mae improved from 3.36865 to 3.36205, saving model to best_boston.h5\n",
            "Epoch 84/500\n",
            "2/2 [==============================] - 0s 97ms/step - loss: 12.1220 - mae: 2.5171 - val_loss: 21.8073 - val_mae: 3.3558\n",
            "\n",
            "Epoch 00084: val_mae improved from 3.36205 to 3.35577, saving model to best_boston.h5\n",
            "Epoch 85/500\n",
            "2/2 [==============================] - 0s 98ms/step - loss: 12.0853 - mae: 2.5124 - val_loss: 21.7755 - val_mae: 3.3504\n",
            "\n",
            "Epoch 00085: val_mae improved from 3.35577 to 3.35045, saving model to best_boston.h5\n",
            "Epoch 86/500\n",
            "2/2 [==============================] - 0s 96ms/step - loss: 12.0423 - mae: 2.5081 - val_loss: 21.7650 - val_mae: 3.3468\n",
            "\n",
            "Epoch 00086: val_mae improved from 3.35045 to 3.34678, saving model to best_boston.h5\n",
            "Epoch 87/500\n",
            "2/2 [==============================] - 0s 106ms/step - loss: 12.0033 - mae: 2.5050 - val_loss: 21.7662 - val_mae: 3.3444\n",
            "\n",
            "Epoch 00087: val_mae improved from 3.34678 to 3.34444, saving model to best_boston.h5\n",
            "Epoch 88/500\n",
            "2/2 [==============================] - 0s 98ms/step - loss: 11.9631 - mae: 2.5007 - val_loss: 21.7506 - val_mae: 3.3401\n",
            "\n",
            "Epoch 00088: val_mae improved from 3.34444 to 3.34012, saving model to best_boston.h5\n",
            "Epoch 89/500\n",
            "2/2 [==============================] - 0s 106ms/step - loss: 11.9207 - mae: 2.4969 - val_loss: 21.7560 - val_mae: 3.3387\n",
            "\n",
            "Epoch 00089: val_mae improved from 3.34012 to 3.33873, saving model to best_boston.h5\n",
            "Epoch 90/500\n",
            "2/2 [==============================] - 0s 95ms/step - loss: 11.8803 - mae: 2.4939 - val_loss: 21.7707 - val_mae: 3.3379\n",
            "\n",
            "Epoch 00090: val_mae improved from 3.33873 to 3.33794, saving model to best_boston.h5\n",
            "Epoch 91/500\n",
            "2/2 [==============================] - 0s 98ms/step - loss: 11.8411 - mae: 2.4905 - val_loss: 21.7808 - val_mae: 3.3366\n",
            "\n",
            "Epoch 00091: val_mae improved from 3.33794 to 3.33657, saving model to best_boston.h5\n",
            "Epoch 92/500\n",
            "2/2 [==============================] - 0s 97ms/step - loss: 11.7959 - mae: 2.4861 - val_loss: 21.7910 - val_mae: 3.3349\n",
            "\n",
            "Epoch 00092: val_mae improved from 3.33657 to 3.33494, saving model to best_boston.h5\n",
            "Epoch 93/500\n",
            "2/2 [==============================] - 0s 107ms/step - loss: 11.7580 - mae: 2.4820 - val_loss: 21.7908 - val_mae: 3.3326\n",
            "\n",
            "Epoch 00093: val_mae improved from 3.33494 to 3.33258, saving model to best_boston.h5\n",
            "Epoch 94/500\n",
            "2/2 [==============================] - 0s 104ms/step - loss: 11.7170 - mae: 2.4772 - val_loss: 21.7949 - val_mae: 3.3317\n",
            "\n",
            "Epoch 00094: val_mae improved from 3.33258 to 3.33166, saving model to best_boston.h5\n",
            "Epoch 95/500\n",
            "2/2 [==============================] - 0s 95ms/step - loss: 11.6875 - mae: 2.4752 - val_loss: 21.7953 - val_mae: 3.3301\n",
            "\n",
            "Epoch 00095: val_mae improved from 3.33166 to 3.33010, saving model to best_boston.h5\n",
            "Epoch 96/500\n",
            "2/2 [==============================] - 0s 105ms/step - loss: 11.6352 - mae: 2.4698 - val_loss: 21.7549 - val_mae: 3.3246\n",
            "\n",
            "Epoch 00096: val_mae improved from 3.33010 to 3.32461, saving model to best_boston.h5\n",
            "Epoch 97/500\n",
            "2/2 [==============================] - 0s 96ms/step - loss: 11.5962 - mae: 2.4658 - val_loss: 21.6966 - val_mae: 3.3178\n",
            "\n",
            "Epoch 00097: val_mae improved from 3.32461 to 3.31775, saving model to best_boston.h5\n",
            "Epoch 98/500\n",
            "2/2 [==============================] - 0s 100ms/step - loss: 11.5541 - mae: 2.4602 - val_loss: 21.6539 - val_mae: 3.3120\n",
            "\n",
            "Epoch 00098: val_mae improved from 3.31775 to 3.31196, saving model to best_boston.h5\n",
            "Epoch 99/500\n",
            "2/2 [==============================] - 0s 97ms/step - loss: 11.5151 - mae: 2.4553 - val_loss: 21.6308 - val_mae: 3.3076\n",
            "\n",
            "Epoch 00099: val_mae improved from 3.31196 to 3.30759, saving model to best_boston.h5\n",
            "Epoch 100/500\n",
            "2/2 [==============================] - 0s 103ms/step - loss: 11.4736 - mae: 2.4503 - val_loss: 21.6186 - val_mae: 3.3048\n",
            "\n",
            "Epoch 00100: val_mae improved from 3.30759 to 3.30480, saving model to best_boston.h5\n",
            "Epoch 101/500\n",
            "2/2 [==============================] - 0s 107ms/step - loss: 11.4409 - mae: 2.4459 - val_loss: 21.5950 - val_mae: 3.3010\n",
            "\n",
            "Epoch 00101: val_mae improved from 3.30480 to 3.30098, saving model to best_boston.h5\n",
            "Epoch 102/500\n",
            "2/2 [==============================] - 0s 96ms/step - loss: 11.4016 - mae: 2.4406 - val_loss: 21.5942 - val_mae: 3.2995\n",
            "\n",
            "Epoch 00102: val_mae improved from 3.30098 to 3.29951, saving model to best_boston.h5\n",
            "Epoch 103/500\n",
            "2/2 [==============================] - 0s 101ms/step - loss: 11.3668 - mae: 2.4371 - val_loss: 21.5904 - val_mae: 3.2973\n",
            "\n",
            "Epoch 00103: val_mae improved from 3.29951 to 3.29730, saving model to best_boston.h5\n",
            "Epoch 104/500\n",
            "2/2 [==============================] - 0s 99ms/step - loss: 11.3353 - mae: 2.4340 - val_loss: 21.6060 - val_mae: 3.2971\n",
            "\n",
            "Epoch 00104: val_mae improved from 3.29730 to 3.29711, saving model to best_boston.h5\n",
            "Epoch 105/500\n",
            "2/2 [==============================] - 0s 107ms/step - loss: 11.2898 - mae: 2.4297 - val_loss: 21.5828 - val_mae: 3.2934\n",
            "\n",
            "Epoch 00105: val_mae improved from 3.29711 to 3.29344, saving model to best_boston.h5\n",
            "Epoch 106/500\n",
            "2/2 [==============================] - 0s 101ms/step - loss: 11.2570 - mae: 2.4260 - val_loss: 21.5617 - val_mae: 3.2905\n",
            "\n",
            "Epoch 00106: val_mae improved from 3.29344 to 3.29046, saving model to best_boston.h5\n",
            "Epoch 107/500\n",
            "2/2 [==============================] - 0s 104ms/step - loss: 11.2211 - mae: 2.4215 - val_loss: 21.5228 - val_mae: 3.2856\n",
            "\n",
            "Epoch 00107: val_mae improved from 3.29046 to 3.28555, saving model to best_boston.h5\n",
            "Epoch 108/500\n",
            "2/2 [==============================] - 0s 98ms/step - loss: 11.1802 - mae: 2.4177 - val_loss: 21.5226 - val_mae: 3.2849\n",
            "\n",
            "Epoch 00108: val_mae improved from 3.28555 to 3.28486, saving model to best_boston.h5\n",
            "Epoch 109/500\n",
            "2/2 [==============================] - 0s 116ms/step - loss: 11.1477 - mae: 2.4156 - val_loss: 21.5392 - val_mae: 3.2857\n",
            "\n",
            "Epoch 00109: val_mae did not improve from 3.28486\n",
            "Epoch 110/500\n",
            "2/2 [==============================] - 0s 102ms/step - loss: 11.1103 - mae: 2.4129 - val_loss: 21.5366 - val_mae: 3.2844\n",
            "\n",
            "Epoch 00110: val_mae improved from 3.28486 to 3.28437, saving model to best_boston.h5\n",
            "Epoch 111/500\n",
            "2/2 [==============================] - 0s 103ms/step - loss: 11.0731 - mae: 2.4098 - val_loss: 21.5363 - val_mae: 3.2837\n",
            "\n",
            "Epoch 00111: val_mae improved from 3.28437 to 3.28370, saving model to best_boston.h5\n",
            "Epoch 112/500\n",
            "2/2 [==============================] - 0s 101ms/step - loss: 11.0395 - mae: 2.4073 - val_loss: 21.5370 - val_mae: 3.2827\n",
            "\n",
            "Epoch 00112: val_mae improved from 3.28370 to 3.28273, saving model to best_boston.h5\n",
            "Epoch 113/500\n",
            "2/2 [==============================] - 0s 107ms/step - loss: 11.0079 - mae: 2.4045 - val_loss: 21.5223 - val_mae: 3.2805\n",
            "\n",
            "Epoch 00113: val_mae improved from 3.28273 to 3.28050, saving model to best_boston.h5\n",
            "Epoch 114/500\n",
            "2/2 [==============================] - 0s 103ms/step - loss: 10.9771 - mae: 2.4015 - val_loss: 21.5160 - val_mae: 3.2799\n",
            "\n",
            "Epoch 00114: val_mae improved from 3.28050 to 3.27994, saving model to best_boston.h5\n",
            "Epoch 115/500\n",
            "2/2 [==============================] - 0s 101ms/step - loss: 10.9411 - mae: 2.3981 - val_loss: 21.5065 - val_mae: 3.2784\n",
            "\n",
            "Epoch 00115: val_mae improved from 3.27994 to 3.27835, saving model to best_boston.h5\n",
            "Epoch 116/500\n",
            "2/2 [==============================] - 0s 95ms/step - loss: 10.9128 - mae: 2.3960 - val_loss: 21.4723 - val_mae: 3.2746\n",
            "\n",
            "Epoch 00116: val_mae improved from 3.27835 to 3.27460, saving model to best_boston.h5\n",
            "Epoch 117/500\n",
            "2/2 [==============================] - 0s 106ms/step - loss: 10.8802 - mae: 2.3923 - val_loss: 21.4412 - val_mae: 3.2704\n",
            "\n",
            "Epoch 00117: val_mae improved from 3.27460 to 3.27042, saving model to best_boston.h5\n",
            "Epoch 118/500\n",
            "2/2 [==============================] - 0s 101ms/step - loss: 10.8459 - mae: 2.3878 - val_loss: 21.4053 - val_mae: 3.2655\n",
            "\n",
            "Epoch 00118: val_mae improved from 3.27042 to 3.26546, saving model to best_boston.h5\n",
            "Epoch 119/500\n",
            "2/2 [==============================] - 0s 102ms/step - loss: 10.8166 - mae: 2.3843 - val_loss: 21.3817 - val_mae: 3.2623\n",
            "\n",
            "Epoch 00119: val_mae improved from 3.26546 to 3.26225, saving model to best_boston.h5\n",
            "Epoch 120/500\n",
            "2/2 [==============================] - 0s 99ms/step - loss: 10.7869 - mae: 2.3808 - val_loss: 21.3753 - val_mae: 3.2601\n",
            "\n",
            "Epoch 00120: val_mae improved from 3.26225 to 3.26014, saving model to best_boston.h5\n",
            "Epoch 121/500\n",
            "2/2 [==============================] - 0s 109ms/step - loss: 10.7568 - mae: 2.3776 - val_loss: 21.3671 - val_mae: 3.2584\n",
            "\n",
            "Epoch 00121: val_mae improved from 3.26014 to 3.25841, saving model to best_boston.h5\n",
            "Epoch 122/500\n",
            "2/2 [==============================] - 0s 92ms/step - loss: 10.7360 - mae: 2.3757 - val_loss: 21.3859 - val_mae: 3.2586\n",
            "\n",
            "Epoch 00122: val_mae did not improve from 3.25841\n",
            "Epoch 123/500\n",
            "2/2 [==============================] - 0s 98ms/step - loss: 10.7033 - mae: 2.3731 - val_loss: 21.3863 - val_mae: 3.2567\n",
            "\n",
            "Epoch 00123: val_mae improved from 3.25841 to 3.25671, saving model to best_boston.h5\n",
            "Epoch 124/500\n",
            "2/2 [==============================] - 0s 101ms/step - loss: 10.6721 - mae: 2.3689 - val_loss: 21.3541 - val_mae: 3.2517\n",
            "\n",
            "Epoch 00124: val_mae improved from 3.25671 to 3.25168, saving model to best_boston.h5\n",
            "Epoch 125/500\n",
            "2/2 [==============================] - 0s 103ms/step - loss: 10.6496 - mae: 2.3653 - val_loss: 21.3498 - val_mae: 3.2494\n",
            "\n",
            "Epoch 00125: val_mae improved from 3.25168 to 3.24936, saving model to best_boston.h5\n",
            "Epoch 126/500\n",
            "2/2 [==============================] - 0s 105ms/step - loss: 10.6189 - mae: 2.3610 - val_loss: 21.3307 - val_mae: 3.2450\n",
            "\n",
            "Epoch 00126: val_mae improved from 3.24936 to 3.24501, saving model to best_boston.h5\n",
            "Epoch 127/500\n",
            "2/2 [==============================] - 0s 108ms/step - loss: 10.5911 - mae: 2.3570 - val_loss: 21.3231 - val_mae: 3.2426\n",
            "\n",
            "Epoch 00127: val_mae improved from 3.24501 to 3.24260, saving model to best_boston.h5\n",
            "Epoch 128/500\n",
            "2/2 [==============================] - 0s 100ms/step - loss: 10.5742 - mae: 2.3533 - val_loss: 21.3023 - val_mae: 3.2393\n",
            "\n",
            "Epoch 00128: val_mae improved from 3.24260 to 3.23927, saving model to best_boston.h5\n",
            "Epoch 129/500\n",
            "2/2 [==============================] - 0s 100ms/step - loss: 10.5387 - mae: 2.3497 - val_loss: 21.3439 - val_mae: 3.2423\n",
            "\n",
            "Epoch 00129: val_mae did not improve from 3.23927\n",
            "Epoch 130/500\n",
            "2/2 [==============================] - 0s 93ms/step - loss: 10.5113 - mae: 2.3476 - val_loss: 21.3857 - val_mae: 3.2444\n",
            "\n",
            "Epoch 00130: val_mae did not improve from 3.23927\n",
            "Epoch 131/500\n",
            "2/2 [==============================] - 0s 97ms/step - loss: 10.4804 - mae: 2.3464 - val_loss: 21.4663 - val_mae: 3.2502\n",
            "\n",
            "Epoch 00131: val_mae did not improve from 3.23927\n",
            "Epoch 132/500\n",
            "2/2 [==============================] - 0s 100ms/step - loss: 10.4576 - mae: 2.3462 - val_loss: 21.5438 - val_mae: 3.2558\n",
            "\n",
            "Epoch 00132: val_mae did not improve from 3.23927\n",
            "Epoch 133/500\n",
            "2/2 [==============================] - 0s 104ms/step - loss: 10.4434 - mae: 2.3481 - val_loss: 21.6136 - val_mae: 3.2603\n",
            "\n",
            "Epoch 00133: val_mae did not improve from 3.23927\n",
            "Epoch 134/500\n",
            "2/2 [==============================] - 0s 127ms/step - loss: 10.4083 - mae: 2.3463 - val_loss: 21.6258 - val_mae: 3.2595\n",
            "\n",
            "Epoch 00134: val_mae did not improve from 3.23927\n",
            "Epoch 135/500\n",
            "2/2 [==============================] - 0s 99ms/step - loss: 10.3814 - mae: 2.3439 - val_loss: 21.6136 - val_mae: 3.2568\n",
            "\n",
            "Epoch 00135: val_mae did not improve from 3.23927\n",
            "Epoch 136/500\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 10.3582 - mae: 2.3411 - val_loss: 21.5972 - val_mae: 3.2533\n",
            "\n",
            "Epoch 00136: val_mae did not improve from 3.23927\n",
            "Epoch 137/500\n",
            "2/2 [==============================] - 0s 101ms/step - loss: 10.3322 - mae: 2.3383 - val_loss: 21.5573 - val_mae: 3.2472\n",
            "\n",
            "Epoch 00137: val_mae did not improve from 3.23927\n",
            "Epoch 138/500\n",
            "2/2 [==============================] - 0s 101ms/step - loss: 10.3066 - mae: 2.3339 - val_loss: 21.5130 - val_mae: 3.2412\n",
            "\n",
            "Epoch 00138: val_mae did not improve from 3.23927\n",
            "Epoch 139/500\n",
            "2/2 [==============================] - 0s 97ms/step - loss: 10.2850 - mae: 2.3298 - val_loss: 21.4603 - val_mae: 3.2345\n",
            "\n",
            "Epoch 00139: val_mae improved from 3.23927 to 3.23448, saving model to best_boston.h5\n",
            "Epoch 140/500\n",
            "2/2 [==============================] - 0s 103ms/step - loss: 10.2633 - mae: 2.3254 - val_loss: 21.4191 - val_mae: 3.2292\n",
            "\n",
            "Epoch 00140: val_mae improved from 3.23448 to 3.22922, saving model to best_boston.h5\n",
            "Epoch 141/500\n",
            "2/2 [==============================] - 0s 105ms/step - loss: 10.2394 - mae: 2.3222 - val_loss: 21.4142 - val_mae: 3.2278\n",
            "\n",
            "Epoch 00141: val_mae improved from 3.22922 to 3.22783, saving model to best_boston.h5\n",
            "Epoch 142/500\n",
            "2/2 [==============================] - 0s 99ms/step - loss: 10.2127 - mae: 2.3199 - val_loss: 21.4245 - val_mae: 3.2280\n",
            "\n",
            "Epoch 00142: val_mae did not improve from 3.22783\n",
            "Epoch 143/500\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 10.1897 - mae: 2.3185 - val_loss: 21.4401 - val_mae: 3.2289\n",
            "\n",
            "Epoch 00143: val_mae did not improve from 3.22783\n",
            "Epoch 144/500\n",
            "2/2 [==============================] - 0s 100ms/step - loss: 10.1657 - mae: 2.3169 - val_loss: 21.4303 - val_mae: 3.2278\n",
            "\n",
            "Epoch 00144: val_mae improved from 3.22783 to 3.22778, saving model to best_boston.h5\n",
            "Epoch 145/500\n",
            "2/2 [==============================] - 0s 109ms/step - loss: 10.1452 - mae: 2.3152 - val_loss: 21.4158 - val_mae: 3.2257\n",
            "\n",
            "Epoch 00145: val_mae improved from 3.22778 to 3.22574, saving model to best_boston.h5\n",
            "Epoch 146/500\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 10.1198 - mae: 2.3125 - val_loss: 21.4027 - val_mae: 3.2228\n",
            "\n",
            "Epoch 00146: val_mae improved from 3.22574 to 3.22281, saving model to best_boston.h5\n",
            "Epoch 147/500\n",
            "2/2 [==============================] - 0s 100ms/step - loss: 10.0956 - mae: 2.3094 - val_loss: 21.3952 - val_mae: 3.2198\n",
            "\n",
            "Epoch 00147: val_mae improved from 3.22281 to 3.21976, saving model to best_boston.h5\n",
            "Epoch 148/500\n",
            "2/2 [==============================] - 0s 102ms/step - loss: 10.0751 - mae: 2.3064 - val_loss: 21.3943 - val_mae: 3.2172\n",
            "\n",
            "Epoch 00148: val_mae improved from 3.21976 to 3.21719, saving model to best_boston.h5\n",
            "Epoch 149/500\n",
            "2/2 [==============================] - 0s 107ms/step - loss: 10.0556 - mae: 2.3032 - val_loss: 21.3783 - val_mae: 3.2128\n",
            "\n",
            "Epoch 00149: val_mae improved from 3.21719 to 3.21276, saving model to best_boston.h5\n",
            "Epoch 150/500\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 10.0321 - mae: 2.2991 - val_loss: 21.3420 - val_mae: 3.2064\n",
            "\n",
            "Epoch 00150: val_mae improved from 3.21276 to 3.20636, saving model to best_boston.h5\n",
            "Epoch 151/500\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 10.0104 - mae: 2.2957 - val_loss: 21.3304 - val_mae: 3.2029\n",
            "\n",
            "Epoch 00151: val_mae improved from 3.20636 to 3.20292, saving model to best_boston.h5\n",
            "Epoch 152/500\n",
            "2/2 [==============================] - 0s 103ms/step - loss: 9.9875 - mae: 2.2930 - val_loss: 21.3059 - val_mae: 3.1984\n",
            "\n",
            "Epoch 00152: val_mae improved from 3.20292 to 3.19842, saving model to best_boston.h5\n",
            "Epoch 153/500\n",
            "2/2 [==============================] - 0s 105ms/step - loss: 9.9710 - mae: 2.2914 - val_loss: 21.2899 - val_mae: 3.1959\n",
            "\n",
            "Epoch 00153: val_mae improved from 3.19842 to 3.19586, saving model to best_boston.h5\n",
            "Epoch 154/500\n",
            "2/2 [==============================] - 0s 105ms/step - loss: 9.9508 - mae: 2.2902 - val_loss: 21.2993 - val_mae: 3.1954\n",
            "\n",
            "Epoch 00154: val_mae improved from 3.19586 to 3.19542, saving model to best_boston.h5\n",
            "Epoch 155/500\n",
            "2/2 [==============================] - 0s 107ms/step - loss: 9.9284 - mae: 2.2890 - val_loss: 21.3052 - val_mae: 3.1963\n",
            "\n",
            "Epoch 00155: val_mae did not improve from 3.19542\n",
            "Epoch 156/500\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 9.9081 - mae: 2.2867 - val_loss: 21.3063 - val_mae: 3.1962\n",
            "\n",
            "Epoch 00156: val_mae did not improve from 3.19542\n",
            "Epoch 157/500\n",
            "2/2 [==============================] - 0s 106ms/step - loss: 9.8891 - mae: 2.2846 - val_loss: 21.3169 - val_mae: 3.1964\n",
            "\n",
            "Epoch 00157: val_mae did not improve from 3.19542\n",
            "Epoch 158/500\n",
            "2/2 [==============================] - 0s 107ms/step - loss: 9.8699 - mae: 2.2820 - val_loss: 21.3081 - val_mae: 3.1939\n",
            "\n",
            "Epoch 00158: val_mae improved from 3.19542 to 3.19388, saving model to best_boston.h5\n",
            "Epoch 159/500\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 9.8490 - mae: 2.2786 - val_loss: 21.3088 - val_mae: 3.1925\n",
            "\n",
            "Epoch 00159: val_mae improved from 3.19388 to 3.19252, saving model to best_boston.h5\n",
            "Epoch 160/500\n",
            "2/2 [==============================] - 0s 105ms/step - loss: 9.8305 - mae: 2.2756 - val_loss: 21.3068 - val_mae: 3.1907\n",
            "\n",
            "Epoch 00160: val_mae improved from 3.19252 to 3.19066, saving model to best_boston.h5\n",
            "Epoch 161/500\n",
            "2/2 [==============================] - 0s 106ms/step - loss: 9.8120 - mae: 2.2718 - val_loss: 21.2716 - val_mae: 3.1860\n",
            "\n",
            "Epoch 00161: val_mae improved from 3.19066 to 3.18598, saving model to best_boston.h5\n",
            "Epoch 162/500\n",
            "2/2 [==============================] - 0s 108ms/step - loss: 9.7934 - mae: 2.2684 - val_loss: 21.2432 - val_mae: 3.1829\n",
            "\n",
            "Epoch 00162: val_mae improved from 3.18598 to 3.18285, saving model to best_boston.h5\n",
            "Epoch 163/500\n",
            "2/2 [==============================] - 0s 105ms/step - loss: 9.7766 - mae: 2.2663 - val_loss: 21.2433 - val_mae: 3.1825\n",
            "\n",
            "Epoch 00163: val_mae improved from 3.18285 to 3.18246, saving model to best_boston.h5\n",
            "Epoch 164/500\n",
            "2/2 [==============================] - 0s 99ms/step - loss: 9.7542 - mae: 2.2638 - val_loss: 21.2286 - val_mae: 3.1795\n",
            "\n",
            "Epoch 00164: val_mae improved from 3.18246 to 3.17947, saving model to best_boston.h5\n",
            "Epoch 165/500\n",
            "2/2 [==============================] - 0s 108ms/step - loss: 9.7337 - mae: 2.2606 - val_loss: 21.2215 - val_mae: 3.1779\n",
            "\n",
            "Epoch 00165: val_mae improved from 3.17947 to 3.17791, saving model to best_boston.h5\n",
            "Epoch 166/500\n",
            "2/2 [==============================] - 0s 97ms/step - loss: 9.7168 - mae: 2.2584 - val_loss: 21.2063 - val_mae: 3.1756\n",
            "\n",
            "Epoch 00166: val_mae improved from 3.17791 to 3.17557, saving model to best_boston.h5\n",
            "Epoch 167/500\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 9.6985 - mae: 2.2565 - val_loss: 21.1871 - val_mae: 3.1734\n",
            "\n",
            "Epoch 00167: val_mae improved from 3.17557 to 3.17338, saving model to best_boston.h5\n",
            "Epoch 168/500\n",
            "2/2 [==============================] - 0s 109ms/step - loss: 9.6844 - mae: 2.2551 - val_loss: 21.1660 - val_mae: 3.1707\n",
            "\n",
            "Epoch 00168: val_mae improved from 3.17338 to 3.17068, saving model to best_boston.h5\n",
            "Epoch 169/500\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 9.6613 - mae: 2.2529 - val_loss: 21.1479 - val_mae: 3.1681\n",
            "\n",
            "Epoch 00169: val_mae improved from 3.17068 to 3.16812, saving model to best_boston.h5\n",
            "Epoch 170/500\n",
            "2/2 [==============================] - 0s 102ms/step - loss: 9.6425 - mae: 2.2505 - val_loss: 21.1465 - val_mae: 3.1666\n",
            "\n",
            "Epoch 00170: val_mae improved from 3.16812 to 3.16664, saving model to best_boston.h5\n",
            "Epoch 171/500\n",
            "2/2 [==============================] - 0s 105ms/step - loss: 9.6266 - mae: 2.2486 - val_loss: 21.1547 - val_mae: 3.1657\n",
            "\n",
            "Epoch 00171: val_mae improved from 3.16664 to 3.16570, saving model to best_boston.h5\n",
            "Epoch 172/500\n",
            "2/2 [==============================] - 0s 103ms/step - loss: 9.6147 - mae: 2.2480 - val_loss: 21.1828 - val_mae: 3.1673\n",
            "\n",
            "Epoch 00172: val_mae did not improve from 3.16570\n",
            "Epoch 173/500\n",
            "2/2 [==============================] - 0s 99ms/step - loss: 9.5927 - mae: 2.2462 - val_loss: 21.1726 - val_mae: 3.1645\n",
            "\n",
            "Epoch 00173: val_mae improved from 3.16570 to 3.16455, saving model to best_boston.h5\n",
            "Epoch 174/500\n",
            "2/2 [==============================] - 0s 105ms/step - loss: 9.5749 - mae: 2.2437 - val_loss: 21.1753 - val_mae: 3.1631\n",
            "\n",
            "Epoch 00174: val_mae improved from 3.16455 to 3.16307, saving model to best_boston.h5\n",
            "Epoch 175/500\n",
            "2/2 [==============================] - 0s 94ms/step - loss: 9.5580 - mae: 2.2424 - val_loss: 21.1913 - val_mae: 3.1636\n",
            "\n",
            "Epoch 00175: val_mae did not improve from 3.16307\n",
            "Epoch 176/500\n",
            "2/2 [==============================] - 0s 114ms/step - loss: 9.5435 - mae: 2.2416 - val_loss: 21.1925 - val_mae: 3.1629\n",
            "\n",
            "Epoch 00176: val_mae improved from 3.16307 to 3.16289, saving model to best_boston.h5\n",
            "Epoch 177/500\n",
            "2/2 [==============================] - 0s 102ms/step - loss: 9.5217 - mae: 2.2392 - val_loss: 21.1515 - val_mae: 3.1581\n",
            "\n",
            "Epoch 00177: val_mae improved from 3.16289 to 3.15806, saving model to best_boston.h5\n",
            "Epoch 178/500\n",
            "2/2 [==============================] - 0s 101ms/step - loss: 9.5073 - mae: 2.2376 - val_loss: 21.1045 - val_mae: 3.1523\n",
            "\n",
            "Epoch 00178: val_mae improved from 3.15806 to 3.15235, saving model to best_boston.h5\n",
            "Epoch 179/500\n",
            "2/2 [==============================] - 0s 95ms/step - loss: 9.4931 - mae: 2.2353 - val_loss: 21.0818 - val_mae: 3.1488\n",
            "\n",
            "Epoch 00179: val_mae improved from 3.15235 to 3.14875, saving model to best_boston.h5\n",
            "Epoch 180/500\n",
            "2/2 [==============================] - 0s 116ms/step - loss: 9.4741 - mae: 2.2320 - val_loss: 21.0431 - val_mae: 3.1441\n",
            "\n",
            "Epoch 00180: val_mae improved from 3.14875 to 3.14409, saving model to best_boston.h5\n",
            "Epoch 181/500\n",
            "2/2 [==============================] - 0s 100ms/step - loss: 9.4651 - mae: 2.2294 - val_loss: 21.0185 - val_mae: 3.1402\n",
            "\n",
            "Epoch 00181: val_mae improved from 3.14409 to 3.14017, saving model to best_boston.h5\n",
            "Epoch 182/500\n",
            "2/2 [==============================] - 0s 98ms/step - loss: 9.4462 - mae: 2.2260 - val_loss: 21.0537 - val_mae: 3.1417\n",
            "\n",
            "Epoch 00182: val_mae did not improve from 3.14017\n",
            "Epoch 183/500\n",
            "2/2 [==============================] - 0s 112ms/step - loss: 9.4309 - mae: 2.2243 - val_loss: 21.0772 - val_mae: 3.1415\n",
            "\n",
            "Epoch 00183: val_mae did not improve from 3.14017\n",
            "Epoch 184/500\n",
            "2/2 [==============================] - 0s 117ms/step - loss: 9.4137 - mae: 2.2215 - val_loss: 21.0780 - val_mae: 3.1392\n",
            "\n",
            "Epoch 00184: val_mae improved from 3.14017 to 3.13916, saving model to best_boston.h5\n",
            "Epoch 185/500\n",
            "2/2 [==============================] - 0s 106ms/step - loss: 9.3995 - mae: 2.2188 - val_loss: 21.0633 - val_mae: 3.1349\n",
            "\n",
            "Epoch 00185: val_mae improved from 3.13916 to 3.13493, saving model to best_boston.h5\n",
            "Epoch 186/500\n",
            "2/2 [==============================] - 0s 99ms/step - loss: 9.3862 - mae: 2.2163 - val_loss: 21.0477 - val_mae: 3.1311\n",
            "\n",
            "Epoch 00186: val_mae improved from 3.13493 to 3.13113, saving model to best_boston.h5\n",
            "Epoch 187/500\n",
            "2/2 [==============================] - 0s 98ms/step - loss: 9.3713 - mae: 2.2138 - val_loss: 21.0321 - val_mae: 3.1274\n",
            "\n",
            "Epoch 00187: val_mae improved from 3.13113 to 3.12742, saving model to best_boston.h5\n",
            "Epoch 188/500\n",
            "2/2 [==============================] - 0s 101ms/step - loss: 9.3582 - mae: 2.2121 - val_loss: 21.0311 - val_mae: 3.1251\n",
            "\n",
            "Epoch 00188: val_mae improved from 3.12742 to 3.12513, saving model to best_boston.h5\n",
            "Epoch 189/500\n",
            "2/2 [==============================] - 0s 109ms/step - loss: 9.3407 - mae: 2.2102 - val_loss: 21.0179 - val_mae: 3.1217\n",
            "\n",
            "Epoch 00189: val_mae improved from 3.12513 to 3.12168, saving model to best_boston.h5\n",
            "Epoch 190/500\n",
            "2/2 [==============================] - 0s 100ms/step - loss: 9.3260 - mae: 2.2085 - val_loss: 21.0384 - val_mae: 3.1218\n",
            "\n",
            "Epoch 00190: val_mae did not improve from 3.12168\n",
            "Epoch 191/500\n",
            "2/2 [==============================] - 0s 111ms/step - loss: 9.3117 - mae: 2.2081 - val_loss: 21.0588 - val_mae: 3.1224\n",
            "\n",
            "Epoch 00191: val_mae did not improve from 3.12168\n",
            "Epoch 192/500\n",
            "2/2 [==============================] - 0s 114ms/step - loss: 9.2938 - mae: 2.2070 - val_loss: 21.0616 - val_mae: 3.1206\n",
            "\n",
            "Epoch 00192: val_mae improved from 3.12168 to 3.12061, saving model to best_boston.h5\n",
            "Epoch 193/500\n",
            "2/2 [==============================] - 0s 106ms/step - loss: 9.2791 - mae: 2.2062 - val_loss: 21.0600 - val_mae: 3.1187\n",
            "\n",
            "Epoch 00193: val_mae improved from 3.12061 to 3.11867, saving model to best_boston.h5\n",
            "Epoch 194/500\n",
            "2/2 [==============================] - 0s 112ms/step - loss: 9.2690 - mae: 2.2057 - val_loss: 21.0388 - val_mae: 3.1149\n",
            "\n",
            "Epoch 00194: val_mae improved from 3.11867 to 3.11494, saving model to best_boston.h5\n",
            "Epoch 195/500\n",
            "2/2 [==============================] - 0s 106ms/step - loss: 9.2518 - mae: 2.2036 - val_loss: 21.0664 - val_mae: 3.1160\n",
            "\n",
            "Epoch 00195: val_mae did not improve from 3.11494\n",
            "Epoch 196/500\n",
            "2/2 [==============================] - 0s 112ms/step - loss: 9.2332 - mae: 2.2017 - val_loss: 21.0685 - val_mae: 3.1143\n",
            "\n",
            "Epoch 00196: val_mae improved from 3.11494 to 3.11434, saving model to best_boston.h5\n",
            "Epoch 197/500\n",
            "2/2 [==============================] - 0s 99ms/step - loss: 9.2206 - mae: 2.1998 - val_loss: 21.0578 - val_mae: 3.1123\n",
            "\n",
            "Epoch 00197: val_mae improved from 3.11434 to 3.11229, saving model to best_boston.h5\n",
            "Epoch 198/500\n",
            "2/2 [==============================] - 0s 103ms/step - loss: 9.2066 - mae: 2.1981 - val_loss: 21.0324 - val_mae: 3.1093\n",
            "\n",
            "Epoch 00198: val_mae improved from 3.11229 to 3.10930, saving model to best_boston.h5\n",
            "Epoch 199/500\n",
            "2/2 [==============================] - 0s 104ms/step - loss: 9.1908 - mae: 2.1963 - val_loss: 21.0341 - val_mae: 3.1082\n",
            "\n",
            "Epoch 00199: val_mae improved from 3.10930 to 3.10816, saving model to best_boston.h5\n",
            "Epoch 200/500\n",
            "2/2 [==============================] - 0s 105ms/step - loss: 9.1786 - mae: 2.1962 - val_loss: 21.0380 - val_mae: 3.1082\n",
            "\n",
            "Epoch 00200: val_mae did not improve from 3.10816\n",
            "Epoch 201/500\n",
            "2/2 [==============================] - 0s 111ms/step - loss: 9.1635 - mae: 2.1955 - val_loss: 21.0336 - val_mae: 3.1068\n",
            "\n",
            "Epoch 00201: val_mae improved from 3.10816 to 3.10679, saving model to best_boston.h5\n",
            "Epoch 202/500\n",
            "2/2 [==============================] - 0s 105ms/step - loss: 9.1493 - mae: 2.1939 - val_loss: 21.0228 - val_mae: 3.1042\n",
            "\n",
            "Epoch 00202: val_mae improved from 3.10679 to 3.10424, saving model to best_boston.h5\n",
            "Epoch 203/500\n",
            "2/2 [==============================] - 0s 101ms/step - loss: 9.1355 - mae: 2.1930 - val_loss: 21.0270 - val_mae: 3.1032\n",
            "\n",
            "Epoch 00203: val_mae improved from 3.10424 to 3.10323, saving model to best_boston.h5\n",
            "Epoch 204/500\n",
            "2/2 [==============================] - 0s 103ms/step - loss: 9.1260 - mae: 2.1922 - val_loss: 21.0251 - val_mae: 3.1017\n",
            "\n",
            "Epoch 00204: val_mae improved from 3.10323 to 3.10174, saving model to best_boston.h5\n",
            "Epoch 205/500\n",
            "2/2 [==============================] - 0s 97ms/step - loss: 9.1145 - mae: 2.1916 - val_loss: 20.9995 - val_mae: 3.0983\n",
            "\n",
            "Epoch 00205: val_mae improved from 3.10174 to 3.09833, saving model to best_boston.h5\n",
            "Epoch 206/500\n",
            "2/2 [==============================] - 0s 104ms/step - loss: 9.0959 - mae: 2.1884 - val_loss: 20.9391 - val_mae: 3.0905\n",
            "\n",
            "Epoch 00206: val_mae improved from 3.09833 to 3.09054, saving model to best_boston.h5\n",
            "Epoch 207/500\n",
            "2/2 [==============================] - 0s 105ms/step - loss: 9.0823 - mae: 2.1848 - val_loss: 20.9220 - val_mae: 3.0868\n",
            "\n",
            "Epoch 00207: val_mae improved from 3.09054 to 3.08680, saving model to best_boston.h5\n",
            "Epoch 208/500\n",
            "2/2 [==============================] - 0s 102ms/step - loss: 9.0690 - mae: 2.1819 - val_loss: 20.8950 - val_mae: 3.0820\n",
            "\n",
            "Epoch 00208: val_mae improved from 3.08680 to 3.08205, saving model to best_boston.h5\n",
            "Epoch 209/500\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 9.0551 - mae: 2.1787 - val_loss: 20.8747 - val_mae: 3.0786\n",
            "\n",
            "Epoch 00209: val_mae improved from 3.08205 to 3.07863, saving model to best_boston.h5\n",
            "Epoch 210/500\n",
            "2/2 [==============================] - 0s 301ms/step - loss: 9.0447 - mae: 2.1758 - val_loss: 20.8576 - val_mae: 3.0754\n",
            "\n",
            "Epoch 00210: val_mae improved from 3.07863 to 3.07541, saving model to best_boston.h5\n",
            "Epoch 211/500\n",
            "2/2 [==============================] - 0s 96ms/step - loss: 9.0306 - mae: 2.1730 - val_loss: 20.8397 - val_mae: 3.0715\n",
            "\n",
            "Epoch 00211: val_mae improved from 3.07541 to 3.07150, saving model to best_boston.h5\n",
            "Epoch 212/500\n",
            "2/2 [==============================] - 0s 103ms/step - loss: 9.0196 - mae: 2.1707 - val_loss: 20.8252 - val_mae: 3.0677\n",
            "\n",
            "Epoch 00212: val_mae improved from 3.07150 to 3.06769, saving model to best_boston.h5\n",
            "Epoch 213/500\n",
            "2/2 [==============================] - 0s 93ms/step - loss: 9.0060 - mae: 2.1684 - val_loss: 20.8074 - val_mae: 3.0644\n",
            "\n",
            "Epoch 00213: val_mae improved from 3.06769 to 3.06443, saving model to best_boston.h5\n",
            "Epoch 214/500\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 8.9945 - mae: 2.1671 - val_loss: 20.7925 - val_mae: 3.0613\n",
            "\n",
            "Epoch 00214: val_mae improved from 3.06443 to 3.06134, saving model to best_boston.h5\n",
            "Epoch 215/500\n",
            "2/2 [==============================] - 0s 97ms/step - loss: 8.9780 - mae: 2.1655 - val_loss: 20.8053 - val_mae: 3.0620\n",
            "\n",
            "Epoch 00215: val_mae did not improve from 3.06134\n",
            "Epoch 216/500\n",
            "2/2 [==============================] - 0s 101ms/step - loss: 8.9673 - mae: 2.1661 - val_loss: 20.8390 - val_mae: 3.0655\n",
            "\n",
            "Epoch 00216: val_mae did not improve from 3.06134\n",
            "Epoch 217/500\n",
            "2/2 [==============================] - 0s 107ms/step - loss: 8.9588 - mae: 2.1670 - val_loss: 20.8735 - val_mae: 3.0687\n",
            "\n",
            "Epoch 00217: val_mae did not improve from 3.06134\n",
            "Epoch 218/500\n",
            "2/2 [==============================] - 0s 115ms/step - loss: 8.9390 - mae: 2.1663 - val_loss: 20.8645 - val_mae: 3.0670\n",
            "\n",
            "Epoch 00218: val_mae did not improve from 3.06134\n",
            "Epoch 219/500\n",
            "2/2 [==============================] - 0s 105ms/step - loss: 8.9266 - mae: 2.1652 - val_loss: 20.8763 - val_mae: 3.0678\n",
            "\n",
            "Epoch 00219: val_mae did not improve from 3.06134\n",
            "Epoch 220/500\n",
            "2/2 [==============================] - 0s 100ms/step - loss: 8.9176 - mae: 2.1650 - val_loss: 20.9061 - val_mae: 3.0691\n",
            "\n",
            "Epoch 00220: val_mae did not improve from 3.06134\n",
            "Epoch 221/500\n",
            "2/2 [==============================] - 0s 98ms/step - loss: 8.9032 - mae: 2.1637 - val_loss: 20.9044 - val_mae: 3.0674\n",
            "\n",
            "Epoch 00221: val_mae did not improve from 3.06134\n",
            "Epoch 222/500\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 8.8917 - mae: 2.1622 - val_loss: 20.9303 - val_mae: 3.0671\n",
            "\n",
            "Epoch 00222: val_mae did not improve from 3.06134\n",
            "Epoch 223/500\n",
            "2/2 [==============================] - 0s 99ms/step - loss: 8.8873 - mae: 2.1603 - val_loss: 20.9283 - val_mae: 3.0646\n",
            "\n",
            "Epoch 00223: val_mae did not improve from 3.06134\n",
            "Epoch 224/500\n",
            "2/2 [==============================] - 0s 105ms/step - loss: 8.8690 - mae: 2.1587 - val_loss: 20.9743 - val_mae: 3.0663\n",
            "\n",
            "Epoch 00224: val_mae did not improve from 3.06134\n",
            "Epoch 225/500\n",
            "2/2 [==============================] - 0s 116ms/step - loss: 8.8565 - mae: 2.1582 - val_loss: 20.9938 - val_mae: 3.0656\n",
            "\n",
            "Epoch 00225: val_mae did not improve from 3.06134\n",
            "Epoch 226/500\n",
            "2/2 [==============================] - 0s 107ms/step - loss: 8.8427 - mae: 2.1561 - val_loss: 20.9957 - val_mae: 3.0627\n",
            "\n",
            "Epoch 00226: val_mae did not improve from 3.06134\n",
            "Epoch 227/500\n",
            "2/2 [==============================] - 0s 111ms/step - loss: 8.8304 - mae: 2.1540 - val_loss: 20.9800 - val_mae: 3.0595\n",
            "\n",
            "Epoch 00227: val_mae improved from 3.06134 to 3.05952, saving model to best_boston.h5\n",
            "Epoch 228/500\n",
            "2/2 [==============================] - 0s 98ms/step - loss: 8.8186 - mae: 2.1513 - val_loss: 20.9456 - val_mae: 3.0544\n",
            "\n",
            "Epoch 00228: val_mae improved from 3.05952 to 3.05442, saving model to best_boston.h5\n",
            "Epoch 229/500\n",
            "2/2 [==============================] - 0s 99ms/step - loss: 8.8084 - mae: 2.1495 - val_loss: 20.9346 - val_mae: 3.0519\n",
            "\n",
            "Epoch 00229: val_mae improved from 3.05442 to 3.05189, saving model to best_boston.h5\n",
            "Epoch 230/500\n",
            "2/2 [==============================] - 0s 108ms/step - loss: 8.7956 - mae: 2.1476 - val_loss: 20.8942 - val_mae: 3.0468\n",
            "\n",
            "Epoch 00230: val_mae improved from 3.05189 to 3.04679, saving model to best_boston.h5\n",
            "Epoch 231/500\n",
            "2/2 [==============================] - 0s 96ms/step - loss: 8.7856 - mae: 2.1454 - val_loss: 20.8626 - val_mae: 3.0434\n",
            "\n",
            "Epoch 00231: val_mae improved from 3.04679 to 3.04340, saving model to best_boston.h5\n",
            "Epoch 232/500\n",
            "2/2 [==============================] - 0s 112ms/step - loss: 8.7740 - mae: 2.1421 - val_loss: 20.8204 - val_mae: 3.0392\n",
            "\n",
            "Epoch 00232: val_mae improved from 3.04340 to 3.03922, saving model to best_boston.h5\n",
            "Epoch 233/500\n",
            "2/2 [==============================] - 0s 96ms/step - loss: 8.7594 - mae: 2.1403 - val_loss: 20.8275 - val_mae: 3.0391\n",
            "\n",
            "Epoch 00233: val_mae improved from 3.03922 to 3.03908, saving model to best_boston.h5\n",
            "Epoch 234/500\n",
            "2/2 [==============================] - 0s 114ms/step - loss: 8.7473 - mae: 2.1391 - val_loss: 20.8294 - val_mae: 3.0391\n",
            "\n",
            "Epoch 00234: val_mae did not improve from 3.03908\n",
            "Epoch 235/500\n",
            "2/2 [==============================] - 0s 114ms/step - loss: 8.7380 - mae: 2.1380 - val_loss: 20.8492 - val_mae: 3.0409\n",
            "\n",
            "Epoch 00235: val_mae did not improve from 3.03908\n",
            "Epoch 236/500\n",
            "2/2 [==============================] - 0s 107ms/step - loss: 8.7262 - mae: 2.1374 - val_loss: 20.8560 - val_mae: 3.0409\n",
            "\n",
            "Epoch 00236: val_mae did not improve from 3.03908\n",
            "Epoch 237/500\n",
            "2/2 [==============================] - 0s 104ms/step - loss: 8.7120 - mae: 2.1358 - val_loss: 20.8529 - val_mae: 3.0395\n",
            "\n",
            "Epoch 00237: val_mae did not improve from 3.03908\n",
            "Epoch 238/500\n",
            "2/2 [==============================] - 0s 99ms/step - loss: 8.7013 - mae: 2.1339 - val_loss: 20.8429 - val_mae: 3.0379\n",
            "\n",
            "Epoch 00238: val_mae improved from 3.03908 to 3.03789, saving model to best_boston.h5\n",
            "Epoch 239/500\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 8.6939 - mae: 2.1325 - val_loss: 20.8476 - val_mae: 3.0364\n",
            "\n",
            "Epoch 00239: val_mae improved from 3.03789 to 3.03635, saving model to best_boston.h5\n",
            "Epoch 240/500\n",
            "2/2 [==============================] - 0s 100ms/step - loss: 8.6787 - mae: 2.1297 - val_loss: 20.8188 - val_mae: 3.0320\n",
            "\n",
            "Epoch 00240: val_mae improved from 3.03635 to 3.03195, saving model to best_boston.h5\n",
            "Epoch 241/500\n",
            "2/2 [==============================] - 0s 105ms/step - loss: 8.6743 - mae: 2.1277 - val_loss: 20.8101 - val_mae: 3.0293\n",
            "\n",
            "Epoch 00241: val_mae improved from 3.03195 to 3.02934, saving model to best_boston.h5\n",
            "Epoch 242/500\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 8.6608 - mae: 2.1253 - val_loss: 20.8382 - val_mae: 3.0317\n",
            "\n",
            "Epoch 00242: val_mae did not improve from 3.02934\n",
            "Epoch 243/500\n",
            "2/2 [==============================] - 0s 106ms/step - loss: 8.6459 - mae: 2.1238 - val_loss: 20.8545 - val_mae: 3.0321\n",
            "\n",
            "Epoch 00243: val_mae did not improve from 3.02934\n",
            "Epoch 244/500\n",
            "2/2 [==============================] - 0s 103ms/step - loss: 8.6381 - mae: 2.1226 - val_loss: 20.8660 - val_mae: 3.0327\n",
            "\n",
            "Epoch 00244: val_mae did not improve from 3.02934\n",
            "Epoch 245/500\n",
            "2/2 [==============================] - 0s 104ms/step - loss: 8.6260 - mae: 2.1209 - val_loss: 20.8434 - val_mae: 3.0298\n",
            "\n",
            "Epoch 00245: val_mae did not improve from 3.02934\n",
            "Epoch 246/500\n",
            "2/2 [==============================] - 0s 100ms/step - loss: 8.6147 - mae: 2.1190 - val_loss: 20.8339 - val_mae: 3.0283\n",
            "\n",
            "Epoch 00246: val_mae improved from 3.02934 to 3.02834, saving model to best_boston.h5\n",
            "Epoch 247/500\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 8.6047 - mae: 2.1181 - val_loss: 20.8425 - val_mae: 3.0288\n",
            "\n",
            "Epoch 00247: val_mae did not improve from 3.02834\n",
            "Epoch 248/500\n",
            "2/2 [==============================] - 0s 103ms/step - loss: 8.5938 - mae: 2.1176 - val_loss: 20.8468 - val_mae: 3.0279\n",
            "\n",
            "Epoch 00248: val_mae improved from 3.02834 to 3.02790, saving model to best_boston.h5\n",
            "Epoch 249/500\n",
            "2/2 [==============================] - 0s 93ms/step - loss: 8.5809 - mae: 2.1166 - val_loss: 20.8428 - val_mae: 3.0267\n",
            "\n",
            "Epoch 00249: val_mae improved from 3.02790 to 3.02671, saving model to best_boston.h5\n",
            "Epoch 250/500\n",
            "2/2 [==============================] - 0s 129ms/step - loss: 8.5730 - mae: 2.1156 - val_loss: 20.8437 - val_mae: 3.0250\n",
            "\n",
            "Epoch 00250: val_mae improved from 3.02671 to 3.02500, saving model to best_boston.h5\n",
            "Epoch 251/500\n",
            "2/2 [==============================] - 0s 104ms/step - loss: 8.5658 - mae: 2.1148 - val_loss: 20.8189 - val_mae: 3.0216\n",
            "\n",
            "Epoch 00251: val_mae improved from 3.02500 to 3.02164, saving model to best_boston.h5\n",
            "Epoch 252/500\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 8.5507 - mae: 2.1123 - val_loss: 20.8479 - val_mae: 3.0227\n",
            "\n",
            "Epoch 00252: val_mae did not improve from 3.02164\n",
            "Epoch 253/500\n",
            "2/2 [==============================] - 0s 93ms/step - loss: 8.5401 - mae: 2.1114 - val_loss: 20.8585 - val_mae: 3.0224\n",
            "\n",
            "Epoch 00253: val_mae did not improve from 3.02164\n",
            "Epoch 254/500\n",
            "2/2 [==============================] - 0s 111ms/step - loss: 8.5292 - mae: 2.1099 - val_loss: 20.8410 - val_mae: 3.0199\n",
            "\n",
            "Epoch 00254: val_mae improved from 3.02164 to 3.01993, saving model to best_boston.h5\n",
            "Epoch 255/500\n",
            "2/2 [==============================] - 0s 93ms/step - loss: 8.5191 - mae: 2.1084 - val_loss: 20.8328 - val_mae: 3.0174\n",
            "\n",
            "Epoch 00255: val_mae improved from 3.01993 to 3.01738, saving model to best_boston.h5\n",
            "Epoch 256/500\n",
            "2/2 [==============================] - 0s 103ms/step - loss: 8.5092 - mae: 2.1068 - val_loss: 20.8665 - val_mae: 3.0184\n",
            "\n",
            "Epoch 00256: val_mae did not improve from 3.01738\n",
            "Epoch 257/500\n",
            "2/2 [==============================] - 0s 103ms/step - loss: 8.4990 - mae: 2.1060 - val_loss: 20.8968 - val_mae: 3.0205\n",
            "\n",
            "Epoch 00257: val_mae did not improve from 3.01738\n",
            "Epoch 258/500\n",
            "2/2 [==============================] - 0s 114ms/step - loss: 8.4875 - mae: 2.1053 - val_loss: 20.9104 - val_mae: 3.0205\n",
            "\n",
            "Epoch 00258: val_mae did not improve from 3.01738\n",
            "Epoch 259/500\n",
            "2/2 [==============================] - 0s 101ms/step - loss: 8.4773 - mae: 2.1042 - val_loss: 20.9198 - val_mae: 3.0210\n",
            "\n",
            "Epoch 00259: val_mae did not improve from 3.01738\n",
            "Epoch 260/500\n",
            "2/2 [==============================] - 0s 116ms/step - loss: 8.4704 - mae: 2.1037 - val_loss: 20.9077 - val_mae: 3.0194\n",
            "\n",
            "Epoch 00260: val_mae did not improve from 3.01738\n",
            "Epoch 261/500\n",
            "2/2 [==============================] - 0s 106ms/step - loss: 8.4584 - mae: 2.1022 - val_loss: 20.8764 - val_mae: 3.0162\n",
            "\n",
            "Epoch 00261: val_mae improved from 3.01738 to 3.01616, saving model to best_boston.h5\n",
            "Epoch 262/500\n",
            "2/2 [==============================] - 0s 107ms/step - loss: 8.4479 - mae: 2.1012 - val_loss: 20.8694 - val_mae: 3.0139\n",
            "\n",
            "Epoch 00262: val_mae improved from 3.01616 to 3.01388, saving model to best_boston.h5\n",
            "Epoch 263/500\n",
            "2/2 [==============================] - 0s 116ms/step - loss: 8.4430 - mae: 2.1008 - val_loss: 20.8450 - val_mae: 3.0101\n",
            "\n",
            "Epoch 00263: val_mae improved from 3.01388 to 3.01007, saving model to best_boston.h5\n",
            "Epoch 264/500\n",
            "2/2 [==============================] - 0s 106ms/step - loss: 8.4278 - mae: 2.0990 - val_loss: 20.8454 - val_mae: 3.0095\n",
            "\n",
            "Epoch 00264: val_mae improved from 3.01007 to 3.00949, saving model to best_boston.h5\n",
            "Epoch 265/500\n",
            "2/2 [==============================] - 0s 104ms/step - loss: 8.4188 - mae: 2.0984 - val_loss: 20.8323 - val_mae: 3.0078\n",
            "\n",
            "Epoch 00265: val_mae improved from 3.00949 to 3.00784, saving model to best_boston.h5\n",
            "Epoch 266/500\n",
            "2/2 [==============================] - 0s 102ms/step - loss: 8.4091 - mae: 2.0970 - val_loss: 20.8023 - val_mae: 3.0037\n",
            "\n",
            "Epoch 00266: val_mae improved from 3.00784 to 3.00375, saving model to best_boston.h5\n",
            "Epoch 267/500\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 8.3963 - mae: 2.0947 - val_loss: 20.7676 - val_mae: 2.9997\n",
            "\n",
            "Epoch 00267: val_mae improved from 3.00375 to 2.99968, saving model to best_boston.h5\n",
            "Epoch 268/500\n",
            "2/2 [==============================] - 0s 103ms/step - loss: 8.3862 - mae: 2.0923 - val_loss: 20.7269 - val_mae: 2.9950\n",
            "\n",
            "Epoch 00268: val_mae improved from 2.99968 to 2.99503, saving model to best_boston.h5\n",
            "Epoch 269/500\n",
            "2/2 [==============================] - 0s 95ms/step - loss: 8.3741 - mae: 2.0890 - val_loss: 20.6555 - val_mae: 2.9873\n",
            "\n",
            "Epoch 00269: val_mae improved from 2.99503 to 2.98728, saving model to best_boston.h5\n",
            "Epoch 270/500\n",
            "2/2 [==============================] - 0s 104ms/step - loss: 8.3775 - mae: 2.0851 - val_loss: 20.5945 - val_mae: 2.9813\n",
            "\n",
            "Epoch 00270: val_mae improved from 2.98728 to 2.98134, saving model to best_boston.h5\n",
            "Epoch 271/500\n",
            "2/2 [==============================] - 0s 104ms/step - loss: 8.3626 - mae: 2.0816 - val_loss: 20.5880 - val_mae: 2.9808\n",
            "\n",
            "Epoch 00271: val_mae improved from 2.98134 to 2.98078, saving model to best_boston.h5\n",
            "Epoch 272/500\n",
            "2/2 [==============================] - 0s 108ms/step - loss: 8.3452 - mae: 2.0799 - val_loss: 20.5950 - val_mae: 2.9810\n",
            "\n",
            "Epoch 00272: val_mae did not improve from 2.98078\n",
            "Epoch 273/500\n",
            "2/2 [==============================] - 0s 100ms/step - loss: 8.3339 - mae: 2.0794 - val_loss: 20.6049 - val_mae: 2.9813\n",
            "\n",
            "Epoch 00273: val_mae did not improve from 2.98078\n",
            "Epoch 274/500\n",
            "2/2 [==============================] - 0s 101ms/step - loss: 8.3228 - mae: 2.0795 - val_loss: 20.6274 - val_mae: 2.9828\n",
            "\n",
            "Epoch 00274: val_mae did not improve from 2.98078\n",
            "Epoch 275/500\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 8.3124 - mae: 2.0798 - val_loss: 20.6467 - val_mae: 2.9837\n",
            "\n",
            "Epoch 00275: val_mae did not improve from 2.98078\n",
            "Epoch 276/500\n",
            "2/2 [==============================] - 0s 122ms/step - loss: 8.3058 - mae: 2.0803 - val_loss: 20.6679 - val_mae: 2.9854\n",
            "\n",
            "Epoch 00276: val_mae did not improve from 2.98078\n",
            "Epoch 277/500\n",
            "2/2 [==============================] - 0s 117ms/step - loss: 8.2956 - mae: 2.0801 - val_loss: 20.6832 - val_mae: 2.9867\n",
            "\n",
            "Epoch 00277: val_mae did not improve from 2.98078\n",
            "Epoch 278/500\n",
            "2/2 [==============================] - 0s 112ms/step - loss: 8.2889 - mae: 2.0793 - val_loss: 20.6801 - val_mae: 2.9862\n",
            "\n",
            "Epoch 00278: val_mae did not improve from 2.98078\n",
            "Epoch 279/500\n",
            "2/2 [==============================] - 0s 102ms/step - loss: 8.2782 - mae: 2.0784 - val_loss: 20.6909 - val_mae: 2.9867\n",
            "\n",
            "Epoch 00279: val_mae did not improve from 2.98078\n",
            "Epoch 280/500\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 8.2666 - mae: 2.0766 - val_loss: 20.6683 - val_mae: 2.9846\n",
            "\n",
            "Epoch 00280: val_mae did not improve from 2.98078\n",
            "Epoch 281/500\n",
            "2/2 [==============================] - 0s 108ms/step - loss: 8.2554 - mae: 2.0743 - val_loss: 20.6384 - val_mae: 2.9811\n",
            "\n",
            "Epoch 00281: val_mae did not improve from 2.98078\n",
            "Epoch 282/500\n",
            "2/2 [==============================] - 0s 105ms/step - loss: 8.2465 - mae: 2.0711 - val_loss: 20.5732 - val_mae: 2.9745\n",
            "\n",
            "Epoch 00282: val_mae improved from 2.98078 to 2.97449, saving model to best_boston.h5\n",
            "Epoch 283/500\n",
            "2/2 [==============================] - 0s 101ms/step - loss: 8.2327 - mae: 2.0675 - val_loss: 20.5186 - val_mae: 2.9675\n",
            "\n",
            "Epoch 00283: val_mae improved from 2.97449 to 2.96751, saving model to best_boston.h5\n",
            "Epoch 284/500\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 8.2247 - mae: 2.0653 - val_loss: 20.4849 - val_mae: 2.9635\n",
            "\n",
            "Epoch 00284: val_mae improved from 2.96751 to 2.96352, saving model to best_boston.h5\n",
            "Epoch 285/500\n",
            "2/2 [==============================] - 0s 99ms/step - loss: 8.2143 - mae: 2.0627 - val_loss: 20.4234 - val_mae: 2.9563\n",
            "\n",
            "Epoch 00285: val_mae improved from 2.96352 to 2.95632, saving model to best_boston.h5\n",
            "Epoch 286/500\n",
            "2/2 [==============================] - 0s 108ms/step - loss: 8.2097 - mae: 2.0612 - val_loss: 20.3839 - val_mae: 2.9524\n",
            "\n",
            "Epoch 00286: val_mae improved from 2.95632 to 2.95245, saving model to best_boston.h5\n",
            "Epoch 287/500\n",
            "2/2 [==============================] - 0s 97ms/step - loss: 8.2003 - mae: 2.0591 - val_loss: 20.3446 - val_mae: 2.9474\n",
            "\n",
            "Epoch 00287: val_mae improved from 2.95245 to 2.94744, saving model to best_boston.h5\n",
            "Epoch 288/500\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 8.1911 - mae: 2.0565 - val_loss: 20.3018 - val_mae: 2.9425\n",
            "\n",
            "Epoch 00288: val_mae improved from 2.94744 to 2.94247, saving model to best_boston.h5\n",
            "Epoch 289/500\n",
            "2/2 [==============================] - 0s 107ms/step - loss: 8.1853 - mae: 2.0542 - val_loss: 20.2988 - val_mae: 2.9429\n",
            "\n",
            "Epoch 00289: val_mae did not improve from 2.94247\n",
            "Epoch 290/500\n",
            "2/2 [==============================] - 0s 111ms/step - loss: 8.1765 - mae: 2.0521 - val_loss: 20.3072 - val_mae: 2.9440\n",
            "\n",
            "Epoch 00290: val_mae did not improve from 2.94247\n",
            "Epoch 291/500\n",
            "2/2 [==============================] - 0s 112ms/step - loss: 8.1676 - mae: 2.0506 - val_loss: 20.3211 - val_mae: 2.9454\n",
            "\n",
            "Epoch 00291: val_mae did not improve from 2.94247\n",
            "Epoch 292/500\n",
            "2/2 [==============================] - 0s 128ms/step - loss: 8.1570 - mae: 2.0498 - val_loss: 20.3670 - val_mae: 2.9504\n",
            "\n",
            "Epoch 00292: val_mae did not improve from 2.94247\n",
            "Epoch 293/500\n",
            "2/2 [==============================] - 0s 99ms/step - loss: 8.1449 - mae: 2.0507 - val_loss: 20.4148 - val_mae: 2.9556\n",
            "\n",
            "Epoch 00293: val_mae did not improve from 2.94247\n",
            "Epoch 294/500\n",
            "2/2 [==============================] - 0s 115ms/step - loss: 8.1362 - mae: 2.0518 - val_loss: 20.4758 - val_mae: 2.9608\n",
            "\n",
            "Epoch 00294: val_mae did not improve from 2.94247\n",
            "Epoch 295/500\n",
            "2/2 [==============================] - 0s 101ms/step - loss: 8.1248 - mae: 2.0518 - val_loss: 20.5087 - val_mae: 2.9621\n",
            "\n",
            "Epoch 00295: val_mae did not improve from 2.94247\n",
            "Epoch 296/500\n",
            "2/2 [==============================] - 0s 111ms/step - loss: 8.1173 - mae: 2.0513 - val_loss: 20.5247 - val_mae: 2.9616\n",
            "\n",
            "Epoch 00296: val_mae did not improve from 2.94247\n",
            "Epoch 297/500\n",
            "2/2 [==============================] - 0s 112ms/step - loss: 8.1078 - mae: 2.0514 - val_loss: 20.5247 - val_mae: 2.9607\n",
            "\n",
            "Epoch 00297: val_mae did not improve from 2.94247\n",
            "Epoch 298/500\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 8.1023 - mae: 2.0499 - val_loss: 20.5162 - val_mae: 2.9591\n",
            "\n",
            "Epoch 00298: val_mae did not improve from 2.94247\n",
            "Epoch 299/500\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 8.0939 - mae: 2.0492 - val_loss: 20.5414 - val_mae: 2.9615\n",
            "\n",
            "Epoch 00299: val_mae did not improve from 2.94247\n",
            "Epoch 300/500\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 8.0808 - mae: 2.0481 - val_loss: 20.5086 - val_mae: 2.9573\n",
            "\n",
            "Epoch 00300: val_mae did not improve from 2.94247\n",
            "Epoch 301/500\n",
            "2/2 [==============================] - 0s 114ms/step - loss: 8.0702 - mae: 2.0468 - val_loss: 20.4491 - val_mae: 2.9497\n",
            "\n",
            "Epoch 00301: val_mae did not improve from 2.94247\n",
            "Epoch 302/500\n",
            "2/2 [==============================] - 0s 106ms/step - loss: 8.0611 - mae: 2.0445 - val_loss: 20.3916 - val_mae: 2.9427\n",
            "\n",
            "Epoch 00302: val_mae did not improve from 2.94247\n",
            "Epoch 303/500\n",
            "2/2 [==============================] - 0s 111ms/step - loss: 8.0537 - mae: 2.0418 - val_loss: 20.3461 - val_mae: 2.9379\n",
            "\n",
            "Epoch 00303: val_mae improved from 2.94247 to 2.93786, saving model to best_boston.h5\n",
            "Epoch 304/500\n",
            "2/2 [==============================] - 0s 104ms/step - loss: 8.0449 - mae: 2.0397 - val_loss: 20.3299 - val_mae: 2.9362\n",
            "\n",
            "Epoch 00304: val_mae improved from 2.93786 to 2.93616, saving model to best_boston.h5\n",
            "Epoch 305/500\n",
            "2/2 [==============================] - 0s 100ms/step - loss: 8.0394 - mae: 2.0384 - val_loss: 20.3461 - val_mae: 2.9375\n",
            "\n",
            "Epoch 00305: val_mae did not improve from 2.93616\n",
            "Epoch 306/500\n",
            "2/2 [==============================] - 0s 108ms/step - loss: 8.0264 - mae: 2.0366 - val_loss: 20.3731 - val_mae: 2.9384\n",
            "\n",
            "Epoch 00306: val_mae did not improve from 2.93616\n",
            "Epoch 307/500\n",
            "2/2 [==============================] - 0s 101ms/step - loss: 8.0267 - mae: 2.0346 - val_loss: 20.3760 - val_mae: 2.9375\n",
            "\n",
            "Epoch 00307: val_mae did not improve from 2.93616\n",
            "Epoch 308/500\n",
            "2/2 [==============================] - 0s 130ms/step - loss: 8.0075 - mae: 2.0332 - val_loss: 20.4273 - val_mae: 2.9411\n",
            "\n",
            "Epoch 00308: val_mae did not improve from 2.93616\n",
            "Epoch 309/500\n",
            "2/2 [==============================] - 0s 99ms/step - loss: 7.9986 - mae: 2.0335 - val_loss: 20.4691 - val_mae: 2.9440\n",
            "\n",
            "Epoch 00309: val_mae did not improve from 2.93616\n",
            "Epoch 310/500\n",
            "2/2 [==============================] - 0s 111ms/step - loss: 7.9888 - mae: 2.0340 - val_loss: 20.5116 - val_mae: 2.9457\n",
            "\n",
            "Epoch 00310: val_mae did not improve from 2.93616\n",
            "Epoch 311/500\n",
            "2/2 [==============================] - 0s 109ms/step - loss: 7.9789 - mae: 2.0352 - val_loss: 20.5643 - val_mae: 2.9484\n",
            "\n",
            "Epoch 00311: val_mae did not improve from 2.93616\n",
            "Epoch 312/500\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 7.9824 - mae: 2.0378 - val_loss: 20.6036 - val_mae: 2.9496\n",
            "\n",
            "Epoch 00312: val_mae did not improve from 2.93616\n",
            "Epoch 313/500\n",
            "2/2 [==============================] - 0s 102ms/step - loss: 7.9707 - mae: 2.0378 - val_loss: 20.5774 - val_mae: 2.9452\n",
            "\n",
            "Epoch 00313: val_mae did not improve from 2.93616\n",
            "Epoch 314/500\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 7.9562 - mae: 2.0363 - val_loss: 20.5770 - val_mae: 2.9451\n",
            "\n",
            "Epoch 00314: val_mae did not improve from 2.93616\n",
            "Epoch 315/500\n",
            "2/2 [==============================] - 0s 115ms/step - loss: 7.9508 - mae: 2.0347 - val_loss: 20.5575 - val_mae: 2.9447\n",
            "\n",
            "Epoch 00315: val_mae did not improve from 2.93616\n",
            "Epoch 316/500\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 7.9387 - mae: 2.0316 - val_loss: 20.5357 - val_mae: 2.9434\n",
            "\n",
            "Epoch 00316: val_mae did not improve from 2.93616\n",
            "Epoch 317/500\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 7.9259 - mae: 2.0287 - val_loss: 20.5022 - val_mae: 2.9409\n",
            "\n",
            "Epoch 00317: val_mae did not improve from 2.93616\n",
            "Epoch 318/500\n",
            "2/2 [==============================] - 0s 107ms/step - loss: 7.9161 - mae: 2.0264 - val_loss: 20.4646 - val_mae: 2.9375\n",
            "\n",
            "Epoch 00318: val_mae did not improve from 2.93616\n",
            "Epoch 319/500\n",
            "2/2 [==============================] - 0s 112ms/step - loss: 7.9093 - mae: 2.0250 - val_loss: 20.4340 - val_mae: 2.9351\n",
            "\n",
            "Epoch 00319: val_mae improved from 2.93616 to 2.93510, saving model to best_boston.h5\n",
            "Epoch 320/500\n",
            "2/2 [==============================] - 0s 100ms/step - loss: 7.9000 - mae: 2.0236 - val_loss: 20.4074 - val_mae: 2.9325\n",
            "\n",
            "Epoch 00320: val_mae improved from 2.93510 to 2.93246, saving model to best_boston.h5\n",
            "Epoch 321/500\n",
            "2/2 [==============================] - 0s 104ms/step - loss: 7.8956 - mae: 2.0212 - val_loss: 20.3747 - val_mae: 2.9287\n",
            "\n",
            "Epoch 00321: val_mae improved from 2.93246 to 2.92869, saving model to best_boston.h5\n",
            "Epoch 322/500\n",
            "2/2 [==============================] - 0s 108ms/step - loss: 7.8828 - mae: 2.0189 - val_loss: 20.3806 - val_mae: 2.9288\n",
            "\n",
            "Epoch 00322: val_mae did not improve from 2.92869\n",
            "Epoch 323/500\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 7.8779 - mae: 2.0174 - val_loss: 20.3738 - val_mae: 2.9288\n",
            "\n",
            "Epoch 00323: val_mae did not improve from 2.92869\n",
            "Epoch 324/500\n",
            "2/2 [==============================] - 0s 112ms/step - loss: 7.8632 - mae: 2.0159 - val_loss: 20.3993 - val_mae: 2.9315\n",
            "\n",
            "Epoch 00324: val_mae did not improve from 2.92869\n",
            "Epoch 325/500\n",
            "2/2 [==============================] - 0s 104ms/step - loss: 7.8580 - mae: 2.0160 - val_loss: 20.4217 - val_mae: 2.9343\n",
            "\n",
            "Epoch 00325: val_mae did not improve from 2.92869\n",
            "Epoch 326/500\n",
            "2/2 [==============================] - 0s 106ms/step - loss: 7.8479 - mae: 2.0158 - val_loss: 20.4589 - val_mae: 2.9376\n",
            "\n",
            "Epoch 00326: val_mae did not improve from 2.92869\n",
            "Epoch 327/500\n",
            "2/2 [==============================] - 0s 101ms/step - loss: 7.8444 - mae: 2.0169 - val_loss: 20.4539 - val_mae: 2.9346\n",
            "\n",
            "Epoch 00327: val_mae did not improve from 2.92869\n",
            "Epoch 328/500\n",
            "2/2 [==============================] - 0s 114ms/step - loss: 7.8282 - mae: 2.0166 - val_loss: 20.4927 - val_mae: 2.9365\n",
            "\n",
            "Epoch 00328: val_mae did not improve from 2.92869\n",
            "Epoch 329/500\n",
            "2/2 [==============================] - 0s 106ms/step - loss: 7.8215 - mae: 2.0181 - val_loss: 20.5158 - val_mae: 2.9375\n",
            "\n",
            "Epoch 00329: val_mae did not improve from 2.92869\n",
            "Epoch 330/500\n",
            "2/2 [==============================] - 0s 106ms/step - loss: 7.8218 - mae: 2.0190 - val_loss: 20.5406 - val_mae: 2.9387\n",
            "\n",
            "Epoch 00330: val_mae did not improve from 2.92869\n",
            "Epoch 331/500\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 7.8069 - mae: 2.0181 - val_loss: 20.5013 - val_mae: 2.9314\n",
            "\n",
            "Epoch 00331: val_mae did not improve from 2.92869\n",
            "Epoch 332/500\n",
            "2/2 [==============================] - 0s 112ms/step - loss: 7.7988 - mae: 2.0158 - val_loss: 20.4699 - val_mae: 2.9265\n",
            "\n",
            "Epoch 00332: val_mae improved from 2.92869 to 2.92645, saving model to best_boston.h5\n",
            "Epoch 333/500\n",
            "2/2 [==============================] - 0s 115ms/step - loss: 7.7869 - mae: 2.0133 - val_loss: 20.4533 - val_mae: 2.9230\n",
            "\n",
            "Epoch 00333: val_mae improved from 2.92645 to 2.92297, saving model to best_boston.h5\n",
            "Epoch 334/500\n",
            "2/2 [==============================] - 0s 117ms/step - loss: 7.7797 - mae: 2.0123 - val_loss: 20.4450 - val_mae: 2.9209\n",
            "\n",
            "Epoch 00334: val_mae improved from 2.92297 to 2.92089, saving model to best_boston.h5\n",
            "Epoch 335/500\n",
            "2/2 [==============================] - 0s 104ms/step - loss: 7.7702 - mae: 2.0113 - val_loss: 20.4425 - val_mae: 2.9198\n",
            "\n",
            "Epoch 00335: val_mae improved from 2.92089 to 2.91977, saving model to best_boston.h5\n",
            "Epoch 336/500\n",
            "2/2 [==============================] - 0s 111ms/step - loss: 7.7635 - mae: 2.0097 - val_loss: 20.4362 - val_mae: 2.9197\n",
            "\n",
            "Epoch 00336: val_mae improved from 2.91977 to 2.91975, saving model to best_boston.h5\n",
            "Epoch 337/500\n",
            "2/2 [==============================] - 0s 109ms/step - loss: 7.7539 - mae: 2.0076 - val_loss: 20.4177 - val_mae: 2.9196\n",
            "\n",
            "Epoch 00337: val_mae improved from 2.91975 to 2.91961, saving model to best_boston.h5\n",
            "Epoch 338/500\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 7.7495 - mae: 2.0060 - val_loss: 20.4185 - val_mae: 2.9222\n",
            "\n",
            "Epoch 00338: val_mae did not improve from 2.91961\n",
            "Epoch 339/500\n",
            "2/2 [==============================] - 0s 112ms/step - loss: 7.7373 - mae: 2.0036 - val_loss: 20.3955 - val_mae: 2.9211\n",
            "\n",
            "Epoch 00339: val_mae did not improve from 2.91961\n",
            "Epoch 340/500\n",
            "2/2 [==============================] - 0s 105ms/step - loss: 7.7272 - mae: 2.0012 - val_loss: 20.3550 - val_mae: 2.9173\n",
            "\n",
            "Epoch 00340: val_mae improved from 2.91961 to 2.91729, saving model to best_boston.h5\n",
            "Epoch 341/500\n",
            "2/2 [==============================] - 0s 104ms/step - loss: 7.7206 - mae: 1.9985 - val_loss: 20.3101 - val_mae: 2.9127\n",
            "\n",
            "Epoch 00341: val_mae improved from 2.91729 to 2.91273, saving model to best_boston.h5\n",
            "Epoch 342/500\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 7.7146 - mae: 1.9959 - val_loss: 20.2733 - val_mae: 2.9092\n",
            "\n",
            "Epoch 00342: val_mae improved from 2.91273 to 2.90922, saving model to best_boston.h5\n",
            "Epoch 343/500\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 7.7119 - mae: 1.9936 - val_loss: 20.2552 - val_mae: 2.9073\n",
            "\n",
            "Epoch 00343: val_mae improved from 2.90922 to 2.90730, saving model to best_boston.h5\n",
            "Epoch 344/500\n",
            "2/2 [==============================] - 0s 98ms/step - loss: 7.7015 - mae: 1.9919 - val_loss: 20.2712 - val_mae: 2.9076\n",
            "\n",
            "Epoch 00344: val_mae did not improve from 2.90730\n",
            "Epoch 345/500\n",
            "2/2 [==============================] - 0s 115ms/step - loss: 7.6905 - mae: 1.9919 - val_loss: 20.3251 - val_mae: 2.9124\n",
            "\n",
            "Epoch 00345: val_mae did not improve from 2.90730\n",
            "Epoch 346/500\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 7.6919 - mae: 1.9937 - val_loss: 20.3995 - val_mae: 2.9188\n",
            "\n",
            "Epoch 00346: val_mae did not improve from 2.90730\n",
            "Epoch 347/500\n",
            "2/2 [==============================] - 0s 123ms/step - loss: 7.6751 - mae: 1.9946 - val_loss: 20.4072 - val_mae: 2.9181\n",
            "\n",
            "Epoch 00347: val_mae did not improve from 2.90730\n",
            "Epoch 348/500\n",
            "2/2 [==============================] - 0s 100ms/step - loss: 7.6644 - mae: 1.9941 - val_loss: 20.4249 - val_mae: 2.9201\n",
            "\n",
            "Epoch 00348: val_mae did not improve from 2.90730\n",
            "Epoch 349/500\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 7.6584 - mae: 1.9938 - val_loss: 20.4316 - val_mae: 2.9210\n",
            "\n",
            "Epoch 00349: val_mae did not improve from 2.90730\n",
            "Epoch 350/500\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 7.6519 - mae: 1.9933 - val_loss: 20.4039 - val_mae: 2.9182\n",
            "\n",
            "Epoch 00350: val_mae did not improve from 2.90730\n",
            "Epoch 351/500\n",
            "2/2 [==============================] - 0s 112ms/step - loss: 7.6444 - mae: 1.9922 - val_loss: 20.3451 - val_mae: 2.9114\n",
            "\n",
            "Epoch 00351: val_mae did not improve from 2.90730\n",
            "Epoch 352/500\n",
            "2/2 [==============================] - 0s 109ms/step - loss: 7.6345 - mae: 1.9901 - val_loss: 20.2920 - val_mae: 2.9058\n",
            "\n",
            "Epoch 00352: val_mae improved from 2.90730 to 2.90580, saving model to best_boston.h5\n",
            "Epoch 353/500\n",
            "2/2 [==============================] - 0s 115ms/step - loss: 7.6353 - mae: 1.9883 - val_loss: 20.2280 - val_mae: 2.9009\n",
            "\n",
            "Epoch 00353: val_mae improved from 2.90580 to 2.90086, saving model to best_boston.h5\n",
            "Epoch 354/500\n",
            "2/2 [==============================] - 0s 112ms/step - loss: 7.6223 - mae: 1.9855 - val_loss: 20.2329 - val_mae: 2.9038\n",
            "\n",
            "Epoch 00354: val_mae did not improve from 2.90086\n",
            "Epoch 355/500\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 7.6115 - mae: 1.9848 - val_loss: 20.2285 - val_mae: 2.9053\n",
            "\n",
            "Epoch 00355: val_mae did not improve from 2.90086\n",
            "Epoch 356/500\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 7.6065 - mae: 1.9834 - val_loss: 20.2211 - val_mae: 2.9059\n",
            "\n",
            "Epoch 00356: val_mae did not improve from 2.90086\n",
            "Epoch 357/500\n",
            "2/2 [==============================] - 0s 127ms/step - loss: 7.5994 - mae: 1.9825 - val_loss: 20.2236 - val_mae: 2.9065\n",
            "\n",
            "Epoch 00357: val_mae did not improve from 2.90086\n",
            "Epoch 358/500\n",
            "2/2 [==============================] - 0s 108ms/step - loss: 7.5906 - mae: 1.9811 - val_loss: 20.1970 - val_mae: 2.9038\n",
            "\n",
            "Epoch 00358: val_mae did not improve from 2.90086\n",
            "Epoch 359/500\n",
            "2/2 [==============================] - 0s 127ms/step - loss: 7.5823 - mae: 1.9799 - val_loss: 20.1862 - val_mae: 2.9020\n",
            "\n",
            "Epoch 00359: val_mae did not improve from 2.90086\n",
            "Epoch 360/500\n",
            "2/2 [==============================] - 0s 105ms/step - loss: 7.5786 - mae: 1.9797 - val_loss: 20.2015 - val_mae: 2.9043\n",
            "\n",
            "Epoch 00360: val_mae did not improve from 2.90086\n",
            "Epoch 361/500\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 7.5668 - mae: 1.9784 - val_loss: 20.2104 - val_mae: 2.9044\n",
            "\n",
            "Epoch 00361: val_mae did not improve from 2.90086\n",
            "Epoch 362/500\n",
            "2/2 [==============================] - 0s 125ms/step - loss: 7.5602 - mae: 1.9786 - val_loss: 20.2152 - val_mae: 2.9029\n",
            "\n",
            "Epoch 00362: val_mae did not improve from 2.90086\n",
            "Epoch 363/500\n",
            "2/2 [==============================] - 0s 109ms/step - loss: 7.5563 - mae: 1.9790 - val_loss: 20.2142 - val_mae: 2.9002\n",
            "\n",
            "Epoch 00363: val_mae improved from 2.90086 to 2.90020, saving model to best_boston.h5\n",
            "Epoch 364/500\n",
            "2/2 [==============================] - 0s 103ms/step - loss: 7.5519 - mae: 1.9792 - val_loss: 20.2200 - val_mae: 2.8993\n",
            "\n",
            "Epoch 00364: val_mae improved from 2.90020 to 2.89925, saving model to best_boston.h5\n",
            "Epoch 365/500\n",
            "2/2 [==============================] - 0s 112ms/step - loss: 7.5435 - mae: 1.9781 - val_loss: 20.2431 - val_mae: 2.9019\n",
            "\n",
            "Epoch 00365: val_mae did not improve from 2.89925\n",
            "Epoch 366/500\n",
            "2/2 [==============================] - 0s 107ms/step - loss: 7.5351 - mae: 1.9771 - val_loss: 20.2613 - val_mae: 2.9033\n",
            "\n",
            "Epoch 00366: val_mae did not improve from 2.89925\n",
            "Epoch 367/500\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 7.5273 - mae: 1.9763 - val_loss: 20.3004 - val_mae: 2.9071\n",
            "\n",
            "Epoch 00367: val_mae did not improve from 2.89925\n",
            "Epoch 368/500\n",
            "2/2 [==============================] - 0s 128ms/step - loss: 7.5221 - mae: 1.9765 - val_loss: 20.3155 - val_mae: 2.9075\n",
            "\n",
            "Epoch 00368: val_mae did not improve from 2.89925\n",
            "Epoch 369/500\n",
            "2/2 [==============================] - 0s 102ms/step - loss: 7.5143 - mae: 1.9753 - val_loss: 20.2909 - val_mae: 2.9042\n",
            "\n",
            "Epoch 00369: val_mae did not improve from 2.89925\n",
            "Epoch 370/500\n",
            "2/2 [==============================] - 0s 108ms/step - loss: 7.5035 - mae: 1.9731 - val_loss: 20.2815 - val_mae: 2.9041\n",
            "\n",
            "Epoch 00370: val_mae did not improve from 2.89925\n",
            "Epoch 371/500\n",
            "2/2 [==============================] - 0s 106ms/step - loss: 7.4952 - mae: 1.9711 - val_loss: 20.2581 - val_mae: 2.9022\n",
            "\n",
            "Epoch 00371: val_mae did not improve from 2.89925\n",
            "Epoch 372/500\n",
            "2/2 [==============================] - 0s 104ms/step - loss: 7.4870 - mae: 1.9687 - val_loss: 20.2154 - val_mae: 2.8979\n",
            "\n",
            "Epoch 00372: val_mae improved from 2.89925 to 2.89791, saving model to best_boston.h5\n",
            "Epoch 373/500\n",
            "2/2 [==============================] - 0s 108ms/step - loss: 7.4771 - mae: 1.9657 - val_loss: 20.1649 - val_mae: 2.8934\n",
            "\n",
            "Epoch 00373: val_mae improved from 2.89791 to 2.89338, saving model to best_boston.h5\n",
            "Epoch 374/500\n",
            "2/2 [==============================] - 0s 104ms/step - loss: 7.4787 - mae: 1.9639 - val_loss: 20.1102 - val_mae: 2.8899\n",
            "\n",
            "Epoch 00374: val_mae improved from 2.89338 to 2.88992, saving model to best_boston.h5\n",
            "Epoch 375/500\n",
            "2/2 [==============================] - 0s 115ms/step - loss: 7.4694 - mae: 1.9610 - val_loss: 20.1163 - val_mae: 2.8942\n",
            "\n",
            "Epoch 00375: val_mae did not improve from 2.88992\n",
            "Epoch 376/500\n",
            "2/2 [==============================] - 0s 129ms/step - loss: 7.4671 - mae: 1.9608 - val_loss: 20.1145 - val_mae: 2.8959\n",
            "\n",
            "Epoch 00376: val_mae did not improve from 2.88992\n",
            "Epoch 377/500\n",
            "2/2 [==============================] - 0s 103ms/step - loss: 7.4557 - mae: 1.9601 - val_loss: 20.0727 - val_mae: 2.8922\n",
            "\n",
            "Epoch 00377: val_mae did not improve from 2.88992\n",
            "Epoch 378/500\n",
            "2/2 [==============================] - 0s 109ms/step - loss: 7.4482 - mae: 1.9587 - val_loss: 20.0483 - val_mae: 2.8901\n",
            "\n",
            "Epoch 00378: val_mae did not improve from 2.88992\n",
            "Epoch 379/500\n",
            "2/2 [==============================] - 0s 114ms/step - loss: 7.4423 - mae: 1.9578 - val_loss: 20.0044 - val_mae: 2.8848\n",
            "\n",
            "Epoch 00379: val_mae improved from 2.88992 to 2.88479, saving model to best_boston.h5\n",
            "Epoch 380/500\n",
            "2/2 [==============================] - 0s 102ms/step - loss: 7.4327 - mae: 1.9562 - val_loss: 20.0192 - val_mae: 2.8857\n",
            "\n",
            "Epoch 00380: val_mae did not improve from 2.88479\n",
            "Epoch 381/500\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 7.4238 - mae: 1.9556 - val_loss: 20.0048 - val_mae: 2.8848\n",
            "\n",
            "Epoch 00381: val_mae did not improve from 2.88479\n",
            "Epoch 382/500\n",
            "2/2 [==============================] - 0s 109ms/step - loss: 7.4163 - mae: 1.9552 - val_loss: 20.0002 - val_mae: 2.8841\n",
            "\n",
            "Epoch 00382: val_mae improved from 2.88479 to 2.88413, saving model to best_boston.h5\n",
            "Epoch 383/500\n",
            "2/2 [==============================] - 0s 115ms/step - loss: 7.4095 - mae: 1.9553 - val_loss: 20.0059 - val_mae: 2.8836\n",
            "\n",
            "Epoch 00383: val_mae improved from 2.88413 to 2.88356, saving model to best_boston.h5\n",
            "Epoch 384/500\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 7.4045 - mae: 1.9555 - val_loss: 20.0249 - val_mae: 2.8843\n",
            "\n",
            "Epoch 00384: val_mae did not improve from 2.88356\n",
            "Epoch 385/500\n",
            "2/2 [==============================] - 0s 104ms/step - loss: 7.3999 - mae: 1.9562 - val_loss: 20.0222 - val_mae: 2.8811\n",
            "\n",
            "Epoch 00385: val_mae improved from 2.88356 to 2.88106, saving model to best_boston.h5\n",
            "Epoch 386/500\n",
            "2/2 [==============================] - 0s 114ms/step - loss: 7.3912 - mae: 1.9550 - val_loss: 20.0193 - val_mae: 2.8799\n",
            "\n",
            "Epoch 00386: val_mae improved from 2.88106 to 2.87993, saving model to best_boston.h5\n",
            "Epoch 387/500\n",
            "2/2 [==============================] - 0s 106ms/step - loss: 7.3866 - mae: 1.9540 - val_loss: 20.0081 - val_mae: 2.8775\n",
            "\n",
            "Epoch 00387: val_mae improved from 2.87993 to 2.87754, saving model to best_boston.h5\n",
            "Epoch 388/500\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 7.3827 - mae: 1.9522 - val_loss: 19.9820 - val_mae: 2.8776\n",
            "\n",
            "Epoch 00388: val_mae did not improve from 2.87754\n",
            "Epoch 389/500\n",
            "2/2 [==============================] - 0s 104ms/step - loss: 7.3681 - mae: 1.9484 - val_loss: 19.9496 - val_mae: 2.8754\n",
            "\n",
            "Epoch 00389: val_mae improved from 2.87754 to 2.87544, saving model to best_boston.h5\n",
            "Epoch 390/500\n",
            "2/2 [==============================] - 0s 107ms/step - loss: 7.3617 - mae: 1.9466 - val_loss: 19.9170 - val_mae: 2.8727\n",
            "\n",
            "Epoch 00390: val_mae improved from 2.87544 to 2.87268, saving model to best_boston.h5\n",
            "Epoch 391/500\n",
            "2/2 [==============================] - 0s 109ms/step - loss: 7.3556 - mae: 1.9451 - val_loss: 19.8944 - val_mae: 2.8713\n",
            "\n",
            "Epoch 00391: val_mae improved from 2.87268 to 2.87130, saving model to best_boston.h5\n",
            "Epoch 392/500\n",
            "2/2 [==============================] - 0s 124ms/step - loss: 7.3492 - mae: 1.9442 - val_loss: 19.9025 - val_mae: 2.8718\n",
            "\n",
            "Epoch 00392: val_mae did not improve from 2.87130\n",
            "Epoch 393/500\n",
            "2/2 [==============================] - 0s 105ms/step - loss: 7.3398 - mae: 1.9445 - val_loss: 19.9626 - val_mae: 2.8786\n",
            "\n",
            "Epoch 00393: val_mae did not improve from 2.87130\n",
            "Epoch 394/500\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 7.3311 - mae: 1.9458 - val_loss: 19.9986 - val_mae: 2.8844\n",
            "\n",
            "Epoch 00394: val_mae did not improve from 2.87130\n",
            "Epoch 395/500\n",
            "2/2 [==============================] - 0s 102ms/step - loss: 7.3335 - mae: 1.9473 - val_loss: 20.0442 - val_mae: 2.8909\n",
            "\n",
            "Epoch 00395: val_mae did not improve from 2.87130\n",
            "Epoch 396/500\n",
            "2/2 [==============================] - 0s 115ms/step - loss: 7.3204 - mae: 1.9464 - val_loss: 20.0328 - val_mae: 2.8871\n",
            "\n",
            "Epoch 00396: val_mae did not improve from 2.87130\n",
            "Epoch 397/500\n",
            "2/2 [==============================] - 0s 111ms/step - loss: 7.3146 - mae: 1.9461 - val_loss: 20.0419 - val_mae: 2.8847\n",
            "\n",
            "Epoch 00397: val_mae did not improve from 2.87130\n",
            "Epoch 398/500\n",
            "2/2 [==============================] - 0s 115ms/step - loss: 7.3100 - mae: 1.9454 - val_loss: 20.0309 - val_mae: 2.8818\n",
            "\n",
            "Epoch 00398: val_mae did not improve from 2.87130\n",
            "Epoch 399/500\n",
            "2/2 [==============================] - 0s 105ms/step - loss: 7.3001 - mae: 1.9444 - val_loss: 20.0362 - val_mae: 2.8794\n",
            "\n",
            "Epoch 00399: val_mae did not improve from 2.87130\n",
            "Epoch 400/500\n",
            "2/2 [==============================] - 0s 116ms/step - loss: 7.2924 - mae: 1.9437 - val_loss: 20.0204 - val_mae: 2.8752\n",
            "\n",
            "Epoch 00400: val_mae did not improve from 2.87130\n",
            "Epoch 401/500\n",
            "2/2 [==============================] - 0s 105ms/step - loss: 7.2856 - mae: 1.9423 - val_loss: 19.9981 - val_mae: 2.8706\n",
            "\n",
            "Epoch 00401: val_mae improved from 2.87130 to 2.87059, saving model to best_boston.h5\n",
            "Epoch 402/500\n",
            "2/2 [==============================] - 0s 114ms/step - loss: 7.2786 - mae: 1.9405 - val_loss: 19.9826 - val_mae: 2.8682\n",
            "\n",
            "Epoch 00402: val_mae improved from 2.87059 to 2.86819, saving model to best_boston.h5\n",
            "Epoch 403/500\n",
            "2/2 [==============================] - 0s 115ms/step - loss: 7.2784 - mae: 1.9400 - val_loss: 19.9778 - val_mae: 2.8707\n",
            "\n",
            "Epoch 00403: val_mae did not improve from 2.86819\n",
            "Epoch 404/500\n",
            "2/2 [==============================] - 0s 114ms/step - loss: 7.2623 - mae: 1.9366 - val_loss: 19.9949 - val_mae: 2.8738\n",
            "\n",
            "Epoch 00404: val_mae did not improve from 2.86819\n",
            "Epoch 405/500\n",
            "2/2 [==============================] - 0s 115ms/step - loss: 7.2561 - mae: 1.9365 - val_loss: 20.0289 - val_mae: 2.8801\n",
            "\n",
            "Epoch 00405: val_mae did not improve from 2.86819\n",
            "Epoch 406/500\n",
            "2/2 [==============================] - 0s 116ms/step - loss: 7.2501 - mae: 1.9364 - val_loss: 20.0500 - val_mae: 2.8837\n",
            "\n",
            "Epoch 00406: val_mae did not improve from 2.86819\n",
            "Epoch 407/500\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 7.2423 - mae: 1.9358 - val_loss: 20.0256 - val_mae: 2.8805\n",
            "\n",
            "Epoch 00407: val_mae did not improve from 2.86819\n",
            "Epoch 408/500\n",
            "2/2 [==============================] - 0s 112ms/step - loss: 7.2358 - mae: 1.9352 - val_loss: 19.9947 - val_mae: 2.8753\n",
            "\n",
            "Epoch 00408: val_mae did not improve from 2.86819\n",
            "Epoch 409/500\n",
            "2/2 [==============================] - 0s 122ms/step - loss: 7.2277 - mae: 1.9331 - val_loss: 19.9543 - val_mae: 2.8663\n",
            "\n",
            "Epoch 00409: val_mae improved from 2.86819 to 2.86631, saving model to best_boston.h5\n",
            "Epoch 410/500\n",
            "2/2 [==============================] - 0s 105ms/step - loss: 7.2231 - mae: 1.9323 - val_loss: 19.9163 - val_mae: 2.8604\n",
            "\n",
            "Epoch 00410: val_mae improved from 2.86631 to 2.86035, saving model to best_boston.h5\n",
            "Epoch 411/500\n",
            "2/2 [==============================] - 0s 106ms/step - loss: 7.2204 - mae: 1.9308 - val_loss: 19.9179 - val_mae: 2.8603\n",
            "\n",
            "Epoch 00411: val_mae improved from 2.86035 to 2.86029, saving model to best_boston.h5\n",
            "Epoch 412/500\n",
            "2/2 [==============================] - 0s 102ms/step - loss: 7.2070 - mae: 1.9296 - val_loss: 19.9400 - val_mae: 2.8642\n",
            "\n",
            "Epoch 00412: val_mae did not improve from 2.86029\n",
            "Epoch 413/500\n",
            "2/2 [==============================] - 0s 122ms/step - loss: 7.2094 - mae: 1.9307 - val_loss: 19.9651 - val_mae: 2.8705\n",
            "\n",
            "Epoch 00413: val_mae did not improve from 2.86029\n",
            "Epoch 414/500\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 7.1996 - mae: 1.9317 - val_loss: 19.9485 - val_mae: 2.8688\n",
            "\n",
            "Epoch 00414: val_mae did not improve from 2.86029\n",
            "Epoch 415/500\n",
            "2/2 [==============================] - 0s 111ms/step - loss: 7.1904 - mae: 1.9294 - val_loss: 19.9275 - val_mae: 2.8706\n",
            "\n",
            "Epoch 00415: val_mae did not improve from 2.86029\n",
            "Epoch 416/500\n",
            "2/2 [==============================] - 0s 107ms/step - loss: 7.1782 - mae: 1.9275 - val_loss: 19.9068 - val_mae: 2.8725\n",
            "\n",
            "Epoch 00416: val_mae did not improve from 2.86029\n",
            "Epoch 417/500\n",
            "2/2 [==============================] - 0s 127ms/step - loss: 7.1771 - mae: 1.9279 - val_loss: 19.8621 - val_mae: 2.8723\n",
            "\n",
            "Epoch 00417: val_mae did not improve from 2.86029\n",
            "Epoch 418/500\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 7.1643 - mae: 1.9255 - val_loss: 19.7991 - val_mae: 2.8675\n",
            "\n",
            "Epoch 00418: val_mae did not improve from 2.86029\n",
            "Epoch 419/500\n",
            "2/2 [==============================] - 0s 111ms/step - loss: 7.1567 - mae: 1.9224 - val_loss: 19.7296 - val_mae: 2.8609\n",
            "\n",
            "Epoch 00419: val_mae did not improve from 2.86029\n",
            "Epoch 420/500\n",
            "2/2 [==============================] - 0s 115ms/step - loss: 7.1679 - mae: 1.9212 - val_loss: 19.6751 - val_mae: 2.8551\n",
            "\n",
            "Epoch 00420: val_mae improved from 2.86029 to 2.85514, saving model to best_boston.h5\n",
            "Epoch 421/500\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 7.1495 - mae: 1.9167 - val_loss: 19.6544 - val_mae: 2.8513\n",
            "\n",
            "Epoch 00421: val_mae improved from 2.85514 to 2.85126, saving model to best_boston.h5\n",
            "Epoch 422/500\n",
            "2/2 [==============================] - 0s 103ms/step - loss: 7.1451 - mae: 1.9164 - val_loss: 19.6603 - val_mae: 2.8492\n",
            "\n",
            "Epoch 00422: val_mae improved from 2.85126 to 2.84919, saving model to best_boston.h5\n",
            "Epoch 423/500\n",
            "2/2 [==============================] - 0s 112ms/step - loss: 7.1323 - mae: 1.9164 - val_loss: 19.7134 - val_mae: 2.8545\n",
            "\n",
            "Epoch 00423: val_mae did not improve from 2.84919\n",
            "Epoch 424/500\n",
            "2/2 [==============================] - 0s 112ms/step - loss: 7.1258 - mae: 1.9172 - val_loss: 19.7897 - val_mae: 2.8613\n",
            "\n",
            "Epoch 00424: val_mae did not improve from 2.84919\n",
            "Epoch 425/500\n",
            "2/2 [==============================] - 0s 126ms/step - loss: 7.1332 - mae: 1.9209 - val_loss: 19.8475 - val_mae: 2.8655\n",
            "\n",
            "Epoch 00425: val_mae did not improve from 2.84919\n",
            "Epoch 426/500\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 7.1233 - mae: 1.9220 - val_loss: 19.8322 - val_mae: 2.8607\n",
            "\n",
            "Epoch 00426: val_mae did not improve from 2.84919\n",
            "Epoch 427/500\n",
            "2/2 [==============================] - 0s 116ms/step - loss: 7.1110 - mae: 1.9198 - val_loss: 19.7659 - val_mae: 2.8523\n",
            "\n",
            "Epoch 00427: val_mae did not improve from 2.84919\n",
            "Epoch 428/500\n",
            "2/2 [==============================] - 0s 106ms/step - loss: 7.1078 - mae: 1.9172 - val_loss: 19.6997 - val_mae: 2.8438\n",
            "\n",
            "Epoch 00428: val_mae improved from 2.84919 to 2.84382, saving model to best_boston.h5\n",
            "Epoch 429/500\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 7.0925 - mae: 1.9133 - val_loss: 19.6818 - val_mae: 2.8413\n",
            "\n",
            "Epoch 00429: val_mae improved from 2.84382 to 2.84129, saving model to best_boston.h5\n",
            "Epoch 430/500\n",
            "2/2 [==============================] - 0s 117ms/step - loss: 7.0904 - mae: 1.9128 - val_loss: 19.6364 - val_mae: 2.8366\n",
            "\n",
            "Epoch 00430: val_mae improved from 2.84129 to 2.83657, saving model to best_boston.h5\n",
            "Epoch 431/500\n",
            "2/2 [==============================] - 0s 108ms/step - loss: 7.0849 - mae: 1.9103 - val_loss: 19.6472 - val_mae: 2.8391\n",
            "\n",
            "Epoch 00431: val_mae did not improve from 2.83657\n",
            "Epoch 432/500\n",
            "2/2 [==============================] - 0s 115ms/step - loss: 7.0713 - mae: 1.9088 - val_loss: 19.6753 - val_mae: 2.8413\n",
            "\n",
            "Epoch 00432: val_mae did not improve from 2.83657\n",
            "Epoch 433/500\n",
            "2/2 [==============================] - 0s 138ms/step - loss: 7.0656 - mae: 1.9101 - val_loss: 19.7287 - val_mae: 2.8492\n",
            "\n",
            "Epoch 00433: val_mae did not improve from 2.83657\n",
            "Epoch 434/500\n",
            "2/2 [==============================] - 0s 115ms/step - loss: 7.0610 - mae: 1.9107 - val_loss: 19.7612 - val_mae: 2.8558\n",
            "\n",
            "Epoch 00434: val_mae did not improve from 2.83657\n",
            "Epoch 435/500\n",
            "2/2 [==============================] - 0s 115ms/step - loss: 7.0572 - mae: 1.9105 - val_loss: 19.7572 - val_mae: 2.8600\n",
            "\n",
            "Epoch 00435: val_mae did not improve from 2.83657\n",
            "Epoch 436/500\n",
            "2/2 [==============================] - 0s 127ms/step - loss: 7.0484 - mae: 1.9096 - val_loss: 19.7091 - val_mae: 2.8561\n",
            "\n",
            "Epoch 00436: val_mae did not improve from 2.83657\n",
            "Epoch 437/500\n",
            "2/2 [==============================] - 0s 107ms/step - loss: 7.0392 - mae: 1.9071 - val_loss: 19.6416 - val_mae: 2.8471\n",
            "\n",
            "Epoch 00437: val_mae did not improve from 2.83657\n",
            "Epoch 438/500\n",
            "2/2 [==============================] - 0s 123ms/step - loss: 7.0310 - mae: 1.9041 - val_loss: 19.5993 - val_mae: 2.8408\n",
            "\n",
            "Epoch 00438: val_mae did not improve from 2.83657\n",
            "Epoch 439/500\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 7.0283 - mae: 1.9022 - val_loss: 19.5920 - val_mae: 2.8376\n",
            "\n",
            "Epoch 00439: val_mae did not improve from 2.83657\n",
            "Epoch 440/500\n",
            "2/2 [==============================] - 0s 107ms/step - loss: 7.0219 - mae: 1.9017 - val_loss: 19.6014 - val_mae: 2.8393\n",
            "\n",
            "Epoch 00440: val_mae did not improve from 2.83657\n",
            "Epoch 441/500\n",
            "2/2 [==============================] - 0s 108ms/step - loss: 7.0109 - mae: 1.9008 - val_loss: 19.5906 - val_mae: 2.8377\n",
            "\n",
            "Epoch 00441: val_mae did not improve from 2.83657\n",
            "Epoch 442/500\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 7.0064 - mae: 1.9014 - val_loss: 19.5829 - val_mae: 2.8366\n",
            "\n",
            "Epoch 00442: val_mae did not improve from 2.83657\n",
            "Epoch 443/500\n",
            "2/2 [==============================] - 0s 111ms/step - loss: 6.9974 - mae: 1.9023 - val_loss: 19.5572 - val_mae: 2.8338\n",
            "\n",
            "Epoch 00443: val_mae improved from 2.83657 to 2.83385, saving model to best_boston.h5\n",
            "Epoch 444/500\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 6.9926 - mae: 1.9026 - val_loss: 19.5215 - val_mae: 2.8307\n",
            "\n",
            "Epoch 00444: val_mae improved from 2.83385 to 2.83068, saving model to best_boston.h5\n",
            "Epoch 445/500\n",
            "2/2 [==============================] - 0s 109ms/step - loss: 6.9857 - mae: 1.9022 - val_loss: 19.4491 - val_mae: 2.8236\n",
            "\n",
            "Epoch 00445: val_mae improved from 2.83068 to 2.82359, saving model to best_boston.h5\n",
            "Epoch 446/500\n",
            "2/2 [==============================] - 0s 114ms/step - loss: 6.9822 - mae: 1.9016 - val_loss: 19.4161 - val_mae: 2.8218\n",
            "\n",
            "Epoch 00446: val_mae improved from 2.82359 to 2.82178, saving model to best_boston.h5\n",
            "Epoch 447/500\n",
            "2/2 [==============================] - 0s 105ms/step - loss: 6.9744 - mae: 1.9004 - val_loss: 19.3821 - val_mae: 2.8222\n",
            "\n",
            "Epoch 00447: val_mae did not improve from 2.82178\n",
            "Epoch 448/500\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 6.9657 - mae: 1.8978 - val_loss: 19.3640 - val_mae: 2.8234\n",
            "\n",
            "Epoch 00448: val_mae did not improve from 2.82178\n",
            "Epoch 449/500\n",
            "2/2 [==============================] - 0s 112ms/step - loss: 6.9589 - mae: 1.8963 - val_loss: 19.3761 - val_mae: 2.8276\n",
            "\n",
            "Epoch 00449: val_mae did not improve from 2.82178\n",
            "Epoch 450/500\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 6.9542 - mae: 1.8950 - val_loss: 19.3744 - val_mae: 2.8314\n",
            "\n",
            "Epoch 00450: val_mae did not improve from 2.82178\n",
            "Epoch 451/500\n",
            "2/2 [==============================] - 0s 105ms/step - loss: 6.9476 - mae: 1.8944 - val_loss: 19.4190 - val_mae: 2.8372\n",
            "\n",
            "Epoch 00451: val_mae did not improve from 2.82178\n",
            "Epoch 452/500\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 6.9402 - mae: 1.8949 - val_loss: 19.4272 - val_mae: 2.8382\n",
            "\n",
            "Epoch 00452: val_mae did not improve from 2.82178\n",
            "Epoch 453/500\n",
            "2/2 [==============================] - 0s 128ms/step - loss: 6.9337 - mae: 1.8948 - val_loss: 19.4241 - val_mae: 2.8383\n",
            "\n",
            "Epoch 00453: val_mae did not improve from 2.82178\n",
            "Epoch 454/500\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 6.9362 - mae: 1.8963 - val_loss: 19.4472 - val_mae: 2.8407\n",
            "\n",
            "Epoch 00454: val_mae did not improve from 2.82178\n",
            "Epoch 455/500\n",
            "2/2 [==============================] - 0s 116ms/step - loss: 6.9215 - mae: 1.8943 - val_loss: 19.4088 - val_mae: 2.8357\n",
            "\n",
            "Epoch 00455: val_mae did not improve from 2.82178\n",
            "Epoch 456/500\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 6.9199 - mae: 1.8921 - val_loss: 19.3991 - val_mae: 2.8334\n",
            "\n",
            "Epoch 00456: val_mae did not improve from 2.82178\n",
            "Epoch 457/500\n",
            "2/2 [==============================] - 0s 106ms/step - loss: 6.9096 - mae: 1.8907 - val_loss: 19.4307 - val_mae: 2.8363\n",
            "\n",
            "Epoch 00457: val_mae did not improve from 2.82178\n",
            "Epoch 458/500\n",
            "2/2 [==============================] - 0s 124ms/step - loss: 6.9036 - mae: 1.8911 - val_loss: 19.4449 - val_mae: 2.8390\n",
            "\n",
            "Epoch 00458: val_mae did not improve from 2.82178\n",
            "Epoch 459/500\n",
            "2/2 [==============================] - 0s 104ms/step - loss: 6.8948 - mae: 1.8893 - val_loss: 19.4524 - val_mae: 2.8377\n",
            "\n",
            "Epoch 00459: val_mae did not improve from 2.82178\n",
            "Epoch 460/500\n",
            "2/2 [==============================] - 0s 117ms/step - loss: 6.8902 - mae: 1.8902 - val_loss: 19.4686 - val_mae: 2.8355\n",
            "\n",
            "Epoch 00460: val_mae did not improve from 2.82178\n",
            "Epoch 461/500\n",
            "2/2 [==============================] - 0s 128ms/step - loss: 6.8824 - mae: 1.8904 - val_loss: 19.4389 - val_mae: 2.8277\n",
            "\n",
            "Epoch 00461: val_mae did not improve from 2.82178\n",
            "Epoch 462/500\n",
            "2/2 [==============================] - 0s 117ms/step - loss: 6.8751 - mae: 1.8900 - val_loss: 19.3983 - val_mae: 2.8213\n",
            "\n",
            "Epoch 00462: val_mae improved from 2.82178 to 2.82133, saving model to best_boston.h5\n",
            "Epoch 463/500\n",
            "2/2 [==============================] - 0s 111ms/step - loss: 6.8809 - mae: 1.8904 - val_loss: 19.3751 - val_mae: 2.8181\n",
            "\n",
            "Epoch 00463: val_mae improved from 2.82133 to 2.81814, saving model to best_boston.h5\n",
            "Epoch 464/500\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 6.8631 - mae: 1.8896 - val_loss: 19.4215 - val_mae: 2.8241\n",
            "\n",
            "Epoch 00464: val_mae did not improve from 2.81814\n",
            "Epoch 465/500\n",
            "2/2 [==============================] - 0s 101ms/step - loss: 6.8652 - mae: 1.8926 - val_loss: 19.4964 - val_mae: 2.8349\n",
            "\n",
            "Epoch 00465: val_mae did not improve from 2.81814\n",
            "Epoch 466/500\n",
            "2/2 [==============================] - 0s 111ms/step - loss: 6.8562 - mae: 1.8932 - val_loss: 19.5157 - val_mae: 2.8409\n",
            "\n",
            "Epoch 00466: val_mae did not improve from 2.81814\n",
            "Epoch 467/500\n",
            "2/2 [==============================] - 0s 127ms/step - loss: 6.8541 - mae: 1.8926 - val_loss: 19.5175 - val_mae: 2.8435\n",
            "\n",
            "Epoch 00467: val_mae did not improve from 2.81814\n",
            "Epoch 468/500\n",
            "2/2 [==============================] - 0s 111ms/step - loss: 6.8433 - mae: 1.8909 - val_loss: 19.4727 - val_mae: 2.8404\n",
            "\n",
            "Epoch 00468: val_mae did not improve from 2.81814\n",
            "Epoch 469/500\n",
            "2/2 [==============================] - 0s 123ms/step - loss: 6.8357 - mae: 1.8890 - val_loss: 19.4151 - val_mae: 2.8343\n",
            "\n",
            "Epoch 00469: val_mae did not improve from 2.81814\n",
            "Epoch 470/500\n",
            "2/2 [==============================] - 0s 105ms/step - loss: 6.8293 - mae: 1.8860 - val_loss: 19.3536 - val_mae: 2.8276\n",
            "\n",
            "Epoch 00470: val_mae did not improve from 2.81814\n",
            "Epoch 471/500\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 6.8229 - mae: 1.8830 - val_loss: 19.2980 - val_mae: 2.8204\n",
            "\n",
            "Epoch 00471: val_mae did not improve from 2.81814\n",
            "Epoch 472/500\n",
            "2/2 [==============================] - 0s 111ms/step - loss: 6.8149 - mae: 1.8802 - val_loss: 19.2577 - val_mae: 2.8158\n",
            "\n",
            "Epoch 00472: val_mae improved from 2.81814 to 2.81584, saving model to best_boston.h5\n",
            "Epoch 473/500\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 6.8137 - mae: 1.8791 - val_loss: 19.2375 - val_mae: 2.8144\n",
            "\n",
            "Epoch 00473: val_mae improved from 2.81584 to 2.81436, saving model to best_boston.h5\n",
            "Epoch 474/500\n",
            "2/2 [==============================] - 0s 104ms/step - loss: 6.8127 - mae: 1.8770 - val_loss: 19.1890 - val_mae: 2.8111\n",
            "\n",
            "Epoch 00474: val_mae improved from 2.81436 to 2.81106, saving model to best_boston.h5\n",
            "Epoch 475/500\n",
            "2/2 [==============================] - 0s 124ms/step - loss: 6.8029 - mae: 1.8746 - val_loss: 19.1801 - val_mae: 2.8102\n",
            "\n",
            "Epoch 00475: val_mae improved from 2.81106 to 2.81024, saving model to best_boston.h5\n",
            "Epoch 476/500\n",
            "2/2 [==============================] - 0s 112ms/step - loss: 6.7969 - mae: 1.8743 - val_loss: 19.1802 - val_mae: 2.8083\n",
            "\n",
            "Epoch 00476: val_mae improved from 2.81024 to 2.80829, saving model to best_boston.h5\n",
            "Epoch 477/500\n",
            "2/2 [==============================] - 0s 107ms/step - loss: 6.7892 - mae: 1.8752 - val_loss: 19.1794 - val_mae: 2.8059\n",
            "\n",
            "Epoch 00477: val_mae improved from 2.80829 to 2.80585, saving model to best_boston.h5\n",
            "Epoch 478/500\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 6.7833 - mae: 1.8754 - val_loss: 19.1787 - val_mae: 2.8043\n",
            "\n",
            "Epoch 00478: val_mae improved from 2.80585 to 2.80428, saving model to best_boston.h5\n",
            "Epoch 479/500\n",
            "2/2 [==============================] - 0s 106ms/step - loss: 6.7771 - mae: 1.8751 - val_loss: 19.1753 - val_mae: 2.8045\n",
            "\n",
            "Epoch 00479: val_mae did not improve from 2.80428\n",
            "Epoch 480/500\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 6.7781 - mae: 1.8745 - val_loss: 19.2122 - val_mae: 2.8109\n",
            "\n",
            "Epoch 00480: val_mae did not improve from 2.80428\n",
            "Epoch 481/500\n",
            "2/2 [==============================] - 0s 125ms/step - loss: 6.7676 - mae: 1.8741 - val_loss: 19.1818 - val_mae: 2.8093\n",
            "\n",
            "Epoch 00481: val_mae did not improve from 2.80428\n",
            "Epoch 482/500\n",
            "2/2 [==============================] - 0s 112ms/step - loss: 6.7571 - mae: 1.8734 - val_loss: 19.1914 - val_mae: 2.8118\n",
            "\n",
            "Epoch 00482: val_mae did not improve from 2.80428\n",
            "Epoch 483/500\n",
            "2/2 [==============================] - 0s 117ms/step - loss: 6.7527 - mae: 1.8732 - val_loss: 19.1938 - val_mae: 2.8135\n",
            "\n",
            "Epoch 00483: val_mae did not improve from 2.80428\n",
            "Epoch 484/500\n",
            "2/2 [==============================] - 0s 109ms/step - loss: 6.7515 - mae: 1.8727 - val_loss: 19.2016 - val_mae: 2.8163\n",
            "\n",
            "Epoch 00484: val_mae did not improve from 2.80428\n",
            "Epoch 485/500\n",
            "2/2 [==============================] - 0s 126ms/step - loss: 6.7436 - mae: 1.8712 - val_loss: 19.2460 - val_mae: 2.8241\n",
            "\n",
            "Epoch 00485: val_mae did not improve from 2.80428\n",
            "Epoch 486/500\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 6.7440 - mae: 1.8725 - val_loss: 19.3016 - val_mae: 2.8308\n",
            "\n",
            "Epoch 00486: val_mae did not improve from 2.80428\n",
            "Epoch 487/500\n",
            "2/2 [==============================] - 0s 108ms/step - loss: 6.7361 - mae: 1.8731 - val_loss: 19.2944 - val_mae: 2.8270\n",
            "\n",
            "Epoch 00487: val_mae did not improve from 2.80428\n",
            "Epoch 488/500\n",
            "2/2 [==============================] - 0s 115ms/step - loss: 6.7312 - mae: 1.8725 - val_loss: 19.2595 - val_mae: 2.8174\n",
            "\n",
            "Epoch 00488: val_mae did not improve from 2.80428\n",
            "Epoch 489/500\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 6.7198 - mae: 1.8707 - val_loss: 19.2158 - val_mae: 2.8089\n",
            "\n",
            "Epoch 00489: val_mae did not improve from 2.80428\n",
            "Epoch 490/500\n",
            "2/2 [==============================] - 0s 116ms/step - loss: 6.7169 - mae: 1.8707 - val_loss: 19.2027 - val_mae: 2.8032\n",
            "\n",
            "Epoch 00490: val_mae improved from 2.80428 to 2.80317, saving model to best_boston.h5\n",
            "Epoch 491/500\n",
            "2/2 [==============================] - 0s 99ms/step - loss: 6.7123 - mae: 1.8699 - val_loss: 19.1600 - val_mae: 2.7961\n",
            "\n",
            "Epoch 00491: val_mae improved from 2.80317 to 2.79609, saving model to best_boston.h5\n",
            "Epoch 492/500\n",
            "2/2 [==============================] - 0s 136ms/step - loss: 6.7090 - mae: 1.8693 - val_loss: 19.1404 - val_mae: 2.7941\n",
            "\n",
            "Epoch 00492: val_mae improved from 2.79609 to 2.79411, saving model to best_boston.h5\n",
            "Epoch 493/500\n",
            "2/2 [==============================] - 0s 112ms/step - loss: 6.7016 - mae: 1.8683 - val_loss: 19.1724 - val_mae: 2.8018\n",
            "\n",
            "Epoch 00493: val_mae did not improve from 2.79411\n",
            "Epoch 494/500\n",
            "2/2 [==============================] - 0s 101ms/step - loss: 6.6952 - mae: 1.8680 - val_loss: 19.2293 - val_mae: 2.8113\n",
            "\n",
            "Epoch 00494: val_mae did not improve from 2.79411\n",
            "Epoch 495/500\n",
            "2/2 [==============================] - 0s 115ms/step - loss: 6.6943 - mae: 1.8686 - val_loss: 19.2364 - val_mae: 2.8135\n",
            "\n",
            "Epoch 00495: val_mae did not improve from 2.79411\n",
            "Epoch 496/500\n",
            "2/2 [==============================] - 0s 117ms/step - loss: 6.6853 - mae: 1.8672 - val_loss: 19.1913 - val_mae: 2.8094\n",
            "\n",
            "Epoch 00496: val_mae did not improve from 2.79411\n",
            "Epoch 497/500\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 6.6777 - mae: 1.8657 - val_loss: 19.1692 - val_mae: 2.8055\n",
            "\n",
            "Epoch 00497: val_mae did not improve from 2.79411\n",
            "Epoch 498/500\n",
            "2/2 [==============================] - 0s 111ms/step - loss: 6.6678 - mae: 1.8637 - val_loss: 19.1538 - val_mae: 2.8048\n",
            "\n",
            "Epoch 00498: val_mae did not improve from 2.79411\n",
            "Epoch 499/500\n",
            "2/2 [==============================] - 0s 112ms/step - loss: 6.6693 - mae: 1.8620 - val_loss: 19.1294 - val_mae: 2.8030\n",
            "\n",
            "Epoch 00499: val_mae did not improve from 2.79411\n",
            "Epoch 500/500\n",
            "2/2 [==============================] - 0s 122ms/step - loss: 6.6587 - mae: 1.8594 - val_loss: 19.1488 - val_mae: 2.8044\n",
            "\n",
            "Epoch 00500: val_mae did not improve from 2.79411\n",
            "CPU times: user 1min, sys: 1.56 s, total: 1min 1s\n",
            "Wall time: 1min 3s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2lTe82V9DjY",
        "outputId": "6d0cb9f1-6c79-4b49-d951-2eda7d727706"
      },
      "source": [
        "!ls -l"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 76\n",
            "-rw-r--r-- 1 root root 70736 Mar 28 13:13 best_boston.h5\n",
            "drwxr-xr-x 1 root root  4096 Mar 18 13:36 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIMATbF49049",
        "outputId": "747884b0-37bb-4bff-fcbe-58aa7f8f6f21"
      },
      "source": [
        "mse, mae = model.evaluate(X_test, y_test)\n",
        "mse, mae"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 2ms/step - loss: 19.1488 - mae: 2.8044\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19.148832321166992, 2.8043906688690186)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_N7sStGHSHL",
        "outputId": "8dd386ed-2867-400e-d25f-3c4c2a92cde4"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBngcZCZ-Bea",
        "outputId": "dee0803e-d978-4cb2-84da-d19312df8fbf"
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "model_google = load_model('/content/drive/My Drive/Colab Notebooks/best_boston.h5')\n",
        "\n",
        "loss, accuracy = model_google.evaluate(X_test, y_test)\n",
        "loss, accuracy"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 3ms/step - loss: 19.1404 - mae: 2.7941\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19.140417098999023, 2.794105291366577)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2ieHiP3Jhjl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}