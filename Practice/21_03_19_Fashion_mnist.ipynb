{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fashion_mnist_송승은.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "PORZqEBOOMeE",
        "outputId": "6eaff80e-1036-4924-84d8-6a8db3540264"
      },
      "source": [
        "import keras\n",
        "keras.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.3'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NmoFKE7OXS8",
        "outputId": "2923ce27-f94f-4990-aa92-c1ea82995b7f"
      },
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "(X_train, y_train),(X_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgL1H0BCOgik",
        "outputId": "ea025e9a-90b3-4da9-bc8d-456eb12322fe"
      },
      "source": [
        "print(len(X_train))\n",
        "print(X_train.shape)\n",
        "\n",
        "print(len(y_train))\n",
        "print(y_train[:5])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000\n",
            "(60000, 28, 28)\n",
            "60000\n",
            "[9 0 0 3 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a53SLOapOqaj",
        "outputId": "73f30579-21f3-4f9b-e755-fc1dcec9c5be"
      },
      "source": [
        "print(len(X_test))\n",
        "print(X_test.shape)\n",
        "\n",
        "print(len(y_test))\n",
        "print(y_test[:5])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000\n",
            "(10000, 28, 28)\n",
            "10000\n",
            "[9 2 1 1 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "XcTe-EjHO10a",
        "outputId": "dff0ea06-9d37-4f2e-ef3a-2438242f1e42"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "digit = X_train[0]\n",
        "plt.imshow(digit, cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0HXQK8uO9qa",
        "outputId": "39a718ba-30cd-4d24-eaa5-3acdb08ca026"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "np.set_printoptions(linewidth=150)\n",
        "\n",
        "print(X_train[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  13  73   0   0   1   4   0   0   0   0   1   1   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   3   0  36 136 127  62  54   0   0   0   1   3   4   0   0   3]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   6   0 102 204 176 134 144 123  23   0   0   0   0  12  10   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 155 236 207 178 107 156 161 109  64  23  77 130  72  15]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   1   0  69 207 223 218 216 216 163 127 121 122 146 141  88 172  66]\n",
            " [  0   0   0   0   0   0   0   0   0   1   1   1   0 200 232 232 233 229 223 223 215 213 164 127 123 196 229   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 183 225 216 223 228 235 227 224 222 224 221 223 245 173   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 193 228 218 213 198 180 212 210 211 213 223 220 243 202   0]\n",
            " [  0   0   0   0   0   0   0   0   0   1   3   0  12 219 220 212 218 192 169 227 208 218 224 212 226 197 209  52]\n",
            " [  0   0   0   0   0   0   0   0   0   0   6   0  99 244 222 220 218 203 198 221 215 213 222 220 245 119 167  56]\n",
            " [  0   0   0   0   0   0   0   0   0   4   0   0  55 236 228 230 228 240 232 213 218 223 234 217 217 209  92   0]\n",
            " [  0   0   1   4   6   7   2   0   0   0   0   0 237 226 217 223 222 219 222 221 216 223 229 215 218 255  77   0]\n",
            " [  0   3   0   0   0   0   0   0   0  62 145 204 228 207 213 221 218 208 211 218 224 223 219 215 224 244 159   0]\n",
            " [  0   0   0   0  18  44  82 107 189 228 220 222 217 226 200 205 211 230 224 234 176 188 250 248 233 238 215   0]\n",
            " [  0  57 187 208 224 221 224 208 204 214 208 209 200 159 245 193 206 223 255 255 221 234 221 211 220 232 246   0]\n",
            " [  3 202 228 224 221 211 211 214 205 205 205 220 240  80 150 255 229 221 188 154 191 210 204 209 222 228 225   0]\n",
            " [ 98 233 198 210 222 229 229 234 249 220 194 215 217 241  65  73 106 117 168 219 221 215 217 223 223 224 229  29]\n",
            " [ 75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195 227 245 239 223 218 212 209 222 220 221 230  67]\n",
            " [ 48 203 183 194 213 197 185 190 194 192 202 214 219 221 220 236 225 216 199 206 186 181 177 172 181 205 206 115]\n",
            " [  0 122 219 193 179 171 183 196 204 210 213 207 211 210 200 196 194 191 195 191 198 192 176 156 167 177 210  92]\n",
            " [  0   0  74 189 212 191 175 172 175 181 185 188 189 188 193 198 204 209 210 210 211 188 188 194 192 216 170   0]\n",
            " [  2   0   0   0  66 200 222 237 239 242 246 243 244 221 220 193 191 179 182 182 181 176 166 168  99  58   0   0]\n",
            " [  0   0   0   0   0   0   0  40  61  44  72  41  35   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFiOiTVlPFfC",
        "outputId": "c8d03a23-d8e9-45e0-86cc-6210f5a9542f"
      },
      "source": [
        "X_train = X_train.reshape((60000, 28*28))  \n",
        "X_test = X_test.reshape((10000, 28*28))\n",
        "\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 784), (10000, 784))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2E4Gn4EPgwx",
        "outputId": "39f128df-073e-4cc8-f9e1-fa039737d0d2"
      },
      "source": [
        "X_train = X_train.astype(float)/255\n",
        "X_test = X_test.astype(float)/255\n",
        "\n",
        "print(X_train[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.00392157 0.         0.         0.05098039 0.28627451 0.         0.         0.00392157\n",
            " 0.01568627 0.         0.         0.         0.         0.00392157 0.00392157 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.         0.         0.01176471 0.         0.14117647 0.53333333 0.49803922 0.24313725\n",
            " 0.21176471 0.         0.         0.         0.00392157 0.01176471 0.01568627 0.         0.         0.01176471 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.02352941 0.         0.4        0.8\n",
            " 0.69019608 0.5254902  0.56470588 0.48235294 0.09019608 0.         0.         0.         0.         0.04705882 0.03921569 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            " 0.60784314 0.9254902  0.81176471 0.69803922 0.41960784 0.61176471 0.63137255 0.42745098 0.25098039 0.09019608 0.30196078 0.50980392 0.28235294\n",
            " 0.05882353 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.00392157\n",
            " 0.         0.27058824 0.81176471 0.8745098  0.85490196 0.84705882 0.84705882 0.63921569 0.49803922 0.4745098  0.47843137 0.57254902 0.55294118\n",
            " 0.34509804 0.6745098  0.25882353 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.00392157\n",
            " 0.00392157 0.00392157 0.         0.78431373 0.90980392 0.90980392 0.91372549 0.89803922 0.8745098  0.8745098  0.84313725 0.83529412 0.64313725\n",
            " 0.49803922 0.48235294 0.76862745 0.89803922 0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.71764706 0.88235294 0.84705882 0.8745098  0.89411765 0.92156863 0.89019608 0.87843137\n",
            " 0.87058824 0.87843137 0.86666667 0.8745098  0.96078431 0.67843137 0.         0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.         0.         0.75686275 0.89411765 0.85490196 0.83529412 0.77647059 0.70588235\n",
            " 0.83137255 0.82352941 0.82745098 0.83529412 0.8745098  0.8627451  0.95294118 0.79215686 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.00392157 0.01176471 0.         0.04705882 0.85882353 0.8627451  0.83137255 0.85490196\n",
            " 0.75294118 0.6627451  0.89019608 0.81568627 0.85490196 0.87843137 0.83137255 0.88627451 0.77254902 0.81960784 0.20392157 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.         0.         0.         0.02352941 0.         0.38823529 0.95686275 0.87058824\n",
            " 0.8627451  0.85490196 0.79607843 0.77647059 0.86666667 0.84313725 0.83529412 0.87058824 0.8627451  0.96078431 0.46666667 0.65490196 0.21960784\n",
            " 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.01568627 0.         0.         0.21568627\n",
            " 0.9254902  0.89411765 0.90196078 0.89411765 0.94117647 0.90980392 0.83529412 0.85490196 0.8745098  0.91764706 0.85098039 0.85098039 0.81960784\n",
            " 0.36078431 0.         0.         0.         0.00392157 0.01568627 0.02352941 0.02745098 0.00784314 0.         0.         0.         0.\n",
            " 0.         0.92941176 0.88627451 0.85098039 0.8745098  0.87058824 0.85882353 0.87058824 0.86666667 0.84705882 0.8745098  0.89803922 0.84313725\n",
            " 0.85490196 1.         0.30196078 0.         0.         0.01176471 0.         0.         0.         0.         0.         0.         0.\n",
            " 0.24313725 0.56862745 0.8        0.89411765 0.81176471 0.83529412 0.86666667 0.85490196 0.81568627 0.82745098 0.85490196 0.87843137 0.8745098\n",
            " 0.85882353 0.84313725 0.87843137 0.95686275 0.62352941 0.         0.         0.         0.         0.         0.07058824 0.17254902 0.32156863\n",
            " 0.41960784 0.74117647 0.89411765 0.8627451  0.87058824 0.85098039 0.88627451 0.78431373 0.80392157 0.82745098 0.90196078 0.87843137 0.91764706\n",
            " 0.69019608 0.7372549  0.98039216 0.97254902 0.91372549 0.93333333 0.84313725 0.         0.         0.22352941 0.73333333 0.81568627 0.87843137\n",
            " 0.86666667 0.87843137 0.81568627 0.8        0.83921569 0.81568627 0.81960784 0.78431373 0.62352941 0.96078431 0.75686275 0.80784314 0.8745098\n",
            " 1.         1.         0.86666667 0.91764706 0.86666667 0.82745098 0.8627451  0.90980392 0.96470588 0.         0.01176471 0.79215686 0.89411765\n",
            " 0.87843137 0.86666667 0.82745098 0.82745098 0.83921569 0.80392157 0.80392157 0.80392157 0.8627451  0.94117647 0.31372549 0.58823529 1.\n",
            " 0.89803922 0.86666667 0.7372549  0.60392157 0.74901961 0.82352941 0.8        0.81960784 0.87058824 0.89411765 0.88235294 0.         0.38431373\n",
            " 0.91372549 0.77647059 0.82352941 0.87058824 0.89803922 0.89803922 0.91764706 0.97647059 0.8627451  0.76078431 0.84313725 0.85098039 0.94509804\n",
            " 0.25490196 0.28627451 0.41568627 0.45882353 0.65882353 0.85882353 0.86666667 0.84313725 0.85098039 0.8745098  0.8745098  0.87843137 0.89803922\n",
            " 0.11372549 0.29411765 0.8        0.83137255 0.8        0.75686275 0.80392157 0.82745098 0.88235294 0.84705882 0.7254902  0.77254902 0.80784314\n",
            " 0.77647059 0.83529412 0.94117647 0.76470588 0.89019608 0.96078431 0.9372549  0.8745098  0.85490196 0.83137255 0.81960784 0.87058824 0.8627451\n",
            " 0.86666667 0.90196078 0.2627451  0.18823529 0.79607843 0.71764706 0.76078431 0.83529412 0.77254902 0.7254902  0.74509804 0.76078431 0.75294118\n",
            " 0.79215686 0.83921569 0.85882353 0.86666667 0.8627451  0.9254902  0.88235294 0.84705882 0.78039216 0.80784314 0.72941176 0.70980392 0.69411765\n",
            " 0.6745098  0.70980392 0.80392157 0.80784314 0.45098039 0.         0.47843137 0.85882353 0.75686275 0.70196078 0.67058824 0.71764706 0.76862745\n",
            " 0.8        0.82352941 0.83529412 0.81176471 0.82745098 0.82352941 0.78431373 0.76862745 0.76078431 0.74901961 0.76470588 0.74901961 0.77647059\n",
            " 0.75294118 0.69019608 0.61176471 0.65490196 0.69411765 0.82352941 0.36078431 0.         0.         0.29019608 0.74117647 0.83137255 0.74901961\n",
            " 0.68627451 0.6745098  0.68627451 0.70980392 0.7254902  0.7372549  0.74117647 0.7372549  0.75686275 0.77647059 0.8        0.81960784 0.82352941\n",
            " 0.82352941 0.82745098 0.7372549  0.7372549  0.76078431 0.75294118 0.84705882 0.66666667 0.         0.00784314 0.         0.         0.\n",
            " 0.25882353 0.78431373 0.87058824 0.92941176 0.9372549  0.94901961 0.96470588 0.95294118 0.95686275 0.86666667 0.8627451  0.75686275 0.74901961\n",
            " 0.70196078 0.71372549 0.71372549 0.70980392 0.69019608 0.65098039 0.65882353 0.38823529 0.22745098 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.15686275 0.23921569 0.17254902 0.28235294 0.16078431 0.1372549  0.         0.\n",
            " 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPijZeqmPjZc",
        "outputId": "404d0703-2ca8-4bc3-b4e5-3a30e10dd792"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "print(y_train[:5])\n",
        "print(y_train.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "(60000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De2hwbntuNPb"
      },
      "source": [
        "# L2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZnoEE2ULFtq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1b609b6-e806-4cf4-f6c5-81990c23a27b"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import regularizers\n",
        "\n",
        "L2 = models.Sequential()\n",
        "L2.add(layers.Dense(512, activation='relu', input_shape=(28*28,), \n",
        "                       kernel_regularizer = regularizers.l2(0.00001)))\n",
        "L2.add(layers.Dense(256, activation='relu',\n",
        "                       kernel_regularizer = regularizers.l2(0.00001)))\n",
        "L2.add(layers.Dense(10, activation='softmax'))\n",
        "L2.summary()\n",
        "\n",
        "Hist_L2 = L2.compile(loss = 'categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 535,818\n",
            "Trainable params: 535,818\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnaRa4A7SwdO",
        "outputId": "d22e45d6-50bd-4e7f-b38e-06257998c9bc"
      },
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "ex = EarlyStopping(monitor='val_accuracy', mode='max', patience=150, verbose = 1)\n",
        "mc = ModelCheckpoint('best-L2.h5', monitor='val_accuracy', mode='max', save_best_only=True, verbose=1)\n",
        "\n",
        "%time\n",
        "Hist_L2 = L2.fit(X_train, y_train, epochs=500, batch_size=128, validation_split=0.2, callbacks=[ex, mc], verbose=1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
            "Wall time: 5.96 µs\n",
            "Epoch 1/500\n",
            "375/375 [==============================] - 5s 6ms/step - loss: 0.7927 - accuracy: 0.7196 - val_loss: 0.4124 - val_accuracy: 0.8521\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.85208, saving model to best-DR_BN.h5\n",
            "Epoch 2/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.4153 - accuracy: 0.8486 - val_loss: 0.3720 - val_accuracy: 0.8718\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.85208 to 0.87183, saving model to best-DR_BN.h5\n",
            "Epoch 3/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.3626 - accuracy: 0.8701 - val_loss: 0.3526 - val_accuracy: 0.8736\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.87183 to 0.87358, saving model to best-DR_BN.h5\n",
            "Epoch 4/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.3311 - accuracy: 0.8816 - val_loss: 0.4462 - val_accuracy: 0.8447\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.87358\n",
            "Epoch 5/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.3096 - accuracy: 0.8909 - val_loss: 0.3387 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.87358 to 0.88583, saving model to best-DR_BN.h5\n",
            "Epoch 6/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.2938 - accuracy: 0.8935 - val_loss: 0.3776 - val_accuracy: 0.8775\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.88583\n",
            "Epoch 7/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.2839 - accuracy: 0.9004 - val_loss: 0.3565 - val_accuracy: 0.8840\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.88583\n",
            "Epoch 8/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.2678 - accuracy: 0.9054 - val_loss: 0.3474 - val_accuracy: 0.8920\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.88583 to 0.89200, saving model to best-DR_BN.h5\n",
            "Epoch 9/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.2610 - accuracy: 0.9075 - val_loss: 0.3689 - val_accuracy: 0.8763\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.89200\n",
            "Epoch 10/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.2521 - accuracy: 0.9118 - val_loss: 0.3716 - val_accuracy: 0.8791\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.89200\n",
            "Epoch 11/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.2457 - accuracy: 0.9124 - val_loss: 0.3417 - val_accuracy: 0.8928\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.89200 to 0.89275, saving model to best-DR_BN.h5\n",
            "Epoch 12/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.2438 - accuracy: 0.9153 - val_loss: 0.3735 - val_accuracy: 0.8792\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.89275\n",
            "Epoch 13/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.2420 - accuracy: 0.9152 - val_loss: 0.4212 - val_accuracy: 0.8752\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.89275\n",
            "Epoch 14/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.2257 - accuracy: 0.9203 - val_loss: 0.3540 - val_accuracy: 0.8906\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.89275\n",
            "Epoch 15/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.2233 - accuracy: 0.9220 - val_loss: 0.3928 - val_accuracy: 0.8843\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.89275\n",
            "Epoch 16/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.2225 - accuracy: 0.9227 - val_loss: 0.3700 - val_accuracy: 0.8942\n",
            "\n",
            "Epoch 00016: val_accuracy improved from 0.89275 to 0.89425, saving model to best-DR_BN.h5\n",
            "Epoch 17/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.2200 - accuracy: 0.9245 - val_loss: 0.3934 - val_accuracy: 0.8877\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.89425\n",
            "Epoch 18/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.2112 - accuracy: 0.9283 - val_loss: 0.4084 - val_accuracy: 0.8884\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.89425\n",
            "Epoch 19/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.2128 - accuracy: 0.9269 - val_loss: 0.3853 - val_accuracy: 0.8931\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.89425\n",
            "Epoch 20/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.2007 - accuracy: 0.9330 - val_loss: 0.4485 - val_accuracy: 0.8855\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.89425\n",
            "Epoch 21/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.2010 - accuracy: 0.9323 - val_loss: 0.4108 - val_accuracy: 0.8895\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.89425\n",
            "Epoch 22/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1928 - accuracy: 0.9337 - val_loss: 0.4423 - val_accuracy: 0.8828\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.89425\n",
            "Epoch 23/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1967 - accuracy: 0.9346 - val_loss: 0.4162 - val_accuracy: 0.8941\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.89425\n",
            "Epoch 24/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1922 - accuracy: 0.9374 - val_loss: 0.4307 - val_accuracy: 0.8905\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.89425\n",
            "Epoch 25/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1876 - accuracy: 0.9387 - val_loss: 0.4748 - val_accuracy: 0.8824\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.89425\n",
            "Epoch 26/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1874 - accuracy: 0.9384 - val_loss: 0.4224 - val_accuracy: 0.8923\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.89425\n",
            "Epoch 27/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1846 - accuracy: 0.9403 - val_loss: 0.4432 - val_accuracy: 0.8938\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.89425\n",
            "Epoch 28/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1884 - accuracy: 0.9381 - val_loss: 0.4953 - val_accuracy: 0.8886\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.89425\n",
            "Epoch 29/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1852 - accuracy: 0.9406 - val_loss: 0.4788 - val_accuracy: 0.8890\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.89425\n",
            "Epoch 30/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1798 - accuracy: 0.9413 - val_loss: 0.4133 - val_accuracy: 0.8905\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.89425\n",
            "Epoch 31/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1784 - accuracy: 0.9432 - val_loss: 0.4699 - val_accuracy: 0.8862\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.89425\n",
            "Epoch 32/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1799 - accuracy: 0.9428 - val_loss: 0.4527 - val_accuracy: 0.8947\n",
            "\n",
            "Epoch 00032: val_accuracy improved from 0.89425 to 0.89475, saving model to best-DR_BN.h5\n",
            "Epoch 33/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1761 - accuracy: 0.9423 - val_loss: 0.5003 - val_accuracy: 0.8913\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.89475\n",
            "Epoch 34/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1738 - accuracy: 0.9440 - val_loss: 0.5335 - val_accuracy: 0.8687\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.89475\n",
            "Epoch 35/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1709 - accuracy: 0.9471 - val_loss: 0.4892 - val_accuracy: 0.8907\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.89475\n",
            "Epoch 36/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1674 - accuracy: 0.9480 - val_loss: 0.4915 - val_accuracy: 0.8914\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.89475\n",
            "Epoch 37/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1649 - accuracy: 0.9493 - val_loss: 0.5194 - val_accuracy: 0.8910\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.89475\n",
            "Epoch 38/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1685 - accuracy: 0.9468 - val_loss: 0.5582 - val_accuracy: 0.8852\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.89475\n",
            "Epoch 39/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1662 - accuracy: 0.9495 - val_loss: 0.4936 - val_accuracy: 0.8839\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.89475\n",
            "Epoch 40/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1625 - accuracy: 0.9484 - val_loss: 0.5370 - val_accuracy: 0.8834\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.89475\n",
            "Epoch 41/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1631 - accuracy: 0.9487 - val_loss: 0.5766 - val_accuracy: 0.8841\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.89475\n",
            "Epoch 42/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1620 - accuracy: 0.9509 - val_loss: 0.4778 - val_accuracy: 0.8924\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.89475\n",
            "Epoch 43/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1546 - accuracy: 0.9529 - val_loss: 0.5616 - val_accuracy: 0.8926\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.89475\n",
            "Epoch 44/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1566 - accuracy: 0.9528 - val_loss: 0.5746 - val_accuracy: 0.8813\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.89475\n",
            "Epoch 45/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1531 - accuracy: 0.9528 - val_loss: 0.5413 - val_accuracy: 0.8918\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.89475\n",
            "Epoch 46/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1524 - accuracy: 0.9534 - val_loss: 0.5518 - val_accuracy: 0.8864\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.89475\n",
            "Epoch 47/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1556 - accuracy: 0.9518 - val_loss: 0.5604 - val_accuracy: 0.8922\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.89475\n",
            "Epoch 48/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1545 - accuracy: 0.9536 - val_loss: 0.5675 - val_accuracy: 0.8910\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.89475\n",
            "Epoch 49/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1510 - accuracy: 0.9552 - val_loss: 0.5514 - val_accuracy: 0.8912\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.89475\n",
            "Epoch 50/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1505 - accuracy: 0.9553 - val_loss: 0.6762 - val_accuracy: 0.8729\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.89475\n",
            "Epoch 51/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1475 - accuracy: 0.9566 - val_loss: 0.5855 - val_accuracy: 0.8852\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.89475\n",
            "Epoch 52/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1539 - accuracy: 0.9537 - val_loss: 0.5327 - val_accuracy: 0.8921\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.89475\n",
            "Epoch 53/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1512 - accuracy: 0.9549 - val_loss: 0.6142 - val_accuracy: 0.8928\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.89475\n",
            "Epoch 54/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1491 - accuracy: 0.9548 - val_loss: 0.5869 - val_accuracy: 0.8882\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.89475\n",
            "Epoch 55/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1465 - accuracy: 0.9567 - val_loss: 0.5478 - val_accuracy: 0.8911\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.89475\n",
            "Epoch 56/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1455 - accuracy: 0.9575 - val_loss: 0.5625 - val_accuracy: 0.8917\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.89475\n",
            "Epoch 57/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1435 - accuracy: 0.9580 - val_loss: 0.7282 - val_accuracy: 0.8723\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.89475\n",
            "Epoch 58/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1494 - accuracy: 0.9582 - val_loss: 0.6328 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.89475\n",
            "Epoch 59/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1434 - accuracy: 0.9589 - val_loss: 0.6065 - val_accuracy: 0.8835\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.89475\n",
            "Epoch 60/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1417 - accuracy: 0.9595 - val_loss: 0.5913 - val_accuracy: 0.8898\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.89475\n",
            "Epoch 61/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1451 - accuracy: 0.9578 - val_loss: 0.6127 - val_accuracy: 0.8887\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.89475\n",
            "Epoch 62/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1454 - accuracy: 0.9592 - val_loss: 0.7751 - val_accuracy: 0.8699\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.89475\n",
            "Epoch 63/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1401 - accuracy: 0.9603 - val_loss: 0.6147 - val_accuracy: 0.8936\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.89475\n",
            "Epoch 64/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1431 - accuracy: 0.9595 - val_loss: 0.6877 - val_accuracy: 0.8865\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.89475\n",
            "Epoch 65/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1390 - accuracy: 0.9602 - val_loss: 0.6184 - val_accuracy: 0.8903\n",
            "\n",
            "Epoch 00065: val_accuracy did not improve from 0.89475\n",
            "Epoch 66/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1371 - accuracy: 0.9621 - val_loss: 0.6653 - val_accuracy: 0.8904\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.89475\n",
            "Epoch 67/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1372 - accuracy: 0.9621 - val_loss: 0.6247 - val_accuracy: 0.8942\n",
            "\n",
            "Epoch 00067: val_accuracy did not improve from 0.89475\n",
            "Epoch 68/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1372 - accuracy: 0.9624 - val_loss: 0.7308 - val_accuracy: 0.8823\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.89475\n",
            "Epoch 69/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1369 - accuracy: 0.9613 - val_loss: 0.6465 - val_accuracy: 0.8895\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.89475\n",
            "Epoch 70/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1384 - accuracy: 0.9608 - val_loss: 0.5938 - val_accuracy: 0.8917\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.89475\n",
            "Epoch 71/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1307 - accuracy: 0.9640 - val_loss: 0.6726 - val_accuracy: 0.8909\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.89475\n",
            "Epoch 72/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1350 - accuracy: 0.9646 - val_loss: 0.6444 - val_accuracy: 0.8903\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.89475\n",
            "Epoch 73/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1304 - accuracy: 0.9639 - val_loss: 0.6530 - val_accuracy: 0.8883\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.89475\n",
            "Epoch 74/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1320 - accuracy: 0.9644 - val_loss: 0.7014 - val_accuracy: 0.8809\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.89475\n",
            "Epoch 75/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1307 - accuracy: 0.9658 - val_loss: 0.7283 - val_accuracy: 0.8889\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.89475\n",
            "Epoch 76/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1393 - accuracy: 0.9632 - val_loss: 0.6842 - val_accuracy: 0.8930\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.89475\n",
            "Epoch 77/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1310 - accuracy: 0.9668 - val_loss: 0.6759 - val_accuracy: 0.8925\n",
            "\n",
            "Epoch 00077: val_accuracy did not improve from 0.89475\n",
            "Epoch 78/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1300 - accuracy: 0.9647 - val_loss: 0.6917 - val_accuracy: 0.8899\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.89475\n",
            "Epoch 79/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1269 - accuracy: 0.9654 - val_loss: 0.7356 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00079: val_accuracy did not improve from 0.89475\n",
            "Epoch 80/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1292 - accuracy: 0.9660 - val_loss: 0.6800 - val_accuracy: 0.8861\n",
            "\n",
            "Epoch 00080: val_accuracy did not improve from 0.89475\n",
            "Epoch 81/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1273 - accuracy: 0.9670 - val_loss: 0.6889 - val_accuracy: 0.8882\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.89475\n",
            "Epoch 82/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1272 - accuracy: 0.9667 - val_loss: 0.6901 - val_accuracy: 0.8930\n",
            "\n",
            "Epoch 00082: val_accuracy did not improve from 0.89475\n",
            "Epoch 83/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1237 - accuracy: 0.9673 - val_loss: 0.7740 - val_accuracy: 0.8814\n",
            "\n",
            "Epoch 00083: val_accuracy did not improve from 0.89475\n",
            "Epoch 84/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1264 - accuracy: 0.9667 - val_loss: 0.7794 - val_accuracy: 0.8808\n",
            "\n",
            "Epoch 00084: val_accuracy did not improve from 0.89475\n",
            "Epoch 85/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1295 - accuracy: 0.9682 - val_loss: 0.7462 - val_accuracy: 0.8817\n",
            "\n",
            "Epoch 00085: val_accuracy did not improve from 0.89475\n",
            "Epoch 86/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1261 - accuracy: 0.9671 - val_loss: 0.6728 - val_accuracy: 0.8882\n",
            "\n",
            "Epoch 00086: val_accuracy did not improve from 0.89475\n",
            "Epoch 87/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1246 - accuracy: 0.9681 - val_loss: 0.7401 - val_accuracy: 0.8883\n",
            "\n",
            "Epoch 00087: val_accuracy did not improve from 0.89475\n",
            "Epoch 88/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1266 - accuracy: 0.9678 - val_loss: 0.7696 - val_accuracy: 0.8913\n",
            "\n",
            "Epoch 00088: val_accuracy did not improve from 0.89475\n",
            "Epoch 89/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1262 - accuracy: 0.9668 - val_loss: 0.7040 - val_accuracy: 0.8894\n",
            "\n",
            "Epoch 00089: val_accuracy did not improve from 0.89475\n",
            "Epoch 90/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1252 - accuracy: 0.9685 - val_loss: 0.8235 - val_accuracy: 0.8696\n",
            "\n",
            "Epoch 00090: val_accuracy did not improve from 0.89475\n",
            "Epoch 91/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1228 - accuracy: 0.9687 - val_loss: 0.7288 - val_accuracy: 0.8835\n",
            "\n",
            "Epoch 00091: val_accuracy did not improve from 0.89475\n",
            "Epoch 92/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1234 - accuracy: 0.9687 - val_loss: 0.7092 - val_accuracy: 0.8889\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.89475\n",
            "Epoch 93/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1241 - accuracy: 0.9675 - val_loss: 0.7035 - val_accuracy: 0.8892\n",
            "\n",
            "Epoch 00093: val_accuracy did not improve from 0.89475\n",
            "Epoch 94/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1233 - accuracy: 0.9692 - val_loss: 0.7257 - val_accuracy: 0.8900\n",
            "\n",
            "Epoch 00094: val_accuracy did not improve from 0.89475\n",
            "Epoch 95/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1211 - accuracy: 0.9690 - val_loss: 0.7921 - val_accuracy: 0.8773\n",
            "\n",
            "Epoch 00095: val_accuracy did not improve from 0.89475\n",
            "Epoch 96/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1217 - accuracy: 0.9699 - val_loss: 0.7539 - val_accuracy: 0.8865\n",
            "\n",
            "Epoch 00096: val_accuracy did not improve from 0.89475\n",
            "Epoch 97/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1231 - accuracy: 0.9689 - val_loss: 0.7614 - val_accuracy: 0.8773\n",
            "\n",
            "Epoch 00097: val_accuracy did not improve from 0.89475\n",
            "Epoch 98/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1214 - accuracy: 0.9698 - val_loss: 0.7817 - val_accuracy: 0.8907\n",
            "\n",
            "Epoch 00098: val_accuracy did not improve from 0.89475\n",
            "Epoch 99/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1210 - accuracy: 0.9701 - val_loss: 0.7799 - val_accuracy: 0.8834\n",
            "\n",
            "Epoch 00099: val_accuracy did not improve from 0.89475\n",
            "Epoch 100/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1186 - accuracy: 0.9692 - val_loss: 0.8086 - val_accuracy: 0.8915\n",
            "\n",
            "Epoch 00100: val_accuracy did not improve from 0.89475\n",
            "Epoch 101/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1160 - accuracy: 0.9723 - val_loss: 0.8705 - val_accuracy: 0.8864\n",
            "\n",
            "Epoch 00101: val_accuracy did not improve from 0.89475\n",
            "Epoch 102/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1169 - accuracy: 0.9726 - val_loss: 0.7581 - val_accuracy: 0.8878\n",
            "\n",
            "Epoch 00102: val_accuracy did not improve from 0.89475\n",
            "Epoch 103/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1157 - accuracy: 0.9713 - val_loss: 0.8067 - val_accuracy: 0.8829\n",
            "\n",
            "Epoch 00103: val_accuracy did not improve from 0.89475\n",
            "Epoch 104/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1171 - accuracy: 0.9725 - val_loss: 0.8146 - val_accuracy: 0.8879\n",
            "\n",
            "Epoch 00104: val_accuracy did not improve from 0.89475\n",
            "Epoch 105/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1178 - accuracy: 0.9706 - val_loss: 0.8211 - val_accuracy: 0.8818\n",
            "\n",
            "Epoch 00105: val_accuracy did not improve from 0.89475\n",
            "Epoch 106/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1185 - accuracy: 0.9706 - val_loss: 0.8021 - val_accuracy: 0.8919\n",
            "\n",
            "Epoch 00106: val_accuracy did not improve from 0.89475\n",
            "Epoch 107/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1155 - accuracy: 0.9717 - val_loss: 0.8278 - val_accuracy: 0.8740\n",
            "\n",
            "Epoch 00107: val_accuracy did not improve from 0.89475\n",
            "Epoch 108/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1193 - accuracy: 0.9708 - val_loss: 0.7756 - val_accuracy: 0.8878\n",
            "\n",
            "Epoch 00108: val_accuracy did not improve from 0.89475\n",
            "Epoch 109/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1137 - accuracy: 0.9737 - val_loss: 0.8252 - val_accuracy: 0.8843\n",
            "\n",
            "Epoch 00109: val_accuracy did not improve from 0.89475\n",
            "Epoch 110/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1191 - accuracy: 0.9720 - val_loss: 0.8399 - val_accuracy: 0.8908\n",
            "\n",
            "Epoch 00110: val_accuracy did not improve from 0.89475\n",
            "Epoch 111/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1130 - accuracy: 0.9738 - val_loss: 0.7576 - val_accuracy: 0.8923\n",
            "\n",
            "Epoch 00111: val_accuracy did not improve from 0.89475\n",
            "Epoch 112/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1177 - accuracy: 0.9718 - val_loss: 0.9638 - val_accuracy: 0.8817\n",
            "\n",
            "Epoch 00112: val_accuracy did not improve from 0.89475\n",
            "Epoch 113/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1164 - accuracy: 0.9725 - val_loss: 0.8103 - val_accuracy: 0.8789\n",
            "\n",
            "Epoch 00113: val_accuracy did not improve from 0.89475\n",
            "Epoch 114/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1143 - accuracy: 0.9737 - val_loss: 0.7806 - val_accuracy: 0.8816\n",
            "\n",
            "Epoch 00114: val_accuracy did not improve from 0.89475\n",
            "Epoch 115/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1163 - accuracy: 0.9736 - val_loss: 0.8687 - val_accuracy: 0.8832\n",
            "\n",
            "Epoch 00115: val_accuracy did not improve from 0.89475\n",
            "Epoch 116/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1159 - accuracy: 0.9728 - val_loss: 0.8180 - val_accuracy: 0.8882\n",
            "\n",
            "Epoch 00116: val_accuracy did not improve from 0.89475\n",
            "Epoch 117/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1127 - accuracy: 0.9730 - val_loss: 0.9564 - val_accuracy: 0.8755\n",
            "\n",
            "Epoch 00117: val_accuracy did not improve from 0.89475\n",
            "Epoch 118/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1130 - accuracy: 0.9730 - val_loss: 0.8649 - val_accuracy: 0.8797\n",
            "\n",
            "Epoch 00118: val_accuracy did not improve from 0.89475\n",
            "Epoch 119/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1130 - accuracy: 0.9729 - val_loss: 0.8387 - val_accuracy: 0.8844\n",
            "\n",
            "Epoch 00119: val_accuracy did not improve from 0.89475\n",
            "Epoch 120/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1185 - accuracy: 0.9712 - val_loss: 0.8803 - val_accuracy: 0.8829\n",
            "\n",
            "Epoch 00120: val_accuracy did not improve from 0.89475\n",
            "Epoch 121/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1128 - accuracy: 0.9738 - val_loss: 0.9069 - val_accuracy: 0.8800\n",
            "\n",
            "Epoch 00121: val_accuracy did not improve from 0.89475\n",
            "Epoch 122/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1145 - accuracy: 0.9733 - val_loss: 0.9067 - val_accuracy: 0.8814\n",
            "\n",
            "Epoch 00122: val_accuracy did not improve from 0.89475\n",
            "Epoch 123/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1138 - accuracy: 0.9742 - val_loss: 0.8786 - val_accuracy: 0.8843\n",
            "\n",
            "Epoch 00123: val_accuracy did not improve from 0.89475\n",
            "Epoch 124/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1121 - accuracy: 0.9748 - val_loss: 0.8722 - val_accuracy: 0.8843\n",
            "\n",
            "Epoch 00124: val_accuracy did not improve from 0.89475\n",
            "Epoch 125/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1105 - accuracy: 0.9755 - val_loss: 0.8265 - val_accuracy: 0.8864\n",
            "\n",
            "Epoch 00125: val_accuracy did not improve from 0.89475\n",
            "Epoch 126/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1104 - accuracy: 0.9749 - val_loss: 0.8304 - val_accuracy: 0.8860\n",
            "\n",
            "Epoch 00126: val_accuracy did not improve from 0.89475\n",
            "Epoch 127/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1124 - accuracy: 0.9739 - val_loss: 0.8486 - val_accuracy: 0.8831\n",
            "\n",
            "Epoch 00127: val_accuracy did not improve from 0.89475\n",
            "Epoch 128/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1100 - accuracy: 0.9754 - val_loss: 0.8013 - val_accuracy: 0.8849\n",
            "\n",
            "Epoch 00128: val_accuracy did not improve from 0.89475\n",
            "Epoch 129/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1100 - accuracy: 0.9749 - val_loss: 0.8862 - val_accuracy: 0.8848\n",
            "\n",
            "Epoch 00129: val_accuracy did not improve from 0.89475\n",
            "Epoch 130/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1053 - accuracy: 0.9758 - val_loss: 0.9508 - val_accuracy: 0.8734\n",
            "\n",
            "Epoch 00130: val_accuracy did not improve from 0.89475\n",
            "Epoch 131/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1094 - accuracy: 0.9762 - val_loss: 0.8466 - val_accuracy: 0.8892\n",
            "\n",
            "Epoch 00131: val_accuracy did not improve from 0.89475\n",
            "Epoch 132/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1094 - accuracy: 0.9755 - val_loss: 0.8287 - val_accuracy: 0.8865\n",
            "\n",
            "Epoch 00132: val_accuracy did not improve from 0.89475\n",
            "Epoch 133/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1093 - accuracy: 0.9752 - val_loss: 0.8281 - val_accuracy: 0.8872\n",
            "\n",
            "Epoch 00133: val_accuracy did not improve from 0.89475\n",
            "Epoch 134/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1120 - accuracy: 0.9748 - val_loss: 0.8575 - val_accuracy: 0.8852\n",
            "\n",
            "Epoch 00134: val_accuracy did not improve from 0.89475\n",
            "Epoch 135/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1101 - accuracy: 0.9750 - val_loss: 0.9167 - val_accuracy: 0.8855\n",
            "\n",
            "Epoch 00135: val_accuracy did not improve from 0.89475\n",
            "Epoch 136/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1078 - accuracy: 0.9754 - val_loss: 0.9070 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00136: val_accuracy did not improve from 0.89475\n",
            "Epoch 137/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1091 - accuracy: 0.9759 - val_loss: 0.7749 - val_accuracy: 0.8854\n",
            "\n",
            "Epoch 00137: val_accuracy did not improve from 0.89475\n",
            "Epoch 138/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1051 - accuracy: 0.9762 - val_loss: 0.8353 - val_accuracy: 0.8844\n",
            "\n",
            "Epoch 00138: val_accuracy did not improve from 0.89475\n",
            "Epoch 139/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1034 - accuracy: 0.9778 - val_loss: 0.8653 - val_accuracy: 0.8887\n",
            "\n",
            "Epoch 00139: val_accuracy did not improve from 0.89475\n",
            "Epoch 140/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1078 - accuracy: 0.9768 - val_loss: 1.0180 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00140: val_accuracy did not improve from 0.89475\n",
            "Epoch 141/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1083 - accuracy: 0.9764 - val_loss: 0.8185 - val_accuracy: 0.8767\n",
            "\n",
            "Epoch 00141: val_accuracy did not improve from 0.89475\n",
            "Epoch 142/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1071 - accuracy: 0.9767 - val_loss: 0.8948 - val_accuracy: 0.8792\n",
            "\n",
            "Epoch 00142: val_accuracy did not improve from 0.89475\n",
            "Epoch 143/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1088 - accuracy: 0.9755 - val_loss: 0.9582 - val_accuracy: 0.8836\n",
            "\n",
            "Epoch 00143: val_accuracy did not improve from 0.89475\n",
            "Epoch 144/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1059 - accuracy: 0.9772 - val_loss: 1.0503 - val_accuracy: 0.8848\n",
            "\n",
            "Epoch 00144: val_accuracy did not improve from 0.89475\n",
            "Epoch 145/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1099 - accuracy: 0.9768 - val_loss: 0.9534 - val_accuracy: 0.8836\n",
            "\n",
            "Epoch 00145: val_accuracy did not improve from 0.89475\n",
            "Epoch 146/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1034 - accuracy: 0.9785 - val_loss: 0.8850 - val_accuracy: 0.8886\n",
            "\n",
            "Epoch 00146: val_accuracy did not improve from 0.89475\n",
            "Epoch 147/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1066 - accuracy: 0.9777 - val_loss: 0.8642 - val_accuracy: 0.8887\n",
            "\n",
            "Epoch 00147: val_accuracy did not improve from 0.89475\n",
            "Epoch 148/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1038 - accuracy: 0.9764 - val_loss: 0.8918 - val_accuracy: 0.8857\n",
            "\n",
            "Epoch 00148: val_accuracy did not improve from 0.89475\n",
            "Epoch 149/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1070 - accuracy: 0.9774 - val_loss: 0.8841 - val_accuracy: 0.8904\n",
            "\n",
            "Epoch 00149: val_accuracy did not improve from 0.89475\n",
            "Epoch 150/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1073 - accuracy: 0.9776 - val_loss: 0.9132 - val_accuracy: 0.8873\n",
            "\n",
            "Epoch 00150: val_accuracy did not improve from 0.89475\n",
            "Epoch 151/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1039 - accuracy: 0.9772 - val_loss: 0.8727 - val_accuracy: 0.8913\n",
            "\n",
            "Epoch 00151: val_accuracy did not improve from 0.89475\n",
            "Epoch 152/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1037 - accuracy: 0.9788 - val_loss: 0.8260 - val_accuracy: 0.8876\n",
            "\n",
            "Epoch 00152: val_accuracy did not improve from 0.89475\n",
            "Epoch 153/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1037 - accuracy: 0.9773 - val_loss: 1.0347 - val_accuracy: 0.8778\n",
            "\n",
            "Epoch 00153: val_accuracy did not improve from 0.89475\n",
            "Epoch 154/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1140 - accuracy: 0.9747 - val_loss: 0.9901 - val_accuracy: 0.8869\n",
            "\n",
            "Epoch 00154: val_accuracy did not improve from 0.89475\n",
            "Epoch 155/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1035 - accuracy: 0.9786 - val_loss: 0.8442 - val_accuracy: 0.8871\n",
            "\n",
            "Epoch 00155: val_accuracy did not improve from 0.89475\n",
            "Epoch 156/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1050 - accuracy: 0.9787 - val_loss: 1.0245 - val_accuracy: 0.8734\n",
            "\n",
            "Epoch 00156: val_accuracy did not improve from 0.89475\n",
            "Epoch 157/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1100 - accuracy: 0.9766 - val_loss: 0.8771 - val_accuracy: 0.8856\n",
            "\n",
            "Epoch 00157: val_accuracy did not improve from 0.89475\n",
            "Epoch 158/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1020 - accuracy: 0.9791 - val_loss: 0.8829 - val_accuracy: 0.8909\n",
            "\n",
            "Epoch 00158: val_accuracy did not improve from 0.89475\n",
            "Epoch 159/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1014 - accuracy: 0.9794 - val_loss: 1.1487 - val_accuracy: 0.8708\n",
            "\n",
            "Epoch 00159: val_accuracy did not improve from 0.89475\n",
            "Epoch 160/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1094 - accuracy: 0.9759 - val_loss: 0.8696 - val_accuracy: 0.8889\n",
            "\n",
            "Epoch 00160: val_accuracy did not improve from 0.89475\n",
            "Epoch 161/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1009 - accuracy: 0.9783 - val_loss: 0.8764 - val_accuracy: 0.8901\n",
            "\n",
            "Epoch 00161: val_accuracy did not improve from 0.89475\n",
            "Epoch 162/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.0997 - accuracy: 0.9785 - val_loss: 0.9283 - val_accuracy: 0.8791\n",
            "\n",
            "Epoch 00162: val_accuracy did not improve from 0.89475\n",
            "Epoch 163/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1017 - accuracy: 0.9786 - val_loss: 0.9313 - val_accuracy: 0.8878\n",
            "\n",
            "Epoch 00163: val_accuracy did not improve from 0.89475\n",
            "Epoch 164/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.0976 - accuracy: 0.9803 - val_loss: 0.8779 - val_accuracy: 0.8882\n",
            "\n",
            "Epoch 00164: val_accuracy did not improve from 0.89475\n",
            "Epoch 165/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1039 - accuracy: 0.9778 - val_loss: 0.9291 - val_accuracy: 0.8886\n",
            "\n",
            "Epoch 00165: val_accuracy did not improve from 0.89475\n",
            "Epoch 166/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1000 - accuracy: 0.9794 - val_loss: 0.9719 - val_accuracy: 0.8866\n",
            "\n",
            "Epoch 00166: val_accuracy did not improve from 0.89475\n",
            "Epoch 167/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1003 - accuracy: 0.9796 - val_loss: 0.9401 - val_accuracy: 0.8857\n",
            "\n",
            "Epoch 00167: val_accuracy did not improve from 0.89475\n",
            "Epoch 168/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1044 - accuracy: 0.9795 - val_loss: 0.9349 - val_accuracy: 0.8880\n",
            "\n",
            "Epoch 00168: val_accuracy did not improve from 0.89475\n",
            "Epoch 169/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1032 - accuracy: 0.9793 - val_loss: 0.9355 - val_accuracy: 0.8843\n",
            "\n",
            "Epoch 00169: val_accuracy did not improve from 0.89475\n",
            "Epoch 170/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1028 - accuracy: 0.9787 - val_loss: 0.9913 - val_accuracy: 0.8852\n",
            "\n",
            "Epoch 00170: val_accuracy did not improve from 0.89475\n",
            "Epoch 171/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1045 - accuracy: 0.9777 - val_loss: 1.1296 - val_accuracy: 0.8819\n",
            "\n",
            "Epoch 00171: val_accuracy did not improve from 0.89475\n",
            "Epoch 172/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1003 - accuracy: 0.9792 - val_loss: 0.9377 - val_accuracy: 0.8899\n",
            "\n",
            "Epoch 00172: val_accuracy did not improve from 0.89475\n",
            "Epoch 173/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1021 - accuracy: 0.9792 - val_loss: 1.0190 - val_accuracy: 0.8800\n",
            "\n",
            "Epoch 00173: val_accuracy did not improve from 0.89475\n",
            "Epoch 174/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1065 - accuracy: 0.9782 - val_loss: 0.9411 - val_accuracy: 0.8880\n",
            "\n",
            "Epoch 00174: val_accuracy did not improve from 0.89475\n",
            "Epoch 175/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1025 - accuracy: 0.9786 - val_loss: 0.9639 - val_accuracy: 0.8809\n",
            "\n",
            "Epoch 00175: val_accuracy did not improve from 0.89475\n",
            "Epoch 176/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.0979 - accuracy: 0.9812 - val_loss: 0.8273 - val_accuracy: 0.8832\n",
            "\n",
            "Epoch 00176: val_accuracy did not improve from 0.89475\n",
            "Epoch 177/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1031 - accuracy: 0.9784 - val_loss: 0.9974 - val_accuracy: 0.8888\n",
            "\n",
            "Epoch 00177: val_accuracy did not improve from 0.89475\n",
            "Epoch 178/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1012 - accuracy: 0.9805 - val_loss: 0.9466 - val_accuracy: 0.8930\n",
            "\n",
            "Epoch 00178: val_accuracy did not improve from 0.89475\n",
            "Epoch 179/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1035 - accuracy: 0.9797 - val_loss: 0.9454 - val_accuracy: 0.8849\n",
            "\n",
            "Epoch 00179: val_accuracy did not improve from 0.89475\n",
            "Epoch 180/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1028 - accuracy: 0.9787 - val_loss: 1.0554 - val_accuracy: 0.8829\n",
            "\n",
            "Epoch 00180: val_accuracy did not improve from 0.89475\n",
            "Epoch 181/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1057 - accuracy: 0.9789 - val_loss: 0.9818 - val_accuracy: 0.8865\n",
            "\n",
            "Epoch 00181: val_accuracy did not improve from 0.89475\n",
            "Epoch 182/500\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1003 - accuracy: 0.9795 - val_loss: 0.8732 - val_accuracy: 0.8882\n",
            "\n",
            "Epoch 00182: val_accuracy did not improve from 0.89475\n",
            "Epoch 00182: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "fRJpuAzsNXRY",
        "outputId": "b5da679e-7917-41f6-b8a3-62ee3663e0a0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = range(1, len(Hist_L2.history['loss'])+1)\n",
        "\n",
        "plt.figure(figsize=(9,6))\n",
        "plt.plot(epochs, Hist_L2.history['loss'])\n",
        "plt.plot(epochs, Hist_L2.history['val_loss'])\n",
        "plt.legend(['Train loss','Test loss'])\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhsAAAFpCAYAAAA1JerqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd5wkdZ333zU5x53ZnNmc2WWXIDIEJYkEMaKCiYc7FU/vCM/dGc7TO73nDj0j4omcERQJkkQFRhaBXZawOefZ2dnJOfSEev741m+qOk73TM/0TM/3/Xrtq7qrq6t/1btQn/58k2XbNoqiKIqiKKNFSqIXoCiKoihKcqNiQ1EURVGUUUXFhqIoiqIoo4qKDUVRFEVRRhUVG4qiKIqijCoqNhRFURRFGVWGFBuWZd1vWVatZVm7wrx+k2VZOyzL2mlZ1suWZa2J/zIVRVEURZmoRONsPABcEeH1o8BFtm2vAv4VuC8O61IURVEUJUlIG+oA27ZftCxrXoTXX/Y8fRWYNfJlKYqiKIqSLMQ7Z+MTwDNxPqeiKIqiKBOYIZ2NaLEs62JEbLwtwjG3ArcCZGdnr589e3a8Ph6AgYEBUlImZ86rXrte+2RDr12vfbIx3q/9wIED9bZtl4V6LS5iw7Ks1cD/AFfatt0Q7jjbtu/DyenYsGGDvW3btnh8/CCVlZVUVFTE9ZwTBb32ikQvIyHotVckehkJQa+9ItHLSAjj/dotyzoe7rURSyTLsuYAjwAfsW37wEjPpyiKoihKcjGks2FZ1q+BCmCKZVlVwJeBdADbtu8FvgSUAj+wLAugz7btDaO1YEVRFEVRJhbRVKN8cIjXPwl8Mm4rUhRFURQlqYhbgmg86O3tpaqqiu7u7mG9v7CwkL1798Z5VRMD77VnZWUxa9Ys0tPTE7wqRVEURRlnYqOqqor8/HzmzZuHE5KJiba2NvLz80dhZeMfc+22bdPQ0EBVVRXz589P9LIURVEUZXzNRunu7qa0tHRYQkMRLMuitLR02O6QoiiKosSbcSU2ABUacUC/Q0VRFGU8Me7ERiJpaGhg7dq1rF27lmnTpjFz5szB5z6fL+J7t23bxu233x7T582bN4/6+vqRLFlRFEVRxj3jKmcj0ZSWlvLWW28B8JWvfIW8vDz+4R/+YfD1vr4+0tJCf2UbNmxgwwat+FUURVGUQNTZGIJbbrmF2267jU2bNnHnnXeydetWzjvvPNatW8f555/P/v37Aens9q53vQsQofLxj3+ciooKFixYwHe+850hP+eee+5h5cqVrFy5km9/+9sAdHR0cPXVV7NmzRpWrlzJQw89BMDdd9/N8uXLWb16tZ8YUhRFUZTxyLh1Nv7lid3sqW6N6T39/f2kpqaGfX35jAK+fM2KmNdSVVXFyy+/TGpqKq2trWzevJm0tDT+/Oc/84//+I/87ne/C3rPvn37eOGFF2hra2PJkiX8zd/8TdhS1Ndff52f/vSnbNmyBdu22bRpExdddBFHjhxhxowZPPXUUwC0tLTQ0NDAo48+yr59+7Asi+bm5pivR1EURVHGEnU2ouC9733voIhpaWnhve99LytXruTzn/88u3fvDvmeq6++mszMTKZMmUJ5eTlnzpwJe/6XXnqJ66+/ntzcXPLy8rjhhhvYvHkzq1at4k9/+hN33XUXmzdvprCwkMLCQrKysvjEJz7BI488Qk5Ozqhcs6IoyqSibj/YdqJXkbSMW2djOA7EaPXZyM3NHXz8xS9+kYsvvphHH32UY8eOhR2Kk5mZOfg4NTWVvr6+mD938eLFvPHGGzz99NP88z//M5deeilf+tKX2Lp1K8899xwPP/ww3/ve93j++edjPreiKIri0HAYvr8Rbn4C5r890atJStTZiJGWlhZmzpwJwAMPPBCXc1544YU89thjdHZ20tHRwaOPPsqFF15IdXU1OTk5fPjDH+aOO+7gjTfeoL29nZaWFq666iq+9a1vsX379risQVEUZdLSUee/VeLOuHU2xit33nknN998M1/72te4+uqr43LOs88+m1tuuYWNGzcC8MlPfpJ169bx7LPPcscdd5CSkkJ6ejo//OEPaWtr49prr6W7uxvbtrnnnnvisgZFUZRJS5/TBLGvJ7HrSGJUbIThK1/5Ssj95513HgcOHBh8/rWvfQ2AioqKwZBK4Ht37doV8lzHjh0bfPyFL3yBL3zhC36vX3755Vx++eVB79u6desQq1cURVGiptcRG71diV1HEqNhFEVRFGVy0+eIDHU2Rg0VG4qiKMrkxoiMvgTMlPr97bD7sbH/3DFGxYaiKIoyuelNoLOx/UE4nPwVhSo2FEVRlMlNopyN3m7o7wFfx9h+bgJQsaEoiqJMbhKVs9HtdIBWsaEoiqIoSc6gszHG1SjdLbL1tY/t5yYALX310NDQwKWXXgpATU0NqamplJWVAVJumpGREfH9lZWVZGRkcP755we99sADD7Bt2za+973vxX/hiqIoyvBJVM5G1+RxNlRseBhqxPxQVFZWkpeXF1JsKIqiKOOUwaZeY5yzMehsJL/Y0DDKELz++utcdNFFrF+/nssvv5zTp08D8J3vfGdwzPsHPvABjh07xr333su3vvUt1q5dy+bNm8Oe89ixY1xyySWsXr2aSy+9lBMnTgDw29/+lpUrV7JmzRre/nbpz7979242btzI2rVrWb16NQcPHhz9i1YURZlMJKqD6CTK2Ri/zsYzd0PNzpjekt3fB6kRLmnaKrjyG1Gfz7ZtPvvZz/L4449TVlbGQw89xD/90z9x//33841vfIOjR4+SmZlJc3MzRUVF3HbbbVG5IZ/97Ge5+eabufnmm7n//vu5/fbbeeyxx/jqV7/Ks88+y8yZMwdHx99777187nOf46abbsLn89Hf3x/1+hVFUZQo6E20s6E5G5Oanp4edu3axTve8Q4A+vv7mT59OgCrV6/mpptu4rrrruO6666L6byvvPIKjzzyCAAf+chHuPPOOwG44IILuOWWW3jf+97HDTfcAEh79K9//etUVVVxww03sGjRonhdnqIoigKJczZMzkZv59h+bgIYv2IjBgfC0BXnEfO2bbNixQpeeeWVoNeeeuopXnzxRZ544gm+/vWvs3NnbC5MKO699162bNnCU089xfr163n99df50Ic+xKZNm3jqqae46qqr+NGPfsQll1wy4s9SFEVRHPoSNBvFhFH6fdDng7TIRQgTGc3ZiEBmZiZ1dXWDYqO3t5fdu3czMDDAyZMnufjii/nmN79JS0sL7e3t5Ofn09bWNuR5zz//fB588EEAfvnLX3LhhRcCcPjwYTZt2sRXv/pVysrKOHnyJEeOHGHBggXcfvvtXHvttezYsWP0LlhRFGUykuicDYDe5M7bULERgZSUFB5++GHuuusu1qxZw9q1a3n55Zfp7+/nwx/+MKtWrWLdunXcfvvtFBUVcc011/Doo48OmSD63e9+l5/+9KesXr2an//85/z3f/83AHfccQerVq1i5cqVnH/++axZs4bf/OY3rFy5krVr17Jr1y4++tGPjtXlK4qiTA4SnbMBSZ8kOn7DKAnGOyb+xRdfDHr9pZdeCtq3ePHisM7DLbfcwi233ALA3Llzef754F74Jo/Dy913383dd98d5aoVRVGUmElUB9Euj7OR5GJDnQ1FURRlcpOo2SjdLWClyuMkr0hRsaEoiqJMbhLVQbS7GfKlwlGdDUVRFEVJZhI5G6VghjxWsTG22Lad6CVMePQ7VBRFiQEjMgb6oL9vbD5zYAC6W6FwpjxXsTF2ZGVl0dDQoDfLEWDbNg0NDWRlZSV6KYqiKBODvh7Aksf9YxRK6WkFbCiYHGJjXFWjzJo1i6qqKurq6ob1/u7u7kl7k/Vee1ZWFrNmzUrwihRFUSYAti2JoVmFEtbo64GM3NH/XFP2qmJj7ElPT2f+/PnDfn9lZSXr1q2L44omDpP52hVFUYaNydfILnbExhhVpJiGXgWaIKooiqIoyY3J18gqcp6PldhwnI2cKZCaGf/S185G+Nl10Fod3/MOExUbiqIoythxYoskRo4XTPfQrEL/56ONaeiVVShhm3g7G9VvwpEX4FhwA8pEoGJDURRFGRt8HfDAVbDtJ4leiYtxMrIT5GxkF0FGXvzFRmeDbJtPxPe8w0TFhqIoijI2tNVIeWnLqUSvxKUvwNkYq8Ze3YHORpzDKB31sm05Gd/zDhMVG4qiKMrY0FYj2/YziV2Hl0GxkQBnw0qBjPzRCaN0OmKjWcWGoiiKMplod8RGx/DaG4wKvYFhlDFyNrqaIbMAUlJGSWw4YRR1NhRFUZRJRXutsx0FZ6NmF/z3GmiL8dyJdDaMwMnIg944i40Oj7MxDhplqthQFEVRxobBMMooOBtndkPTMTj+19jel7AE0WY3TyQjZ/Scjb4u93ECUbGhKIqijA3G0fC1xf/mapyB02/F+D7TZ8MkiI6hs2HclNEIo3TUQ1q2PB4HFSkqNhRFUZSxwRs+MSGVeOHrlG11GLHxu0/Co7cF7zc5GlnF/s9Hmy6vszFKpa/TVsnjcZC3oWJDURRFGTnNJ2SSaSTazri/tuOdJNrriI3Tb4XOUajbD3X7gvebDqKJ6LORHeBsmO/v4J+gdu/wzz3QD11NMMMZYTEOKlJUbCiKoigjo6MevnM27Hsi8nHtNTB1ufM4zkmixhnoboGmoyFeb3cbaXkZdDYS0Gdj0NnIBWxX+Dx6G/zlP4Z/7s5GOV/pWeKaqLOhKIqiTHjaz8BAL7SeDn9Mn8/f2o93GMU4GxA6lOLrCN0m3eRsZORCStrYOBu93e6kWfPZZo293dIjo6Vq+Oc3PTZyS6Fw9sRwNizLut+yrFrLsnaFed2yLOs7lmUdsixrh2VZZ8d/mYqiKMq4xdzEI5VvmrDJ1JWANTo5GzlTIDUjdJJoj+NsBIZYjJORliV/xmI2inFYsjylryDuS5szOK11BF1WTfVJTikUzYaWiZEg+gBwRYTXrwQWOX9uBX448mUpiqIoE4YeR2z4OsMfYxp6Fc6CnBLoiLez0SE5EOXLg52NgQF5faA32Lno6xKRYVmQljk2zkaQ2PA4G2ZKa9tp6O8b3vlNj42cKRPH2bBt+0WgMcIh1wI/s4VXgSLLsqbHa4GKoijKOMc4G5EqKkyzrbyp8mc0nI30HJixFk5v93cwvI5LYN5Gb7eIDBDRMRY5G2YuikkQTc+RrVds2AMiOIbDYBhlijgb3c3Q0zb89caBeORszAS8sqnK2acoiqJMBnqcG3ikMIpxNvKmQm7Z6ORsZOTC9LVyc2065lmfZ8hZYN5GX7dbITPmzoan9BUcseEJnwwVSjmzG+oPBe/vcMIo2SXibMDIckDiQNpYfphlWbcioRamTp1KZWVlXM/f3t4e93NOFPTaKxO9jISg116Z6GUkhPF27XOOv8UC4EzVUfaGWdfcY1uZh8WLr+9laQcUtB5nyzCuIdy1n91QQ19aHkdO97MB2P3nX1FXfgEA2Z2n2OQc98bLL9BaWD34vqWnjlPYZ7OlspINPf101VSxe5S/2/IzL7Mc2LrjAJ2HO8ltP8Y5wK43t1LUvJNZznF7XvkjtUdc8RN47We/fgf9qZlsX/s1v/OfdXA701JzeemllyloqedsYMfmp2gsTdwAvHiIjVPAbM/zWc6+IGzbvg+4D2DDhg12RUVFHD7epbKyknifc6Kg116R6GUkBL32ikQvIyGMu2v/cyUchalFeUwNt64nHoPaUi665DLofQG2vUbFRRdJrkQMhL323WkwZRYlV34E3rybFcU+MMdVvwlb5eHZKxbCWZ73194PA8VyzgOl5OXkj/53u/Ug7IWNb38H5E+FxqOwDVYumgf7dokb0XKS5TMLWP42dy1B176jDzprg7/H+p9B11Q5tnUJvHkXq+cUwTmjfF0RiEcY5ffAR52qlHOBFtu2hxloUhRFUSYcgzkb7eGPaT8D+dPkcV65hD0iHR8rvR2QniuhkLIl/g28IoVRgnI2xiCM0lIlZbY5JfLcW43SekrWn1kwdBilp01CRoG5HR31khwKErZKzUh4kuiQzoZlWb8GKoAplmVVAV8G0gFs274XeBq4CjgEdAIfG63FKoqiKOMQU43SG6Eapa1GRAZArrNtr4XM/PiswdcpA81AbuJdzZ7XIiSIBuZsxFMAhaP+IJQshNR0eR5YjTJtFRTMhJahxIaz1to9UDDD3d/Z4OZqpKQ45xrnYsO27Q8O8boNfDpuK1IURVEmFt3RlL7WQtlSeZznERulC+Ozht5Ot6ojswA6jriveQVET4gE0XQjNrLcSo7RpH4/lC9zn6dnA5a0GG+vFXFQeBpaIyR19ve5HUfP7IGzLnNf62yQqhxDUeLLX7WDqKIoijIyeoYofbVtJ4wyVZ4Pio04JSwODLjVKCD9K7wOhrfsM6j01emzAU41yiiXvvb5JEdjyhJ3n2VJKKXxMGCLS1E4M3IFic9zTbV73Me27R9GAQmlxLuvSYyo2FAURVFGxlAdRDsbpaFWnsnZcERHvIaxmV/4xtnIKvTPzTAiKCU9ROlrjys20rNHlrPR3+vMJYlA42Gw+yUvw0tGroRXQMRGwSxxKEw79UC8eShndnv2t8p3nVPq7suZ4pbDJggVG4qiKMrIGMrZGOyx4TgaOaVgpcTP2TDhm0GxUSC//E0HThNGyZ8WImcjjs7G5v+C750T+Rx1+2U7ZbH//owcaDgsjwtmSqdVcJt8BWLcmsLZck5zraZVea7H2cgtle9jrIbMhUDFhqIoijIyjFvQ1y3jzQMxosJUo6Skyq/teDX2Mo5KhsfZAI8Iapck0OziEDkbPZBuxMYIZ6Mc3Sw5Hye3hD/GuBdTFvnvz8gVRwLcMAqET+w0Amr2RujvgUYnR8U4GN4winncMQb5KGFQsaEoihItm++Bv/y/RK9ifDEwIDdwU9ERyt3wtio35JXHT2wEOhuZBbI1wqKnHTLznPBKqHblXmdjmGJjoN8dAHfoz+GPq98vboTJLzGY8teMPFl/gREbTkXKI7cy+8Qj7vHG2Zh9rmxrnVCKSXD1hlGMyzEWya9hULGhKIoSLfufgX1PJnoV4wtfO5LU6IzEClX+6m1Vbsgrj1/SovnMwQRRx9kwwsLXLjfxwFwOcEpfPc5Gf0/wZNhoqD8on2OlwqHnwh9Xtz84hOJde8EMSRg1YqP1lLxnx0MUNXuGrxuxMXO9hKRq98rzDs94eYM6G4qiKBOI3s5gGz4SfT3w24+5sfhkxHwf+Y7YCOVstJ6GjHxxFwzxHMZmPjM9IIziHRBnHAPv39/AgIiLdE+fDRhebkP1G7Jd/X44s0uuOZCBARElgcmh4C82QEI7uWVSkfLGz2R5fZ6kUBNGyZ0iPTtMkmhniDDKoLORuCRRFRuKoijR4usI/mUcicYjsPuRyLb6aGPbQzeHGgndUYiNpqNQMs9/nxnGNhwXIZBBZ8OTIAqus9HT5oRRCvzDKCZkMthBNNt/fyycel0E1bm3yfPDzwcf01olCakhnQ1HiBV45pgWzJR/Q9t/Lcvr83y3pholM196dpjy1856cWi8YRoTUlFnQ1EUZQLg64jN2TBdLNtqRmc90XDwj/DtVaPX1MnY+Sb5M1QYpfEIlCzw31c4S1yFeLgb5jPTowij9LSJwwAesREHZ+PUG9JIa9pqcW1CCcy6A7KNxtkA+Y6OvSSORNFc0nu9zcmc7z0jD6aukN4dvg5JEM0p9Z+VklUk4R3N2VAURZkA9HZCvy/6m5G52cWrxHM4nN4hfR2aT4zO+Y34MjfJwHbf/X3QdDxYbBTPk23z8ZGvwRfgbAQmiPo65GaeWQDY7v4gZ8PJ3egL09siHH0+CZ3MWCc3+bMuE2cjsDKnPkzZK4QWGwUzZb0Fs2D5uyWMYpwgXxukZkJahogNbPjOOtj3lH9yKEjL8pxSdTYURVHGPbbthgiiDaV0jwNno+mobOPVQCsQI6iMsxHYsry1Sko6A8VG0VxnfXEQG4HORmZgGMVTjQLBYmOkORtndokInXm2PF94ifzdV7/pf1zdfsgu8e+BYTBr94ZRTK+NdTdBdgkpdp+7ZhMaAjjrHfCOr4rImboCVt0YfP7cKQnN2YjHiHlFUZTkp68bcH5V9rRCXtnQ7xkPzkbjKIuNoRJETf+HILExR7ZNx0a+Bl9An43UNMmfGAyjOAmigbkcveGcjRhzNkxy6Mz1sl14iXQrffQ2uOo/5DmETw4Fj7PhERuzNsjQunUfgUN/kn1dzSKOetrdIXbpWXDB5yKvUZ0NRVGUCYD3F3tgr4ZwjIecjcFmT6N0owlMEA1sWR5ObGTkSG5D87GRr6G3E7BcsQBOMmir40i1uTkb3jUH5myY5l6xOhun3pDqDzNpNacEPvQgDPTBz6+HH18KP74Eql4LHUIBmL4GypZByXx339zz4Y6DMkhtcO3OvylfuwiqaMmdojkbiqIo4x7vTTTaJFEjSjrrZW7GWOPrdHtcjKazYaW6oYEgZ+Oo3MzNXBQvRXPjE0bxOUPY/JIiC+XG3NsF9oCEHAJzOQbDKJ4+G979XnraYMdv4fHPuALKcOoNCaF4P/+sy+BvX4VLvyR9MLKKYPm1sOHjoa9h3gXw6VeDm30NXk+RbP0qbGIQGzlTEupsaBhFURQlGvycjRhzNkBu9t7kv7HAG6IYtZyNVnERTM5BYM5G4xH5tZ4S4rdt8Vw4EaG1d7T0drg9Ngymp8ZgiCUvuErFDDlLCxQbAc7G9ofg95+V6hmz7rffIY99HZL4ufza4HWlZ8GFfy9/Rkq2IzaMW9bT5s6aiYbcKfLvsb8XUtNHvp4YUWdDURQlGvycjbbwx3nxhlsSEUoxyaEZ+aOXHNjTKjf2lBRxMEKFUQJDKIbieZJAOlLXx9fpJnkaTGtyn6dENCiM4ogHb7tyCJ60+tYvZFbJx5+VUEntPve12n3inExbNbJrGIpBZ8MjNjLywh8fiKlQGWoq7SihYkNRFCUavL/Yow2jdDW7v7gTkSRq7P5Z60fX2TDhiYxc/zDKwICEUbx5CF6K5sqNuqVqZGvo7QwOPxixMdj8Ki+4SqUvSmejdi/MvQDmnAtlS93JreA20ypfNrJrGIrAMIqvPbYwSoLno6jYUBRFiYbe4YRRWtzpnolwNhqPyk2qdFFsYuPYS3Dg2eiO7Wl1qzwycvxFWdtpCT2EdTZM+eux6NcWCl+IMIpJEB0Mo+RKT4q0bOgxYsMRFekBzoY3Z6O9Tr47IybKl0L9AbeHRu0eOWdxGEEVL4wrMxhGiVFsJHg+iooNRVGUaPANM0G01BEbiXA2mhxXIbcMupqiD1f86cvw6w/A7seGPtbP2cjzb+oVrhLFYHptRNvYa8/j5LYfDd7f2+mWvRoGwyjOekzlhnfya1DORoh25XXOgDMjNsqWOiPdnXXU7hEBEionJZ6kptGXmi1hlIF+CVeps6EoipJk9A6j9LW7Wf4nn1OaOGejeH5sg7hsGxoOyvaRT8HhFyIf39PiOhvpOf7f01Bio2AmpKRFV5HS0wa/+yTzj/4q+LXeTjdB1ZBZIJ1TjcgzDbCM4wHhcza8YRQzTbV8uWzLHNFR5+RtnNkD5SuGXn8c6EvLFWdjUEDFkrNhnI3ENPZSsaEoihINJjyQUxqdszHQ74QYiqTsM5KzMdAPD38CfnoV/Ow6+MM/jny9/b3SorxkgSs2ogmldDaImLroLnFlHrwpcpgjUs5G4xFIzfBvVOUlNU26ZEbjbBx6Dvp9FLTuDx7e5gvjbAC0VrtrM/sDczYGO4iGKH2t3QPZxdITBKDM6ZNRt1dCEh21o5+v4dCXlufkoThJr5mxiI0SwFJnQ1EUZVxjqizyp0eXs2FuaFmFkD81srNx7CXY9bD8Ym2thle/75+EOBxaTsovexNGgejERv1B2c7aANd+T647sO22wbblxpflFRsBzkbxPEhJDf95RXOjy9nY/7R8RG9LsDjp7QyRs2HEhjPx1rgA3jHzvd2AJYIInJJQK0Bs7BVXw/TQyMx3K1JMcujU5UOvPw6I2Gj2n/gaLSmpIpo0Z0NRFGUc43O6VOaWRVf6asRGdhTOxp7H5Gb5sT/ARx+XfXt/P7L1mpyCYq/YiOJG0+CIjdKz3HknXU2hj/V1iKDJ9IRR/HI2jg6dOFk8b+gwSn+fJKya8tKTrwWsI1Q1irOmQWcjVBilW9wMIyQspwupERu27YiNAOfCVKQEhlhGmeAwSgxiAxLaRVTFhqIoSjSYX89ZBdGFUUw/BONstNe6o8299PfBnt/D4iskFFAwHWadA3ufHNl6TY+NEk/ORlRi45D80i+a45ZbdjWHPtZ8D15nw+Rs2HbkHhuG4rlyA+xpD3/MiVfk+3zbF+hPyZK2315CNfUya285JdeT5rgXfmGUbjdPw5CW6eZstFbLNQaKDVORUrNDBquZEMso05ue64RRnO89ljAKOF1ENWdDURRl/OLrEDGQWRBjGMVxNgZ6QzsEx1+Sm+2K6919S98Fp9+C5pPDX6+3TXhWkSRiRhVGOSQCISVVchlSM/w7oXox30NmiDBKR52IgHA9NgzhKlIOPw91B+Tx/mdknPqid9JacJa/2OjzyQySwJwNs6bWU/6JlN4wSl93cDOw9GzX2TDORVkIZ6O/R9wWb4hllBlRGAUgt1SdDUVRlHHNoLNRGJ2z0RXgbIA7p8TL7sekkmLRO9x9y66R7b6nhr/exqNOvkSK3Axzy6ITGw2HJIQC8r6soiicDSc/IiNXLH7blpwRcIeThcOEWbyhlM5G+MWN8OOL4cAfYf9TsOAiyMyjtWCJOAqmbNXk0gRWo3gHl3nFRlahiIm+HsnZiORshGvYZcRHR92Y5WuAE0bxtbuiNZZqFEjofBQVG4qiKNHg65CbaWaBCI+helYE5mxAcJJof5/kZiy5wv8XdulCuaHtG0EopSmgc2duFDea/j4JfRixAZJUGC5noyfA2UjPAWy5mZuuoIWzIn9mqMZe+5+WXJDcKfCr98lrS64EELEx0Aent8uxxkkJV40C/uEGb8vyvm63t4bBm7NRu1f+7nJK/I8p80xuHfGkCvgAACAASURBVKNKFHCcDXCTXs33Hi25U6CrMXQ4b5RRsaEoihIN3pwNGDpJ1FuNYgZmBSaJHtsspabeEIph2bvg+F+H/0u05ZT/jT4aZ6PlhIR7/MRG0dBhFG/OBogwi1Zs5JRKhc/h59x9e5+Awjlw219FZKTnwpKrAEdsgBtKMTkigc5GepZbZRLobACceh1qdgaLlLRMp0oFp2FXCDFhKlJgzHpsgEdsmO92ODkb9kB48TiKqNhQFEWJBm/OBgwdSulultHrGXluVUegs7H/GREwZ10W/P6l75Ibw2v/E9xXIpq19rTITdwQjdioPyRb02IdogujZIYSG6dEAGQXR/5My4L1H4NDf5ay2+5WyddYdo3cTD/wK/j7vYPfYW9GkeR5GLFh+noE5l6Af3jHYNb66/fLr/y33+n/HuNsDPRLxUm4SpOypbItXxr5+uJIX5pzHS0nISU9OAQ0FIPD2MY+lKJiQ1EUJRp8TpdK8yt+qCTR7ha52VmW3Owy8oOdjaqtMHN96Bvl9DWw6J1Q+e/w21ti+zXaelq23pH2uWVDuyQNjtgIDKNE62yYihBfh9wQC2dFlzy54WNy89x6Hxz8I/T73LwVy/IPiYBU65wMcDYCHQpwhYXXASiZD1hy/k+/JiEsL2mZkg/y4v+Tpl/TVoZe89KrYfGVwWsbRXrTPc5GrMmhIAmikJC8jbQx/0RFUZR40VEvDae8yZWjRa9xNpz/yQ/lbHQ1+9+IAht79XZDzS44/zOh329Z8MEH4eXvwPNfgzO74G+3SNfNoWhzxIbX2cgplWswuSehaDgoTob5BQwSRonkbFgpbpjCbHs75YY4VAjFkFcOK98Db/1KBE/eVJi9KfzxszdKE7SWU27ORmAYBTzOhkdslC2Bu4+HFwlp2XD0RRGCq94HK24IfdyGj8mfMcR1Nk65ScexkJO4+SjqbCiKMnF59QeSQGgqE0YTn5OzkRnG2ehogO+dI3kAIM5GdpH7emBjr5odkh8xc334z0xJhbd9Hq7+L7kJ1x+Ibq1tYZwNiPyr1lSieN2IrCIRFWbKqZfuVhFf5vgMr7NRBYVh2pSHYtOtUmlx+HkJIUUabDZ9rWxrdkR2NrI8A+L89kdwI7KLJNfj6nvghvvcibDjgMGcjf6e2JNDQRKPb3oY5pwf34VFgYoNRVEmLvUHJa+hs3H0P6vX6VJpblSBzkbtbhEDh5xEx+4QzobpZgmSoAgwc8PQnz3nPNmaCoyhMJ9jckXAFRuRftXWH/LP1wBXMIUaPtfVCJmeazSOSVejzAwZquzVy8z1Eh4BN4QSjqkrAAtOe8RGYFMvcL//WBIp3/l1+PQWOOcTY9Y/I1oGnQ2IvewV5O9n0Tsgryx+i4oSFRuKokxczFTR0baFbdu/9BWCnQ0TIjG9Gbpb3C6WIDfT5uPQcFieV22TAWUF0xmS0rPkZlqzI7r1tp2WHBFvXH8oZ6OnHdqq5devF5PgGZgzYtsy02XGWnefCWWY+SrRhlEMl3xRXI15b4t8XGaerLNmh5sgGio0FCpBdCjyyobuepogBlIzpbkZxF6JkmBUbCiKMjEx7bAhutHpI6HfJ30f/EpfA8WGE7o444iNwJyN5dfJdtcjsj21LXIIxUtKKkxdGZuzEShihpr8OpgcGuBsGMEUmCRas0OuebEnwdKEMswQuVjFxoKL4AO/dAaiDcG01UM7G4NVMsNIphyvGKdpOAmiCUTFhqIoE5IMX6N7oxntMIr313Oa8+sySGw4+Rj1+6XhV2DORuFMCYfsfkTchaZj0YsNkOqU0zuia8jUdto/ORSGFhvHNst2xjr//eYaAp2NA88Cln9yrnEQ6ocpNmJh+mrpCxI4Qt6LEUoTzAWISKik1wmAig1FUSYkOZ2e/IfRdjYCfz17J4cajLPR74MzuyWJLzARccX1EmZ565fyfFYU+RqG6WvA1+YOWPPSetoVRCAhHW9yKMjNOD03fBhl/x+kQZXp6GkIN4ztwLMilkzDMggOoxTEkCAaK2YC7IlXRfyFGmMf2GwsGRgUUMNIEE0gKjYURZmQZHeddp+Mdt+AwZbYzk0rMz+Es1Hj3ghOvCJbb84GwPJrAQte/C8pGZ2+lqiZvka2p9/y39/dAj88D/78FXk+MBDa2QBxN0KNuu9slDU7LcH9MDkb3jBKe60kuC6+3P/YtAzpl9HXLeWrsTadioVp5vvYHroSBTwuwMQKOUQke2K6NSo2FEWZkGR3nZYSxayiMXA2TJdK56YWavJr22lJbLRSpc04BDsb+dPkmJ4W6UwZyw2jbKlcb2Dextb7JMRhOmp21svskFBiY8ZaaZoVWFly6M+SkxJSbIRwNg7+CbCDxQa4N/7RdDVAEjnzpzu5NGGci2xnpskYNt4adTSMoiiKMnZkd1XLVNO88tERG3/5D9j2U3kcOOwrq8Df2bBtcTaK50nlyHHH2cgOcDbAnYMy8+zY1pOWIQLFKzZ62uGVH8jjM3tkkJrJYQhV5fK2L4jQ2HKf//79z0BuOcwIsaa0TGl05c3ZOPAHudFPWx18vLnxj2a+hsF8fjhnY+HFcO33Y8uNGe9kaYKooijKmJHddRpKFkq3y3iLjWMvwQtfd3MrAod9BTob3S3S2jp/mowcN6W4gWEUkFBKTqm0Io+V6WtEbJhZKdt+Ij0tNt4qOSL1BzzdQ2cEv3/GWmmx/cr33PX3+cTZWHx5+EZa3mFsfT5pvLX48tB9KEyoKZYeG8PF5G2EqkQBqWpZ9+HIDcImGlqNoiiKMkYMDIjYKDViI47VKP298NQ/yGMzY2SwGsU4G4X+U19NHkT+dP8poKHERu4UuOPw0I2rQjF9DXQ1kdlTJ27Ly9+FBRfLIDOQ7qWRnA2AirtEOGx13I0TL4tL40xVDUl2sRtGObNLOn0uuDj0seY7GgtnY7rjbIQTG8nIBA2j6GwURVEmHm2nSR3wSfOlnlY3XyEW+nvh5JbgBlJb7oW6vXJjr9klbboDq1EyA8Iog27CNJkaagiXKzDczpROkujc4w/D/d+VMtaL7pSun6mZ0vsiPUeST3PLQ59jxjpYdLkIla4mcUrSsmBBRfjPzSpy8zxMU7KyMNNOzU1wPIRRkhENoyiKogB9PaP/GY3ODa90oQyX6myIfQz7a/8DD1wNtfvcfW01UPkNaVR19kcl+bC9NrhLZVaBOBum54XpHprnhFEM8U5MnLoCrFRmnH5Wvufrfghzz5dwwdTl4my0VYvQiDSw7bIvS0fRbfdLf41l10S+YWcXuTkbDYcAy5meGoL0MXQ2iueJ8JtMzob5Xr2t6CcA6mwoihI/6g/CD86FW/8SfjR3PDC/rksWOu5DnzgN4W7ubWekY+eSq1xXYedvZXtmF5Q7v9IP/lFCBJd+CZpPOO+tDuFs5AO29L3IKvQ4G1MlryM9V15Py4jnVcso+vf/gu17DrDm+s/5OyTTVsHeJyElbegW6FNXwGe3yeM+39AdO7OLpaEYiNgomh2+rHUsczYsC6785th81nhhQQV8emtwW/lxjjobiqLEj/qDcuOv2zf0sSOh8TD9KRlSXmnGoYdLEj3wR+lD8eCH4MgLzvuPuIPQave6x57ZLUKhbJlbOtpa7Rlj7gmjgJtk2VbjziJJSRHxMlrllkuvoqlkbXAoZtpqSRatfiN0cmg40jKGDutkeRJEzWTYcGTkSkjHdCwdbdZ+COZfODafNR6wLChbkuhVxIyKDUVR4keXk6gZqnFUPGk4QnfWNLmxG7HRESA2Bvrh2X+CX71Xwht50+Clb8lrZj5JzpRgsTF1uZzX9IlorZY+G2nZblVD4HyUthp/W3vljf4zQ8YCU5nR1RTdcLdYyC4Sx6e/V4RaJLGx4ga48AvjbmKqklg0jKIoSvzoHCOx0XiEruzp5ALkhnA2ervgd5+EfU/Chk/A5f8Gr/0Y/vjPUPW6iI3Z54pAMH0rbFtCKsuvlec5pdJEyzgb3pwG42yYipRAsXHe347GVUfGjF3HDt3QaySYpMT6AyKwIomNRZfJH0XxEJWzYVnWFZZl7bcs65BlWXeHeH2OZVkvWJb1pmVZOyzLilBDpShK0mKcjbZRFBv9vdB0lM4c54YaGEbpaoL/fTfsewqu+Ca86x5Iz4L1t0ho48nPQe1uWHUjlC+TgWi+Tsm76GqS6aogLkb+NEdsdPh3qcybKttGZ05JuPbgY0lmvjsaPXAuykgxLcurnDyPkomVL6AkniHFhmVZqcD3gSuB5cAHLctaHnDYPwO/sW17HfAB4AfxXqiiKBOAsXA2jr8Mfd20FjhJnYFiY9v9ULUV3ve/cO5t7vsy86X5Vc1OKQ1dfq2IDWyZUnpmtxw31dMno2CmG0bxOhvly0VwHHzW7R6aP3XULjlqTCgl3pUKppGUyXOZYMmJSuKJxtnYCByybfuIbds+4EHg2oBjbMCMoCsEqlEUZfIxmLNRO3qfsf9pSMuiscQZhZ6RJ+EO07Xz9HYpiVwe+L8pYNNtknsx/yJpc162TPbX7pMQCoiQMBTMkGoUX6d/eWVKinQAPfScXGt/T+KdDfCIjTg7G1kesZGSDkVz4nt+JemJJmdjJnDS87wK2BRwzFeAP1qW9VkgFwgZsLMs61bgVoCpU6dSWVkZ43Ij097eHvdzThT02isTvYyEMN6ufW31UYoAX1MVL4/Gumybc996hPbCVbR29Q1e+3mpeTQe3s3+9Eo2Hn2Njtw57A7z+QWrvowvo5juykqsgX4utNKoev1ZMnsaKMws49Ut7lTVhc39zGiuoq0/C9tKYbvnnKW9s1nV08rBx/+DRcDuk43U9YzCNYcg3N97Rs98Zsx9H8d2V8Oemrh9XnZnFZsA+8weOnNm8NqLm+N27lgZb//mx5KJfO3xShD9IPCAbdv/ZVnWecDPLctaadv2gPcg27bvA+4D2LBhg11RURGnjxcqKyuJ9zknCnrtFYleRkIYd9e+R/6Tz+htpeLCC4bu3wCSD9HbFV2pZM1O+EstWZd/kbzWPPfa985kemEG088/BypPk7Pp5gjfS8D+/cuYk9UBPfUwd73/+zL3QNVjFNEO5cv9X/OdA3v/i0UtMuF1xaZLpcHWGBD57/09zIv3B3bUw1awGCB39uqE/psbd//mx5CJfO3RhFFOAd6OKbOcfV4+AfwGwLbtV4AsYIyKrBVFGTd0NsqIdWy5QUXDn74M/3NZdB1A9z0FWMFlpTklkrNRu1c+25t3MRTlSyX0Un8g+H0m0bLlZHCXyoxcmP92aW0OE66jY0x4e4ZovoYyDKIRG68BiyzLmm9ZVgaSAPr7gGNOAJcCWJa1DBEbdfFcqKIo4xzblpwNUxERbZLomV3QdBSajw997L6nYPYmybfwklMq4qZmpzyfGkP30vJlstaBvvBiAzt0O+8lHtGTl8RiIzXdnXkSqexVUcIwpNiwbbsP+AzwLLAXqTrZbVnWVy3Lerdz2N8Dn7Isazvwa+AW2451UIGiKBMaXzv0+9zW39EmiTYdk+2JLaFfr9kl7cmbT8igsaUhKuvNmPkzu6STZ9Hc6NdtkkQhWKR4S0i9pa+GRZfLNqsw+YeBmSRRLXtVhkFUORu2bT8NPB2w70uex3uAC+K7NEVREsLRzdLWu+L/RpdzYTBlr2XLYO8T0TkbvV3uXJETr8Ca9/u/Xn8QfnQh2ANSBQGw5Org8+ROkXba1W+JO5ESQ3PkckdspGYG30jzpkqZrD0QWkwUzYapq8QVSXayi6G1Sp0NZVhoB1FFGQ7Vb0meQDKVALadkQ6bO38jz+ddCAsvjv79pux10NmIQmw0OaGTlDQ48Wrw61vuldcu/zeo3SNW/pQQNzvTa6P6TdjwsejXDOKCpOc4Y9oD/peYmi4TVNtrQjsbAO/+bxFNyU52kXwHyZyboowaKjYUZTg8/HGYdQ7c8KNEryR+/PJGGaD2ti/Ay98VdyMWsWGcjfwZElaIJoxiQihnvQMOPCPnyClxz/fWr2DVe2HjpyKfx7zH7o8tXwPEBVl1o5trEkjBDBEb4cIkM9fH9nkTlSmLxOXRmSfKMNBBbIoyHDob3F/yycBAv3TQPO/TcNmXYfZGOFIZ+T2NR+D+K6DdyQXvapJtTomEH6JyNpx232s+INuq19zX3viZjHY/92+GPo9xNsBtbBUL7/4uvO3zoV8zeRuB1SiTjSv/H9z0cKJXoUxQVGwoSqzYtgzgMkO4koGOOnEFzKTTBRfD6R3Bk1S9bH9Q8ixOOomdxtnINmIjSmcjI0+6caakyflA5p9svU9COdGIhxxTaW+5ORjxwoiNjDBhlMlCapqMo1eUYaBiQ1FipbdLbszJJDZanQkD5sa68GLAhqOV4d+z/xnZGnfCOD3ZxVKa2h5FB8umY1A8X0IU09e6eRu7fgetp8RpiQbjbJQujL8oUGdDUUaMig1FiRUjMnpaE7uOeGIqQsx8jxnrILMwfCil5ZSUoYKbd9HZKO9JTYve2Wg8CsVOmeqcc2X2xrafwuOfkSoPU1o6FCZnI5ZmXtFi3J5kL21VlFFExYaixIoRG91JJDYCnY2UVJh/IRyuDN3Z88AfZJtd4o5Z72qEHGcUeV659N3oaQ//mQMD0sirZL48n3Oe9Ol48u+k7fctT0RfwpqWKYPXVr4nuuNjYcY6qUjRkk9FGTZajaIosWIcjZ42uREnQ3Z+a7XkTOSWufsWXgz7npRE0MAW1fufkfDH9DWuw9HZ6IYz8pxx6x21kJkX+jPbz0Bft0xoBREYueWw7Bq48pux9fgAeN/PYjs+WqYsgjsOjs65FWWSoGJDUWLFOBt2v+RvJIO93nZa2m2npLr7Fjhlr0de8Bcbvg44+iKc8wlxFPY9Cf19jrPhJGqaduLtteFLSk2uhxEbOSXwDweSQ7wpiuKHhlEUJVa8iaHJkiTaWg0F0/33lSyAgllw/GX//YdfgP4eGYZWPF+6Z7ae8u+RYZyNSOWvJtejeL67T4WGoiQlKjYUJVaSUWy0nXaTQw2WJd06mwIGpB14RhJB557vuhJNR0VsZAeKjQhJok3HpElU4ezwxyiKkhSo2FCUWPETGy2JW0e0tFZDb/cQx5z2HzpmKJwt49W9nNwqyaOp6W5yZ/1B8LW5zkZOqYyaj+RsNB4V50R7NyhK0qNiQ1FixVvyOt6djT4ffP9cePk74Y/pbhWhEOhsgMx+aT/jipWBAXE6TA5HwUwZkHbqDXme7VSjpKRKsulQYZSSebFekaIoExAVG4oyMBDbIK2RhlH++h2o/Gbs7wukq1mmkUaidre4L9VvhT/G9Ngw/SS8mBBH6ynZttdIvoYZ4Z6SKn0yqh2xYZwNcBp7DRFGMWEYRVGSGhUbirL91/CtFeICRMNIxcaex2Dnb2N/n5e2M3DPcqbVPB/5uOo3ZVu/P/wxgz02QjkbjthoPiHbwaTOee4xxfOgzjl/tkdsFM6Gml1SqRJIT7uUxarYUJRJgYoNRWk8LIPVOqLoeAkiMDIL3Mex0lHvugnDZe/vobeDouZdkY8zYqPxaHgxFdg91ItxNkzeRkixMR9wGn95nY21H4LWKllrICedtuRlSyOvX1GUpEDFhqIYwRDNlFJzvEmmHE4X0c4G6a45kg6kex4HIL/tUOTjqt+Uig+7X0RVKAK7h3opmCHvb/aIjcAKkhJP6arX2VhyJZQslHH1gV1IX39AkkjPuizy+hVFSQpUbCjKoNiIwdnIKYW0rNjno/R2idCA4bsb7XVw/K+QWUhOZ1X4luC93VC7123OVRcmlNJ2GrKKID07+LXUdMif4e9sBFaQeF0Or7ORkiqD1Krf8O/V0VYD+54W5yMtc6irVRQlCVCxoSgxOxutkJkvf2INo3TUu4+NoxAre38viaEX3I6FDTU7Qx93Zrc03Fr9PnlefyD0ca3VoV0NQ9Fsf2fDDE4zmKZcqZnBk1HXfkiE2cvfdfe9+XNxWtZ/LPxnKoqSVKjYUJThOBvDFht17uPhOht7HpehYGtvkucmLyMQUyEy9wIonBPe2RhKbBTOhhZPgmiQ2Jgn25yS4A6g6dmw8VZpBLb/GRjoh9d/BvPfHjxvRVGUpEXFhqIMJ2djuGKjs8F9PBxno6Mejm2G5ddBwXR6MkrgdJiy1uq3ZFZJ4SwoWxy+IiVU91AvRbNlpHxPm3xHgRUkGTkyV8Wbr+Fl462SCPrrD8BPrxLhoq6GokwqVGwoyrDFRsHYOxv7npQQyvJr5RT5Z0VwNt6U8eiWBVOWQP0h6Snipb9XHJ2hnA27H05skefeWSaGaSvDl7HmlMD/eREq/lHcltwyWPquyNepKEpSoVNfFWVQbNRFPg6gr0eaWhmx0Xx86Pd4MTkbhXOkRXisHPmLJGhOWwVAW/5Cphx70BVABl8n1O2FpVfL87LF0NclroJXFLSfAeyhnQ2AYy/KNpSouPF+IMIQtbRMqLgL1rxf+m5oi3JFmVSos6EopjokGmfDVH5kFjhhlBirUTrqJJFyyiJoG0YYpWYnTF8zmBvRln8WYMPpHcHH2QPibIA4GwB1AUmiRvBEdDbmyPboZtmGEhtZhZBVMPT6i+fJcDdFUSYVKjaUyc1Av0dsRJEgasTFSHI2csukW2eszoavU3plOK4GGLFBcCilaqtsZ6yVbZkjNkzexsAAtFRJ/gcMITZmyfb0W5CRJ9UliqIoMaBhFGVyY4RGbrl0EO1ph8y88McbcTHobLRJwyrLgh2/kSTJRREaVXXUQW6p9K7oqJWQQmqU/xnW7hW3YtrKwV29GUUSVvGKDV8nvPIDmHG2Gx7JKZFk0br90k3059fLWHgQp6VoTvjPzciR93bWizMRWHGiKIoyBCo2lMmNEQ+lZ8nNv/1MlGIjX8IGA33Q1y0lns//qwwoiyg26uXGXTBDhEP7GSgMMQAtFGecfhpTV/rvn7EWql6TduRpGfDK9yREc+NP/IVB2RI48Sr87zVyHVf9p5Sfli+XMEgkimaL2CiaG/k4RVGUEGgYRZncmBwM0/NhqFCKV2yYhMzuVnEoWk7J+PVIdNY7YRQnbBFLRUrNLsjID77hr7heElUf/JC4Fi99G5ZdA3PP9z9uymJoOCjXcPPvYeOnYOElkD9t6M827cl1cJqiKMNAnQ1lYnP4eayB/uG/3+tswNBJon5hFM8wtn6flIe2noocGumoh9wpbngjll4bNTth6gpICfiNsOpGCQc98XfwwwtkLZf9S/D7578dDv4RPvhrSTKNBRNmUbGhKMowUGdDmbg0HYefX0957ebhn8MkfEbtbAQkiJp9ZnaI3S+TTkPh64DeThEbkZyNwy9I/wsvAwPSfnzayuDjAdbfIuWn/T44929Cd+dceQN8fnfsQgPU2VAUZUSo2FAmLk6DrKzuKPpjhMM4FUVzwUodesx8qDBKTxs0n3CPCRdKMT02cqZIRUdqhjghXur2w8+vk6moXpqPg68tOF/Dy8ob4O/3h3Y1DMNN7px1DmQX+1XCKIqiRIuKDWXi0tUEQIavafjnMNUo2UXiOAwZRmkVUZKe7R9GMYPKwL/RV2u1TDkFydcAydmwLMmVCCx/bTom272/999/Zpdsp62OvL7c0uAwSzyYtR7uOiYlu4qiKDGiYkOZuAyKjcbhn8PrVOSVR5cgmpkvYsEvjHJCyl6tVH9n4zc3wyOfksfG2cidItv8GcFhlBYnBHPsr9Dpua6aXWClQPmy2K9RURQlwajYUCYuzs04s2cEzoYRGxn5kDc1ugRR42gEOhsl86Fgputs9PdKI6yTr8njQLFRMD04QdSEVex+OPAHd3/NTihZKD0vFEVRJhgqNpSJS1ycjVZIy5bqkbyp0Tsb4O9sNJ+QJMriua6zUbdfEjb7uiS50wxhywlwNmzbPX/LKWnSVTAT9j3l7j+zM3xyqKIoyjhHxYYycRkUG83+N+xY6Gl3RYMJowRORvU7vtU9Pi0D0rKgu0XCH0WzJdHUOBs1O933Vb0mORtpWZCRK/sKpkt1SneLe1zrKWkPvvRqOPScdAOtPyRiJlJyqKIoyjhGxYYycXHERord55/fEAtepyJvKgz0QndzdMeDPG48KpNgi+aKs9F+Bnq7oGaHuCa55SI2Ohrc5FBwe2148zZaqqSj6NJ3iSOy5YfwwNWSD7Li+uFdo6IoSoJRsaFMXLo8AqO9Znjn8BMb5c65IoRSQomNM7vlceFst7tn80m3CdfsjY7YqPMfYlbgtCkfzNOwJYejYIZ0/8wqgue+Km3NP/Z06N4ZiqIoEwAVG8rEpatJppCCW14aK4HOBkROEg0lNsxAsyInZwOkhLVmB0xfDbM2QOMRmbiaW+a+14iH+kOy7agXh6RgFqSmw5oPynj3jz2jVSiKokxoVGwoE5euJnd0+lBVJOHwhRIbsTgbBe5jr7NxbLPkYkxbJQ2xQPIuTCUKiPDILoHaPfLcdB41g9ku/zp8bjtMOWt416YoijJOULGhTFy6mqDM+cUfF2fDhFHCCJf+Pkno9AoM8zirUKbA5k2Vke37npT909bAjHXSfwP8xYZliWNRt0+etzjhFBNeSUkdnQZdiqIoY4z+n0yZmAz0Q1czFMygLzUnPmIjs0ASOpvDtBv3eRqAGcxjM6gsJUUeNx5xm3Bl5EruBrhlr4aypVC7z8nXcMRG4azhXYuiKMo4RcWGMjHpbgFsyC7Gl1E8sgRRk/dhWTIZdf8zoUtpeyKIjcI57j6TtzFlsduEy4RSvDkbIGKkp0UqUlqqZF5KoCBRFEWZ4KjYUCYmTtkrOSX0ZBZD2zByNvp6pOmWVzysuF4muFZtCz4+ktgomu3uM3kb3qFlg2IjQEiYxM/aPeJsFMzQ0ImiKEmH/l9NmZgYsZFdjC+jZHjORo8zhM2bg7H0KnEXdj8a4vhIzoZHbBhnwzs0bcmVsOHjMOc8/3OanJPafU7Zq4ZQFEVJqvSGHgAAIABJREFUPlRsKBOTQLHRdib2LqI9rbL1ioesQjjrMhEbgZ1EB8WGR5xkOY+9zkbxfNlOX+Puyy6Cd33LPd6QWyqhlbq9kiBqKlEURVGSCBUbysRkUGw4YZS+Llc8GPY9BY9/OrwIGRQPef77V9wAbdVQtTXg+FDipEi2RZ6cjcVXwA0/hnkXRnctZUulMVhbtVuJoiiKkkREJTYsy7rCsqz9lmUdsizr7jDHvM+yrD2WZe22LOtX8V2mogTg52wUy+PAvI29T8Cbv3BLSwPxmTBKvv/+JVfIDJNdj/jvP7MHsPyTPJdcCdf8N0xf6+5Ly4DV74s+96J8GZzeDgN96mwoipKUDPl/Q8uyUoHvA1cCy4EPWpa1POCYRcD/BS6wbXsF8HejsFZFcTGzULIKXbERmLdhSkn3/D70OULlYJjni94Bex6T3hoAfT5442ew6J0S+jBk5ML6W9x5J8OhfJm0JAd1NhRFSUqi+em1EThk2/YR27Z9wIPAtQHHfAr4vm3bTQC2bQ8xp1tRRkhXE2QWQmqa5GxAcK+N1mrZ7h1KbBQEv7b2w9Lc6/WfuufoqIWNnxr52gMp87QiV7GhKEoSEo3YmAmc9DyvcvZ5WQwstizrr5ZlvWpZ1hXxWqCihKSrCXLE0ejJNGEUj9gwQ80yC+DMLmg4HHyOUDkYhsWXw/yL4PmvybTW134CxfNg4aXxvQ6A8qXuY23opShKEpIWx/MsAiqAWcCLlmWtsm3bb1a3ZVm3ArcCTJ06lcrKyjh9vNDe3h73c04UJtu1rzp1mPS+NN6orKS9a4D+lAyq927jcG8lAGm97bytt5OTs65hdtUTHHnyW5yYe6PfOWaf2M5CYPOWN+lPyw76jJwpN3LO0c00//g6ipt3cHjBLZx88cVRuZ7zMopJ6+tg85btMYVkJtvfuxe99spELyMh6LVXJnoZwyIasXEK8NT1McvZ56UK2GLbdi9w1LKsA4j4eM17kG3b9wH3AWzYsMGuqKgY5rJDU1lZSbzPOVGYdNd+8KtQPIeKigoqKytJLZzB7KJ0Zpvv4Mxu+CvMPu898HI1C3p2saDie/7neP6vcMTiwkuvCH+DT9lD8ZYfQloWC2/8MgtzSkbnek6eDS1VVFx8cUxvm3R/7x702isSvYyEoNdekehlDItowiivAYssy5pvWVYG8AEgMAj+GOJqYFnWFCSsciSO61QUf7qaILvYfZ43zX+AmsnXKJgJy94N1W9CU8DMk542CbNEchIq7pZzr34/jJbQALjyP+CG+0bv/IqiKAlkSLFh23Yf8BngWWAv8BvbtndblvVVy7Le7Rz2LNBgWdYe4AXgDtu2G0Zr0ZH4z2f3869P7knERytjSVeTjGc35E+T+SIGU4lSMAOWO/9M9z/tf47AcfGhyC6Cz7wGV/3nyNcciSlnwYy1Qx+nKIoyAYkqZ8O27aeBpwP2fcnz2Aa+4PxJKHtOt1LX1pPoZSijycCATHz1OhtFs2WA2kC/jGZvrZapq3lTITVdmm6deAXO/Rv3Pb624IZeoQjs+qkoiqLERNJ1EM3JSKWjpy/Ry1C89LTL0LN40d2Mmfg6SNky6O+BxqPyvPWUKzQAZp8LJ7b4dxONxtlQFEVRRkzSiY28zDQ6fCo2xhU/uxaeuTP0a52N8MK/g68z+vN5Jr4OYspH6/bKtrVaQiiG2Rul6VfzCXefig1FUZQxIenERk5GGh09/YlehmLoaoJT26DuQPBrtg1P/h385Ruw78kYzulUVHudjSlLZFsbRmzMOVe2J7e4+1RsKIqijAlJJzbyMlPp8PVhxzoBVBkdql6XbUeIprI7H4Y9j8vjI5WRz9PRAH/5D+io95uLMkhmnuRl+IkNT++58uWQkRcgNtohQ8WGoijKaBOvpl7jhpzMNGwbunr7yclIusubeJibe3ud//6WU/D038PsTZBXDodfEKcjVBnq6R3w4E3QckJyMhY6vSi8YgMkb6NuH3S3SndQr7ORkgqzNkjehkGdDUVRlDEh6ZyN3EwRGBpKGScYsdHTAr3d7v5n/xH6e+G6H8JZl8l49foQoZbdj8JP3gl2Pyy/Drb/Go7+RV7LDuh7Ub4M6g+6eRn5M/xfn30u1O4WMdJ0XARJoGBRFEVR4k7yiY2MVACtSBlLjm6G72+SsISXgX449bobqujwuBvVb8p49tKFsKBC9gWGUt78Bfz2YzB9DdxaKaPcs4tlP0BWof/x5ctgoBeOvSTPCwLFxkaZrnpqG/zhbkjPgXU3DeuSFUVRlOhJPrFhnA2tSBk79jwm4Yu6/f77a/eAr13GtYObt2HbMjTNiIHieVA8X0Iphm33w+OflpDJRx6VUEt2EVx0l7zuTHz1o8ypSDn8nGwDxcascwALKr8hDb4q7tLBZ4qiKGNA8omNDA2jjDkmD6LpqP9+E0JZerVsTd5GZ6P0xPAmcC6oEEeivxe2PwRPfh4WXQ4f+DVk5LjHbfg4lCyA3NLgdUxZDFius5E/3f/1rAKYukLWVbYUzv3bYVysoiiKEitJJzZyMp0wijobY0N3q+RBADQGjMM5+RrkljuOAq6z0ebMLfGKgYUXS0fPl78Lv/8MzLsQ3v8LSM/yP2daBnzwIbj2B8FrycgRl6S3E3KmBL8XJCEVpP24afilKIqijCpJV66R54RROtXZGBtObZM8CHC7dxpObpE8ibxyed7uiI1WZ4aJN8wx70LAguf+BUoXwft/LsIiFGWLw6+nfJk4LIEhFMOFfw9nXQrzL4x4WYqiKEr8SD5nQxNEx5aTW2UGybRV/mGU9jp5PnsjpGfLdFWTIBrK2cgpkdLU7BL40EPDrxIxeRveEI2XwpluWEdRFEUZE5LW2dAwyhhx4lUoXwHT1sChP7v7q7bKdtZG2eaWeZyNasCSSa1ebvyplLgWzxv+esqXyTacs6EoiqKMOUnobJgEURUbI2bX7+C+CugP810O9EPVNnEvSubJ7BFfh7x26g2wUqVsFSSUYpyN1moRH4E5E0WzRyY0QMWGoijKOCTpxEZGWgoZqSl0+DRnY8Rsf1D6YYRqtgVOaWubzB0pni/7mo7JtvoNp0W4U0nidTbaTo+eGChbBmd/VEMliqIo44ikExsgFSnqbIyQPh8c+6s8rn4z9DEnXpXt7E1SjgqSJGrb8p4Za91j88qh/Yw8bh1FsZGaBu/+rutwKIqiKAknKcVGrk5+HTlVr0GvExI5/VboY05uhbxpMgCtxDgbR6H5uAxLm3m2e2xuOXQ3i4hpqw7ugaEoiqIkLUmXIAqQq87GyDnyguRclC8P7WzYNpx4BeZskuFp2cWQVSS9Nk69IcfM8IiNvDLZtpwUIVKgYkNRFGWykJzORmaaVqOMlMMvwMz1MP/tULMzOEn0zG4RDgsvdfeVzJcwSvWbkJohQsWQ6/TaOL1dtoFD0hRFUZSkJTnFRkaaOhsjoatZEjwXVEjeRV+3zD7xsvcJ6a+x5Cp3X8kCCaNUvyl9N7xNufICxIY6G4qiKJOG5BQbmal0ajXK8Dm2WbqCLrwYZqyTfYF5G3ufgDnnueERkIqU5pNQ/Zb7PkOuc9yg2AjTdEtRFEVJOpJTbGSk0a7OxvA5UgnpuTBzA5QslBHx3ryNhsMyD2XZNf7vK5kvTbl8bf75GhDsbGiCqKIoyqQhKRNEc9TZGBmHX4B5b3PDINPXiFth2PuEbJe+y/99ptcGBDsbGbkiYLoaISNPJrAqiqIok4LkdDYy1dkYZKAferuiP76jHhoPi9gwzFjrJIn2yvO9T4iYKJrt/17TayM9B8qWBJ/bhFzU1VAURZlUJKfYyEjD1zdAb/9AopeSeP74RfjBudELjpqdsjVtxkGERX+PJIm2nJJJr4EhFJBZJ2nZ8t6U1ODXTUWKJocqiqJMKpIyjJLrGTNfmJOUeip6Tr4qLcS3/Aje9ndDH2/ExrRV7r7pTifQJ/4OWk/J46UhxIZlwTmf8H+vF5O3oWWviqIok4rkFBtmzLyvj8Kc9CGOTmIGBqDWKVl96R6ZGZJTEvk9NTulUsR7XMkC6RLadAzmXwgrboCyxaHff/nXw587T50NRVGUyUhyio1MnfwKSNOt3g7Y+H9g630iON75tcjvqdkZ7EykpMDtbwGWPB4ug2EULXtVFEWZTCRljCE30zgbk7wipXavbFe+B9Z+CLbcJ30wwtHbJRNeQ4VBUlJHJjRAE0QVRVEmKckpNjImmbNx6g1pwhVI7R7Zli+Fi+6UJE9TthqK2r3SJyNczsVIKZor25L5kY9TFEVRkorkFBuTKYxS/Sb8+GKm1G8Nfq1un4QssgrlRp9ZIO3Ew3Fml2xHS2ycdRncWglTV4zO+RVFUZRxSVKLjUnR2OvkawDktR8Ofq12D5Qvk8eWBcXzZFCaoc8H91XAnsflec1O6RZaNG901mpZwc2+FEVRlKQnOcWGU40yKRp7OTNLcjsCcjH6+6DugCs2QMIXXmej4ZA4I8/+M/T1iNiYumLkuRmKoiiK4iEp7yquszEJxIYzsyS347j//qajkqPhHfNePB+aT0hXUZBkUICWE/D6/0LNrtELoSiKoiiTlqQUG9npxtlI8jCKr0PyMtKyye6qgd5u9zVTiVK21N1XPA/6fdBaLc+N2Ji5Hp77qgxQU7GhKIqixJmkFBspKRY5Gal0JnsYpWanVKEsuwaLAVc8gCM2LP8ZJaYKxIRS6g9A4Rx4x7+K0AAVG4qiKErcSUqxARJK6Uj2MIqZxLr2g7Kt2+e+VrtHnIyMXHefmcra6BEbUxbBvAtgwcWQkuaf46EoiqIocSB5xUZGKh2hwigtp8C2x35B0fKnL8NL347u2Oo3IW8azH0bA1aa21cDxNkIFA6Fs0RQNB2VVub1B2GK03b8+nvhpt9CenZ8rkNRFEVRHJJXbGSmBffZaD4J314Fh/6cmEVFw46HYNtP/Pf197nj3b1UvymlpGkZdGXPcOeg9PXImPhAsZGS6s44aauG3k53xkn+NFh4SdwvR1EURVGSV2xkhAijNB2VDpmNRxKzqKHwdUDbaakYaTnl7n/ow/LHS0+bhEFmyETWjtw5rrNx/GUY6INpq4M/o3i+hFHq9svzKWEGqimKoihKnEhesZEZIozSXivbjvqxX1A0NB1zH594RbbdrXDoT3DgWWircV+v2QnYg02yOnLnQPNxESxb74OcUlh8RfBnmF4b9QfluYoNRVEUZZRJWrGREypB1NysO8ep2GjwdAE98apsj74oLgW22+kTBvtrMN3jbICIkv3PwPqPQXpW8GcUz4fuFji5RdqY55bF/zoURVEUxUPSio28jBA5G+1nZNtRN/YLigYT3pm53hUbh/4sLcSnLIFdj7jHnnhV5p7kTwWgI9cZcvbHL4KVAud8IvRnFM+T7eHnxNWwrPhfh6IoiqJ4SFqxkZOZSmfYMErD2C8oGhqPQM4UWHS5DEXrahaxseAiWP1eOPkqtFRB1TaZ3rri+sG3dmVPhbQsaK2C5ddCwYzQn2F6bXS3aAhFURRFGROSVmzkOWEU21vm2j6KYRTbht6ukZ2j8QiULIA55wI2vPkLaDkp01JX3CDH7PodPPl5qR6puNt9r5XqiodNt4X/DONsgIoNRVEUZUxIWrGRk5HGgA3dvQPuztFMEH3rl3DPspEJjsajIjZmbZB+GC/dI/vPuhRKF8L0NfDCv0HNDrji3yEz3//9i94JCy+F2RvDf0ZGLuRJ6EXFhqIoijIWJK3YyMsMMfnV5Gx0NUnvinhydLOc11tREgu9XRICKVkggmD6GuhskFyNIif5c+V7oK9b+mEsvy74HJd+ET7yyNB5GMbdULGhKIqijAHJJzZe/E947l/JyQiY/Nrnk5t3bhlgQ1djfD+3Zqdshys2mpypraULZTvnPNkueod7zJoPwtJ3wdX3jCyxs3g+pKRD8dzhn0NRFEVRoiQqsWFZ1hWWZe23LOuQZVl3RzjuPZZl2ZZlbYjfEmOkdg9sf5DcQGfDVKCYkevxDKX0dkO90yQrFrFx4I/ufBNTiWISOOe/XbaLL3ePzyuHD/zSPWa4nPe3cM23ITV9ZOdRFEVRlCgYUmxYlpUKfB+4ElgOfNCyrOUhjssHPgdsifciY2LWOdBaRXG/iIlOn1ORYkIoU1fKNp5JonX7nF4YRC826g/Cgx+Cxz8jyaWDYmOBbBe9Ez71/9u77/A4qzvR498zvWikUe/VluReZGEDxlgGHBxqGgkpEBIIaSTLlptsNje7++Tm3huyy+YS2M3GCyQkZKnBWW/oTdgUG9fgbsuy1Wz1OhqVKef+8Y5lyZKwsDUeS/p9nodHmvO+887vzBk0P5/2vnk66ZhMmYth6ZfOfp4QQggxCSbSs7EcqNJaV2utB4EngZvHOO9/AfcB/ZMY30eXcwkAKZ3GsIavP5IEDCUb842fk9mzcWoIxR5/ejjkw2gNL34fwgFo2gMndxvJhjPR+A+MYZLsssmLUQghhIiRiSQb2UDdsMf1kbIhSqkyIFdr/fwkxnZuMhaB2U5m9wcAHG3xGeVDyUakU8Y/iXttNO0FqxsKrphYz8ahF4xNtVb/rbE3xs7fnV72KoQQQkwzlvO9gFLKBPwLcMcEzr0buBsgPT2dysrK8335EXw+H5Vvv8tSdwEcqcRrv5LXdx1mdqiW/ONbKQQ2HWhmFYqafds47i+elNddcnAzyplLd6+FrLajbH7zTaNnQmuUDqJNp+dGmEIDXLLtXsKuPLbrS5iTtILkXU8SMtvp9C7gwDm+Jz6fb9Lfz6lC6l4Z6zBiQupeGeswYkLqXhnrMM7JRJKNBiB32OOcSNkpHmABUKmMFRIZwEal1E1a6+3DL6S1Xg+sBygvL9cVFRXnHvkYKisrqaiogIG1sO1hlhd4qe4IUFGxGv60EZqSuPLqa2F7IgWpcRRMxutrDe/VwaLPkpBSCvUbqSifZ2wjvvc52PhduGcbxGca529dD/3NcPtGVhethkIbPHYjllAv6XMvI/0cYxqq+wwkda+IdRgxIXWviHUYMSF1r4h1GOdkIsMo24BipVShUsoG3ApsPHVQa92ltU7RWhdorQuALcCoROOCyimHYD8VCU0cbfEZy199Tac3s3KnTN4E0c4aGOg2Jp6e2r/i1FDKkVdgsMfY8OuUXb8zJmgWrTYe519x+nkyjCKEEGIaOmuyobUOAvcALwMHgKe11vuUUj9WSt0U7QDPSWSSaJm5Cq3hwMnuSLKRZhx3p07e/VFOTQ7NWDQ62aiLLMzZ9TsIh+HkB8bun0uGrQQxmU6vDJFkQwghxDQ0oTkbWusXgBfOKPv7cc6tOP+wzlNCDniyyPfvA+axp76LZb6m0xtluZKh5dDkvFbjXuMuq2lzjZ9gJBu+ZmPSZ+YSY7XJ8c3GxFCzDRZ+ZuQ1VnwTnEmQHbvtSYQQQohomX47iJ6SU46jaScpcTb2NnQZX/5DPRvDhlEC/fDMHad7KD6qxj2QXAw2F1gd4Mkyko1TvRprfwwOL2z7D/jgKWMHUFfSyGvY44xbwpumb3MIIYSYuabvt1vuclRnDZelhzlWf8K4p8ipORuuFPC3QzhkJAX7NsBb9539muHQ6LLGPZCx4PTjxILTyYbZZtzBddHnjFvC93XIZlpCCCFmnGmcbKwA4OPOfXS3RhbPDJ8gija+/OveN8oOPg+ddaOv098N7/wCfnMD/CQNtj96+lhXPXTVQtawzbdOJRu1WyFrKVjsUHa7cSw+G4oqJq+OQgghxBQwfZON7HJIKeWKlqdIocMoG5FsYOwiWrcFPJFlqTt+Pfo6L/wNvPojoyfElQJ7nj19rOp14+fsa06XJRZAzwk4sWso4SFjASy9DVZ/H0zmSauiEEIIMRVM32TDZILLv0N810E+YXrHKBs+jALGCpW6bcbNzkqvgx2/MeZwnOJrMYZYlt8N33rXGAKp3WIkHgBVrxm9Famlp59zakVKOHA62QC4+SFY9uVo1FQIIYS4qE3fZANg0WfRcRncYnnLeDx8gihAzbsw0AW5l8LyrxlbmO/bcPr5u34HoUEov9N4XLIOdAiOvgGhIFS/BbOvHnm791PJBoxMNoQQQogZanonGxY7asXXMaEZxHL6JmenejYORm7lkrscCldDSgm8+yAM+o3JoDt+DQWrIG2OcV72MuO5h16Ehu1GojLr6pGvOXyDrrjUqFdRCCGEuNhN72QDoPyrDJpdNGsv9Z19Rpkr2fjZtMfY4CupyOiduPofoHm/cev3g3+Czloo/+rpa5lMxpBL1atw+CVQ5tETPuPSwOaBvMsvRO2EEEKIi970TzacXrqv+BFPBSt4ZV/kzq/mYb0cuStOD4PMvcGYW1H9Jjx7pzHHY84NI69Xci30d8H7Dxvboju9I48rBbf/Ea4ec88zIYQQYsaZ/skGkLLmW7yccjsv72s8XXhqKOXMeRVLvwQ3/NyY4LnsK2CxjTxetAZMVuOeJ8NXoQyXU27ciE0IIYQQMyPZALh2fgbbjrfT5hswCtzjJBtgDJ3cswNWf2/0MUc8FKw0fj9zvoYQQgghRplRyUZYw+sHmo0CV7Kxw2fWkrGfkDJ7/D0xLrkLZl01/nOFEEIIMWTGJBvzs+LJ9jp5ZX9kKGXpbbDmh8YOnx/V3Bvhtg2yQZcQQggxATMm2VBK8bH56Ww60krvQBBK18EV98Y6LCGEEGLamzHJBhhDKYPB8MiJokIIIYSIqhmVbCwvSKIkPY5/qzxKOKxjHY4QQggxI8yoZMNkUnznqmKqmn28uFd6N4QQQogLYUYlGwDXLcxkVqqbB984Ir0bQgghxAUw45INs0lxz1WzOdjYwyv7m2IdjhBCCDHtzbhkA+DGRVkUJLt44PUjhKR3QwghhIiqGZlsWMwm/nJtCQdOdvOHnfWxDkcIIYSY1mZksgFw0+IsluZ5+aeXDxn7bgghhBAiKmZssqGU4kc3zKOlZ4B/f+torMMRQgghpq0Zm2wAlOUlctPiLNZvqqa+wx/rcIQQQohpaUYnGwDf//gczCbFvU/uJhAKxzocIYQQYtqZ8clGttfJTz+9iO01Hdz34sFYhyOEEEJMOzM+2QBjsuiXL8vn4beP8eKek7EORwghhJhWJNmI+OH181iS6+V/PPsB1S2+WIcjhBBCTBuSbETYLCb+7Ytl2Cwmvvn4TvyDshxWCCGEmAySbAyT5XXywK1LONzcw//csBetZXdRIYQQ4nxJsnGGVcWp3Ht1Cc/tauD+Vw5LwiGEEEKcJ0usA7gYfeeq2TR0+nnozSqOtfVy/y2LcVjNsQ5LCCGEmJIk2RiDyaS479OLmJUax09fOkhDRx+/vXM58Q5rrEMTQgghphwZRhmHUoqvr57FL7+4jH0nuvjKr7fJPVSEEEKIcyDJxlmsW5DBg59fyu66Tu58bBt9g6FYhySEEEJMKZJsTMC6BZn8y2cXs/VYO7c9spWO3sFYhySEEEJMGZJsTNDNS7L51y+U8UFDF5/+5bvUtsmN24QQQoiJkGTjI7huYSa/v2sF7f5Brv/FZn783/s51tob67CEEEKIi5okGx/RJQVJbPjWSlaXpvLb946z5p8r+cFze+gPyFwOIYQQYiyy9PUcFKa4eegLZTR397N+UzUPv32MnTUd/OsXlzI7zRPr8IQQQoiLivRsnIe0eAf/84Z5PPbV5bT6Brjhwbf51VtHCYTCsQ5NCCGEuGhIsjEJVpek8sJfrOLK4lT+74sHuemhd9ha3RbrsIQQQoiLgiQbkyQ93sH628v51W3L6Ogd5HPrt/Clh7eyo6Y91qEJIYQQMSVzNibZtfMzuLI4ld9vreGXlUf59C/fY2mel6+sLGTd/AxsFsnvhBBCzCySbESB02bmrlVFfGFFHk9vq+Ox92r47hO7SHRZuX5RJp9cmsOy/MRYhymEEEJcEJJsRJHLZuGOlYXcflkBbx1p4bmdDTy7o57Ht9RyZUkqP/j4HOZmxsc6TCGEECKqJNm4AEwmxZrSNNaUptE7EOSJ92t58I0qrvvFZq4qTeOqucaxLK8z1qEKIYQQk06SjQvMbbdw16oiblmWy79vOsp///kErx9sBiAn0Ul5fiLXL8rimrlpKKViHK0QQghx/iaUbCil1gEPAGbgYa31T884/lfAXUAQaAG+qrWumeRYp5UEl5Xvr5vD964tparZx6YjrWw/3s7bVW38cfcJLp+VzN/fOI85GTLMIoQQYmo7a7KhlDID/wqsBeqBbUqpjVrr/cNO2wWUa639SqlvAj8DPheNgKcbpRTF6R6K0z3ceUUhwVCYJ96v5f5XD3PdA5v5VsVsvnt1saxiEUIIMWVNpGdjOVClta4GUEo9CdwMDCUbWus3h52/BfjSZAY5k1jMJm67rIAbF2fxk+cP8NCbVbxxsJnbL8unpz9IXyDEytnJlOUlyjCLEEKIKUFprT/8BKU+A6zTWt8VeXwbsEJrfc845z8ENGqtfzLGsbuBuwHS09OXPfnkk+cZ/kg+n4+4uLhJvWas7WwK8pt9A3QPjixPdiguyTCzONVCcaKJfn/vtKv7RE3Hdp8oqbvUfaaRul+8dV+zZs0OrXX5WMcmdYKoUupLQDmweqzjWuv1wHqA8vJyXVFRMZkvT2VlJZN9zVirAL4+EKTVN0Ci24YCXt3fxMY/n+D1qlZeOh4kzm4hy2VmWXEy5fmJ3LA4E7vFHOPIL5zp2O4TJXWviHUYMSF1r4h1GDExles+kWSjAcgd9jgnUjaCUuoa4IfAaq31wOSEJ8BYweK2n26qT5Xl8KmyHHwDQd4+0srbVS28f6iel/ae5In3a7nvpYPcsbKA8vwkshOdZMQ7MJtkyEUIIURsTCTZ2AYUK6UKMZKMW4EvDD9BKbUU+BXGcEvzpEcpxhRnt7BuQQbrFmRQWdnG6tWr2XyklfWbqvnZS4eGzjObFBnxDnKjgLH9AAAV5ElEQVSTnFy/MJNbynNxWGdOz4cQQojYOmuyobUOKqXuAV7GWPr6qNZ6n1Lqx8B2rfVG4J+AOOCZyKTFWq31TVGMW4xBKcWVJalcWZJKbZufY229NHT0Ud/hp6Gzj0ONPfzov/bx/147whXFKdR39NHQ0ceaOan85doS0jyOWFdBCCHENDShORta6xeAF84o+/thv18zyXGJ85SX7CIv2TWiTGvN+8fa+dWmat4/1k5ekouleV6e2V7Pf+0+wecuySU/yUVavIPLipJJdNtiFL0QQojpRHYQnUGUUqwoSmZFUfKI8mOtvfzspYP89r0aQmFjdZLLZubzy/O4aXEW9R19HGv1sao4lcW53liELoQQYgqTZENQmOLml19aRjis6fAPUtPu5/H3avjNu8d55O1jQ+f9/LUj3Ht1Md9aM5v+QIg/13dit5gpzfAQZ5ePkhBCiLHJN4QYYjIpkuPsJMfZKctL5C/XlrCrrpOiFDdp8XZ+8qcD3P/qYX6/tZYW38BQLwhAQbKLGxdn8emyHApS3DGshRBCiIuNJBtiXLlJLnKTTs/7eODWJVSUpvL8ByeZlxVPWX4iwZDmcFMPW6rbeOjNKh58o4o0j50kt43kOBuJLhvJbhuJbuNnSpydkgwPBcluWY4rhBAzhCQbYsKUUkN7fAy3dl46314zm5NdfWzcfYKjLT7aewO09w5worObNt8A3f3BEc9xWs1keh24bRa8LivrFmRw85JsGY4RQohpSP6yi0mTmeDk66tnjXksEArT6Q/Q1N3PgZPd7D/ZTXPPAP6BIPUdffxww17+z/MHKM3w0OkP0DMQJMvrZFaKm4wEB06rGZfdQlGKm5IMD1kJDrk3jBBCTBGSbIgLwmo2keqxk+qxsyA7YcQxrTW76jp5YmstDZ19zMty4rZZqO/08151Gy09AwTDI+/hY7eYSI93kJHgYJ4rwMpQGKvZhNaaxu5+0jyya6oQQlwsJNkQMaeUoiwvkbK8xHHPCYTC+PqDHG3xcbCxh9p2P03d/Rxq7OE3xwZ57Z8rWVGYzJbqNho6+8hKcPD55XmsKErmUGM3Bxp7cNvMZHud5Ke4WZLjlX1EhBDiApFkQ0wJVrOJRLeNcncS5QVJQ+Vaax589nXeaLbz6v5GLp+Vwh2XF7DpSAv3v3p46Lx4h4WBYJiBYHiorCjFzZI8L2V5iSS5bWw63MLbVa0Up8XxjdWzWF6YNGqoZiAYoqlrgKaeflLj7OQnu2Q4RwghzkKSDTGlKaVYlGrhu7esHFH+tSuLONbay9FmH3Oz4slKMLZib+sd5EiTj111Heys6eStQy08t9O4r6DHbmFFUTK7ajv43PotzE6LI9FlxWIy0dVnzDdp6x0c8TrZXieXFCTiddlw2cwUpcaxuiSVVI99xHn9gRD7TnQxPytB7ksjhJhxJNkQ01ZhipvCM/b8SImzkxJn57JZxi6qWmvq2vto8Q2wKCcBq9lEfyDEM9vreO1AM4PBMMFwmIwEB0vyvGTGO0hPcJAe76C23c87R1rZeqwd30CQvsHQ0NySBdnxzMuMZ3ZaHNUtvTz/wUl6BoIkOK18YkkWK2enYLeasVtMJLttpHkcxDstQ70kg8Ew/7G5mqbufv56bSkJLuuFffOEEGISSbIhZjSl1Kj7yDisZm67rIDbLis46/NvuzR/6HetNftOdFN5qJl3qtp442ALT2+vx2Uz8/EFmawqTuGNg808sa2Ox96rGXWt9Hg71y/MoizfywOvHeFIsw+Tgpf3NfKTTyykKNVNY5fRu9IfCDEQCLGzapA3u/bisJn5TFkOxemeSXlfhBBiMkmyIcQkUUqxIDuBBdkJ3HNVMQCd/kHsFjNOmzF08oml2XT5A9S2+xkMhegPhGn1DdDSM8DWY+08vqWGR985RlaCg0fvKCc1zsHfPPNnvvbb7eO+bnx9A32BEL96q5rlhUkUJrsJhjVhrY2fYU1Juoc1c1KZn5VAS88ADZ1+bGYz6fHGjrGyckcIEU2SbAgRRV7X6BUvCS4rC10Jo8rvWlVEV1+AnbUdXFKQNLTB2cbvrGTj7hNYzIqMeCepHht2ixm71cQH297jmqvW0Oob4Nkd9fxhRz01bb1YTCbMJoXFpNDAC3tP8vPXDmNScMYqYmwWE4tzEliWn0SS24pvIER3X4CGzj5OdPZht5goTIljdloc1y3MID/ZGJrSWg9tW69QOK1mPA4LJklchBBnkGRDiItIgtPKmtK0EWV2i5lbynPHPN8S+WJPibPzjdWz+MY4m6q1+QbYdKSFI00+Mr1OcrxOAqEwTT0D1LT2sqO2g4c3Vw/NOYmzW8jyOsj2OukPhHmnqpU/7KznvpcOsrwgCa/Lys7aDlp9IyfMmhQkue3Mz4pnaZ6XgmQ3SoHZpEh228lIcGBWiobOPpp7+kl02chOdOJ1WgmENIFQmMFQmMFgGK/LSmaC83zfUiHERUCSDSFmgOQ4O59cmvOh5/QHQoTCGqfVPGbvxMmuPp7b2cCGXQ009fRzZXEqi3ISsFvNhLWmbzA0tEvsnoYuHnj9CFqP8UIfweJcL+vmZ9DVF2B3XQddfUHK8xO5tCiZLl+Y/kAoaqt7mrr7sUWWXAshzo8kG0IIgLN+aWcmOPn2mtl8e83sCV2vpz9Aq28QrTWBkKbVN0BjVz8hrcn2Oknz2OnwB2jo9NPdF8RmMWE1m7CaFTazieNtfp7fc4L7XjqI1ayYlxlPSpyN53bW87stxgTbv3v7JeIdFqxmY9goN8nFwuwEchKd1LT5qW71YTObyE50kuV1kh35r7G7n121nVQ1+7BbTLhsxnb4LquZQCjMu0fbONLsw2JSVJSm8cml2SzKSSDb66Shs4+nt9fx1uEWFmQncO38DLK9Tg439XCstRe7xYTXZSM93k5puodUj33cvViCoTDBsJbl0GLak2RDCBEVHocVj+P0kt1SxlspkzROOXyzYhbN3f3EO61DX8iBUJj9J7p5fvN2POn5Q/NGAqEw1S29PLWtjr5ACI/DuJdOIGRsh9/pD4y4tt1iYnZaHKGwxj8Ywj8YxD8YIqw15flJ3FKeQ5tvkA27GnjtQBMADquJ/kAYk4JFOV7+uKuB/9xa+6HvQ5LbRnl+IquKU8hOdHK4ycehxh4ONfZQ1eLDrBSfWZbDV1YWUJDsZjAUpsM/yNHmXo619eJ1WilOjyM30YXFrDApxUlfmP/+8wlq2noJhIzJwEtyvawpTTvnOTNaawaC4RGJz5uHmvntu8dZkpvIp8qyR9wFWoiPQpINIcRFLS3eMeKx1Wxica6XjiwLFRXFo84PhTVdfQESXdYRPQq+gSAnOvto6OwjyWVjbmY8NovprK//vXVz2F3XwaFGH1XNPpLcVj5VlkOW10l/IMQ7Va10+gOUZngoTHETDBmvX9/p51BjD/tOdPPe0TZe2d80dM2MeAelGR5WFafQ1jvIU9vqhnprJm7X0G9KgdbGrrjXL8qkps3P3hNd2C1m5mZ6mJPhIS/JTW6Sk3AY6jv81Hf0Dftp/N47GGJpnpfrFmSyu76T5z84SUqcjTcPtfDz1w5zZUkq36qYxYozdtft8gc41NRDcVrcmMNO/QFjiC3NYx+VDGmt2VHTgcNqpiTdM6E2ORv/YBCTUtJjdBGRZEMIMa2YTYqkMb7w4uwWStI9lHzEvUjMJsWy/CSW5Y/ugXFYzVw9N31UeYLLSl6yi8tnpQDGF+rxNj+tvgGK0+JGrVL6/ro5/HFXA72DxnCSx2FlVoqbwlQ3nf4AR5p9nOjsIxTWBEOanqbjfGrNCopS3dgtJoJhzYt7G3lkczUPvlFFVoKDBdkJDATDvH2kdWiX3DN57BZyklzkJ7tYOTsFt93MGweb+d8vHMBmMfHXa0u4e3URLT3GaqfHt9Rw6/otLM71khFvpy8Qpr7dT3Vr79B7VZ6fyJI8L1obScaehi72NnQRCGkcVhMFyW4W53gpL0hEA49sPsahph7AWBk1NzOeRdkJLMxJwGY20d47SFNPP9UtvRxv7cUU6GPbwEFmpcZxvLWXg409JLpsrC5NJT/ZxZPv1/HsjnpsFhO3XZrPFy/Noz8Q5nhbLzWtvdS0+znR2QeAxWwizWOnLC+R4vQ43q1q46W9jZhM8FdrS1leOH6v24USDIWxmM8/AYs1pc93Btc5Ki8v19u3j793wLmorKykoqJiUq85VUjdK2IdRkxI3StiHUZMjFd3rTW9g6GhZdOndPQOUtfhp669LzK3xUlOoosE59g709a2+XFYTaN6lfoDIZ7eXseT79cZk4ltZlI9dpbkeilJ9/BBfSev7m/iaIsPs0lhNZkoyfBwSUES2V4HNW1+qlp87KrtpKvPGNYqTfdw56pCnFYzexq6+KC+k70N3fgGgkOvazUrCpLdFKS4OdrQQm2PsYeMSUFBipuWngF6+o3zbWYTNy/Joqc/yMv7G0dNUnZFbshoUopAOMyJzj76A6fvmTQnw0OnP0Bjdz/XzE3n0qIkUuLsKAV17X4au/tZlO1l7bx0nDYzrx1o4uV9TaTE2SjLS2RBdgJpHjtuu4X23kEONfZwsqsPpcCkFJkJzqEeoGAoTE+/0QvjspuH7lzdHwjz6oEmHt9Sw7bj7RQmu1mYk4BnoJXb113KrNQ4qiPvo91q4orZKSS5bexp6GLDrgbCYc21CzJYUZhMIBQ2bt3Q4qO6pZfqFh9fu7KI+Vmjl9+fL6XUDq11+VjHpGdDCCGmCaXUqEQDINFtI9FtY1GOd0LXGb6j7nAOq5nbLyvg9nF21107L52//ljpWa8fDmuqWnx09wVYlp84NCRz4+KsoePH23rRQLLbRrzDOjT8UllZyfLLr6C+o4+8JBcOq5lgKMzuuk4ON/lYOy996N5E1S0+XtnfREqcnYLITsGpcSMn7AZCYQ6e7OFQUw9leV6KUuPoGwzxyNvV/GpT9dB8nVPcNjOPb6nFvMHYW8Y3ECQlzoZvIMiv3zk+dJ7NYmJw2I0fR7+XphFJDjBqH5y8JBd3riyktt3P1up2GrsHefzAJiwmNbRM/ZQ0j53mngFsFhNmpXjsvRrcNjP+QGhEwpWV4ODmJdnMz/rwNppskmwIIYS4oEwm9aHDWSaToig1btzjLptlxPMtZhPlBSPvCA1QlBrHN1aPfx0w5gAtzDGGbU5x2szcc1Ux314zm+6+IK29A2ityfa6cFhN7G3o5sW9J+nwD3L9wiwum5VMWGsOnOzmUGMPbb2DtPcOkuaxU5LuGZpYGwqHqe/oo6rZR2NXPx6HlXinhbCGvsEg/YGw0SNkVizM8bJqdsqIOS7PvPAGOq2Yw409lGZ4WJqXSO9AkM1HWth/spsrZqdy/aJMrGZF5aEW3j3aSmqcg6JUN0Wpxr2iXLbYfO1LsiGEEEKMQSlFgss66kaIZyYnAGYUi3K8Z+09mp3moeKMjfsmKtVlomKMDf4W545+zesWZnLdwsxzep1omPqzToQQQghxUZNkQwghhBBRJcmGEEIIIaJKkg0hhBBCRJUkG0IIIYSIKkk2hBBCCBFVkmwIIYQQIqok2RBCCCFEVEmyIYQQQoiokmRDCCGEEFElyYYQQgghokqSDSGEEEJElSQbQgghhIgqpYff6P5CvrBSLUDNJF82BWid5GtOFVL3mUnqPjNJ3Wemi73u+Vrr1LEOxCzZiAal1HatdXms44gFqbvUfaaRukvdZ5qpXHcZRhFCCCFEVEmyIYQQQoiomm7JxvpYBxBDUveZSeo+M0ndZ6YpW/dpNWdDCCGEEBef6dazIYQQQoiLzLRINpRS65RSh5RSVUqpv411PNGklMpVSr2plNqvlNqnlPqLSPk/KqUalFK7I/9dF+tYo0EpdVwptSdSx+2RsiSl1KtKqSORn4mxjnOyKaVKh7XtbqVUt1Lq3una7kqpR5VSzUqpvcPKxmxnZfhF5P//D5RSZbGL/PyNU/d/UkodjNRvg1LKGykvUEr1DWv/f49d5OdvnLqP+xlXSv0g0u6HlFLXxibqyTFO3Z8aVu/jSqndkfIp1+5TfhhFKWUGDgNrgXpgG/B5rfX+mAYWJUqpTCBTa71TKeUBdgCfAD4L+LTW/xzTAKNMKXUcKNdatw4r+xnQrrX+aSTZTNRafz9WMUZb5DPfAKwAvsI0bHel1JWAD/it1npBpGzMdo58+XwHuA7jPXlAa70iVrGfr3Hq/jHgDa11UCl1H0Ck7gXAn06dN9WNU/d/ZIzPuFJqHvAEsBzIAl4DSrTWoQsa9CQZq+5nHL8f6NJa/3gqtvt06NlYDlRprau11oPAk8DNMY4parTWJ7XWOyO/9wAHgOzYRhVzNwOPRX5/DCP5ms6uBo5qrSd7U7yLhtZ6E9B+RvF47Xwzxh9orbXeAngjSfmUNFbdtdavaK2DkYdbgJwLHtgFME67j+dm4Emt9YDW+hhQhfF9MCV9WN2VUgrjH5RPXNCgJtF0SDaygbphj+uZIV++kex2KbA1UnRPpJv10ek4lBChgVeUUjuUUndHytK11icjvzcC6bEJ7YK5lZF/dGZCu8P47TzT/gZ8FXhx2ONCpdQupdRbSqlVsQoqysb6jM+kdl8FNGmtjwwrm1LtPh2SjRlJKRUH/AG4V2vdDfwSmAUsAU4C98cwvGi6QmtdBnwc+Hak63GINsYFp/bY4IdQStmAm4BnIkUzpd1HmO7tPB6l1A+BIPD7SNFJIE9rvRT4K+A/lVLxsYovSmbkZ/wMn2fkPzCmXLtPh2SjAcgd9jgnUjZtKaWsGInG77XWzwForZu01iGtdRj4D6Zwd+KH0Vo3RH42Axsw6tl0qts88rM5dhFG3ceBnVrrJpg57R4xXjvPiL8BSqk7gBuAL0aSLSJDCG2R33cAR4GSmAUZBR/yGZ8p7W4BPgU8dapsKrb7dEg2tgHFSqnCyL/6bgU2xjimqImM3T0CHNBa/8uw8uFj1J8E9p753KlOKeWOTIpFKeUGPoZRz43AlyOnfRn4r9hEeEGM+BfOTGj3YcZr543A7ZFVKZdiTKI7OdYFpiql1Drge8BNWmv/sPLUyIRhlFJFQDFQHZsoo+NDPuMbgVuVUnalVCFG3d+/0PFdANcAB7XW9acKpmK7W2IdwPmKzM6+B3gZMAOPaq33xTisaFoJ3AbsObUMCvg74PNKqSUYXcvHga/HJryoSgc2GPkWFuA/tdYvKaW2AU8rpe7EuJPwZ2MYY9REEqy1jGzbn03HdldKPQFUAClKqXrgH4CfMnY7v4CxEqUK8GOs0Jmyxqn7DwA78Grk879Fa/0N4Ergx0qpABAGvqG1nugEy4vOOHWvGOszrrXep5R6GtiPMbT07am6EgXGrrvW+hFGz9GCKdjuU37pqxBCCCEubtNhGEUIIYQQFzFJNoQQQggRVZJsCCGEECKqJNkQQgghRFRJsiGEEEKIqJJkQwghhBBRJcmGEEIIIaJKkg0hhBBCRNX/B4u9qa8PhIXoAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 648x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UpTKiAGOE4z",
        "outputId": "4e89a9c2-596b-4e29-f1a7-3f52c960d3cf"
      },
      "source": [
        "loss_L2, accuracy_L2 = L2.evaluate(X_test, y_test)\n",
        "print('Loss = {:.5f}'.format(loss_L2))\n",
        "print('Accuracy = {:.5f}'.format (accuracy_L2))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.9960 - accuracy: 0.8863\n",
            "Loss = 0.99603\n",
            "Accuracy = 0.88630\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BP4jlwCQtogg"
      },
      "source": [
        "# L2 + Batch Normal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tec88fHZxHLJ",
        "outputId": "d7cdaf47-f720-4329-cd08-2e43d8c15f65"
      },
      "source": [
        "L2_BN = models.Sequential()\n",
        "L2_BN.add(layers.Dense(512, input_shape=(28*28,),\n",
        "                       kernel_regularizer=regularizers.l2(0.00001)))\n",
        "L2_BN.add(layers.BatchNormalization())\n",
        "L2_BN.add(layers.Activation('relu'))\n",
        "L2_BN.add(layers.Dense(256,\n",
        "                       kernel_regularizer=regularizers.l2(0.00001)))\n",
        "L2_BN.add(layers.BatchNormalization())\n",
        "L2_BN.add(layers.Activation('relu'))\n",
        "L2_BN.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "L2_BN.summary()\n",
        "\n",
        "L2_BN.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 538,890\n",
            "Trainable params: 537,354\n",
            "Non-trainable params: 1,536\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMeEJq_iVcBl",
        "outputId": "3a808f10-2e1d-4fd1-9324-9691dba7bc0f"
      },
      "source": [
        "ex = EarlyStopping(monitor='val_accuracy', mode='max', patience=100, verbose = 1)\n",
        "mc = ModelCheckpoint('best-L2_BN.h5', monitor='val_accuracy', mode='max', save_best_only=True, verbose=1)\n",
        "\n",
        "%time\n",
        "Hist_L2_BN = L2_BN.fit(X_train, y_train, epochs=500, batch_size=128, validation_split=0.2, callbacks=[ex, mc], verbose=1)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "375/375 [==============================] - 4s 7ms/step - loss: 0.5669 - accuracy: 0.7990 - val_loss: 0.4579 - val_accuracy: 0.8398\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.83983, saving model to best-DR_BN.h5\n",
            "Epoch 2/500\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.3436 - accuracy: 0.8761 - val_loss: 0.3962 - val_accuracy: 0.8580\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.83983 to 0.85800, saving model to best-DR_BN.h5\n",
            "Epoch 3/500\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.2864 - accuracy: 0.8977 - val_loss: 0.3918 - val_accuracy: 0.8637\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.85800 to 0.86367, saving model to best-DR_BN.h5\n",
            "Epoch 4/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.2634 - accuracy: 0.9075 - val_loss: 0.4183 - val_accuracy: 0.8615\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.86367\n",
            "Epoch 5/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.2431 - accuracy: 0.9140 - val_loss: 0.3558 - val_accuracy: 0.8816\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.86367 to 0.88158, saving model to best-DR_BN.h5\n",
            "Epoch 6/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.2215 - accuracy: 0.9238 - val_loss: 0.4213 - val_accuracy: 0.8684\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.88158\n",
            "Epoch 7/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.2041 - accuracy: 0.9295 - val_loss: 0.3468 - val_accuracy: 0.8882\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.88158 to 0.88825, saving model to best-DR_BN.h5\n",
            "Epoch 8/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1883 - accuracy: 0.9367 - val_loss: 0.4309 - val_accuracy: 0.8740\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.88825\n",
            "Epoch 9/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1831 - accuracy: 0.9378 - val_loss: 0.4362 - val_accuracy: 0.8660\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.88825\n",
            "Epoch 10/500\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.1698 - accuracy: 0.9439 - val_loss: 0.3979 - val_accuracy: 0.8813\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.88825\n",
            "Epoch 11/500\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.1571 - accuracy: 0.9501 - val_loss: 0.4186 - val_accuracy: 0.8787\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.88825\n",
            "Epoch 12/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1525 - accuracy: 0.9514 - val_loss: 0.4197 - val_accuracy: 0.8840\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.88825\n",
            "Epoch 13/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1444 - accuracy: 0.9539 - val_loss: 0.4255 - val_accuracy: 0.8833\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.88825\n",
            "Epoch 14/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1381 - accuracy: 0.9565 - val_loss: 0.4488 - val_accuracy: 0.8887\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.88825 to 0.88867, saving model to best-DR_BN.h5\n",
            "Epoch 15/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1289 - accuracy: 0.9611 - val_loss: 0.5383 - val_accuracy: 0.8688\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.88867\n",
            "Epoch 16/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1272 - accuracy: 0.9631 - val_loss: 0.5606 - val_accuracy: 0.8701\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.88867\n",
            "Epoch 17/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1207 - accuracy: 0.9648 - val_loss: 0.5645 - val_accuracy: 0.8686\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.88867\n",
            "Epoch 18/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1144 - accuracy: 0.9678 - val_loss: 0.5986 - val_accuracy: 0.8813\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.88867\n",
            "Epoch 19/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1085 - accuracy: 0.9696 - val_loss: 0.4861 - val_accuracy: 0.8832\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.88867\n",
            "Epoch 20/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1068 - accuracy: 0.9720 - val_loss: 0.5486 - val_accuracy: 0.8783\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.88867\n",
            "Epoch 21/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1079 - accuracy: 0.9721 - val_loss: 0.6265 - val_accuracy: 0.8728\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.88867\n",
            "Epoch 22/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1014 - accuracy: 0.9753 - val_loss: 0.6151 - val_accuracy: 0.8715\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.88867\n",
            "Epoch 23/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1023 - accuracy: 0.9743 - val_loss: 0.5363 - val_accuracy: 0.8860\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.88867\n",
            "Epoch 24/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.0988 - accuracy: 0.9754 - val_loss: 0.6313 - val_accuracy: 0.8823\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.88867\n",
            "Epoch 25/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.0945 - accuracy: 0.9775 - val_loss: 0.7647 - val_accuracy: 0.8639\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.88867\n",
            "Epoch 26/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.0959 - accuracy: 0.9775 - val_loss: 0.7074 - val_accuracy: 0.8669\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.88867\n",
            "Epoch 27/500\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.0935 - accuracy: 0.9798 - val_loss: 0.6709 - val_accuracy: 0.8650\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.88867\n",
            "Epoch 28/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.0897 - accuracy: 0.9800 - val_loss: 0.6184 - val_accuracy: 0.8790\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.88867\n",
            "Epoch 29/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.0890 - accuracy: 0.9810 - val_loss: 0.8203 - val_accuracy: 0.8718\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.88867\n",
            "Epoch 30/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.0912 - accuracy: 0.9801 - val_loss: 0.6278 - val_accuracy: 0.8846\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.88867\n",
            "Epoch 31/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.0854 - accuracy: 0.9825 - val_loss: 0.7014 - val_accuracy: 0.8802\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.88867\n",
            "Epoch 32/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.0865 - accuracy: 0.9827 - val_loss: 0.7312 - val_accuracy: 0.8763\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.88867\n",
            "Epoch 33/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0883 - accuracy: 0.9828 - val_loss: 0.6791 - val_accuracy: 0.8863\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.88867\n",
            "Epoch 34/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0838 - accuracy: 0.9840 - val_loss: 0.8186 - val_accuracy: 0.8785\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.88867\n",
            "Epoch 35/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0831 - accuracy: 0.9840 - val_loss: 0.7052 - val_accuracy: 0.8756\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.88867\n",
            "Epoch 36/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0836 - accuracy: 0.9840 - val_loss: 0.7228 - val_accuracy: 0.8848\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.88867\n",
            "Epoch 37/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.0825 - accuracy: 0.9850 - val_loss: 0.9378 - val_accuracy: 0.8671\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.88867\n",
            "Epoch 38/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.0828 - accuracy: 0.9846 - val_loss: 0.7017 - val_accuracy: 0.8790\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.88867\n",
            "Epoch 39/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.0817 - accuracy: 0.9856 - val_loss: 0.6958 - val_accuracy: 0.8798\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.88867\n",
            "Epoch 40/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.0833 - accuracy: 0.9854 - val_loss: 0.7467 - val_accuracy: 0.8786\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.88867\n",
            "Epoch 41/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.0796 - accuracy: 0.9861 - val_loss: 0.7999 - val_accuracy: 0.8780\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.88867\n",
            "Epoch 42/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0840 - accuracy: 0.9848 - val_loss: 0.7112 - val_accuracy: 0.8863\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.88867\n",
            "Epoch 43/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0804 - accuracy: 0.9869 - val_loss: 0.7385 - val_accuracy: 0.8837\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.88867\n",
            "Epoch 44/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.0813 - accuracy: 0.9877 - val_loss: 0.7357 - val_accuracy: 0.8806\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.88867\n",
            "Epoch 45/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0803 - accuracy: 0.9870 - val_loss: 0.8854 - val_accuracy: 0.8662\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.88867\n",
            "Epoch 46/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0815 - accuracy: 0.9867 - val_loss: 0.8260 - val_accuracy: 0.8781\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.88867\n",
            "Epoch 47/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.0771 - accuracy: 0.9877 - val_loss: 0.9760 - val_accuracy: 0.8597\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.88867\n",
            "Epoch 48/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.0797 - accuracy: 0.9874 - val_loss: 0.8700 - val_accuracy: 0.8699\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.88867\n",
            "Epoch 49/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0781 - accuracy: 0.9881 - val_loss: 0.8012 - val_accuracy: 0.8854\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.88867\n",
            "Epoch 50/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.0819 - accuracy: 0.9863 - val_loss: 0.7986 - val_accuracy: 0.8856\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.88867\n",
            "Epoch 51/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.0802 - accuracy: 0.9866 - val_loss: 0.8526 - val_accuracy: 0.8789\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.88867\n",
            "Epoch 52/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0772 - accuracy: 0.9886 - val_loss: 0.7498 - val_accuracy: 0.8886\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.88867\n",
            "Epoch 53/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0771 - accuracy: 0.9882 - val_loss: 1.0373 - val_accuracy: 0.8817\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.88867\n",
            "Epoch 54/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0810 - accuracy: 0.9873 - val_loss: 0.7123 - val_accuracy: 0.8815\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.88867\n",
            "Epoch 55/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0779 - accuracy: 0.9885 - val_loss: 0.8396 - val_accuracy: 0.8826\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.88867\n",
            "Epoch 56/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0753 - accuracy: 0.9894 - val_loss: 0.7616 - val_accuracy: 0.8896\n",
            "\n",
            "Epoch 00056: val_accuracy improved from 0.88867 to 0.88958, saving model to best-DR_BN.h5\n",
            "Epoch 57/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0767 - accuracy: 0.9880 - val_loss: 0.8454 - val_accuracy: 0.8913\n",
            "\n",
            "Epoch 00057: val_accuracy improved from 0.88958 to 0.89133, saving model to best-DR_BN.h5\n",
            "Epoch 58/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.0759 - accuracy: 0.9889 - val_loss: 0.8622 - val_accuracy: 0.8829\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.89133\n",
            "Epoch 59/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.0757 - accuracy: 0.9891 - val_loss: 0.8723 - val_accuracy: 0.8836\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.89133\n",
            "Epoch 60/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0775 - accuracy: 0.9895 - val_loss: 0.8413 - val_accuracy: 0.8773\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.89133\n",
            "Epoch 61/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0784 - accuracy: 0.9882 - val_loss: 0.9518 - val_accuracy: 0.8795\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.89133\n",
            "Epoch 62/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0764 - accuracy: 0.9889 - val_loss: 0.9652 - val_accuracy: 0.8733\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.89133\n",
            "Epoch 63/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0755 - accuracy: 0.9892 - val_loss: 0.8976 - val_accuracy: 0.8839\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.89133\n",
            "Epoch 64/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0765 - accuracy: 0.9892 - val_loss: 0.9084 - val_accuracy: 0.8780\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.89133\n",
            "Epoch 65/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0763 - accuracy: 0.9895 - val_loss: 0.8757 - val_accuracy: 0.8712\n",
            "\n",
            "Epoch 00065: val_accuracy did not improve from 0.89133\n",
            "Epoch 66/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0773 - accuracy: 0.9891 - val_loss: 0.9982 - val_accuracy: 0.8764\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.89133\n",
            "Epoch 67/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0765 - accuracy: 0.9892 - val_loss: 0.9445 - val_accuracy: 0.8818\n",
            "\n",
            "Epoch 00067: val_accuracy did not improve from 0.89133\n",
            "Epoch 68/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0756 - accuracy: 0.9896 - val_loss: 1.1239 - val_accuracy: 0.8641\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.89133\n",
            "Epoch 69/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0755 - accuracy: 0.9894 - val_loss: 0.9225 - val_accuracy: 0.8830\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.89133\n",
            "Epoch 70/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0741 - accuracy: 0.9895 - val_loss: 1.0302 - val_accuracy: 0.8758\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.89133\n",
            "Epoch 71/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0741 - accuracy: 0.9891 - val_loss: 0.9642 - val_accuracy: 0.8643\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.89133\n",
            "Epoch 72/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0768 - accuracy: 0.9895 - val_loss: 0.8965 - val_accuracy: 0.8865\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.89133\n",
            "Epoch 73/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0701 - accuracy: 0.9917 - val_loss: 0.9441 - val_accuracy: 0.8775\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.89133\n",
            "Epoch 74/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0722 - accuracy: 0.9909 - val_loss: 0.8787 - val_accuracy: 0.8859\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.89133\n",
            "Epoch 75/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0762 - accuracy: 0.9898 - val_loss: 0.9075 - val_accuracy: 0.8853\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.89133\n",
            "Epoch 76/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0743 - accuracy: 0.9900 - val_loss: 0.9897 - val_accuracy: 0.8861\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.89133\n",
            "Epoch 77/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0726 - accuracy: 0.9905 - val_loss: 1.0601 - val_accuracy: 0.8799\n",
            "\n",
            "Epoch 00077: val_accuracy did not improve from 0.89133\n",
            "Epoch 78/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0719 - accuracy: 0.9912 - val_loss: 0.9620 - val_accuracy: 0.8816\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.89133\n",
            "Epoch 79/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0716 - accuracy: 0.9912 - val_loss: 0.8685 - val_accuracy: 0.8878\n",
            "\n",
            "Epoch 00079: val_accuracy did not improve from 0.89133\n",
            "Epoch 80/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0735 - accuracy: 0.9909 - val_loss: 0.9881 - val_accuracy: 0.8738\n",
            "\n",
            "Epoch 00080: val_accuracy did not improve from 0.89133\n",
            "Epoch 81/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0749 - accuracy: 0.9896 - val_loss: 0.8922 - val_accuracy: 0.8857\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.89133\n",
            "Epoch 82/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0730 - accuracy: 0.9908 - val_loss: 0.9893 - val_accuracy: 0.8827\n",
            "\n",
            "Epoch 00082: val_accuracy did not improve from 0.89133\n",
            "Epoch 83/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0735 - accuracy: 0.9908 - val_loss: 0.9619 - val_accuracy: 0.8785\n",
            "\n",
            "Epoch 00083: val_accuracy did not improve from 0.89133\n",
            "Epoch 84/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0747 - accuracy: 0.9908 - val_loss: 0.9280 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00084: val_accuracy did not improve from 0.89133\n",
            "Epoch 85/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0750 - accuracy: 0.9909 - val_loss: 0.9766 - val_accuracy: 0.8810\n",
            "\n",
            "Epoch 00085: val_accuracy did not improve from 0.89133\n",
            "Epoch 86/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0740 - accuracy: 0.9916 - val_loss: 1.0316 - val_accuracy: 0.8674\n",
            "\n",
            "Epoch 00086: val_accuracy did not improve from 0.89133\n",
            "Epoch 87/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0723 - accuracy: 0.9914 - val_loss: 0.9584 - val_accuracy: 0.8855\n",
            "\n",
            "Epoch 00087: val_accuracy did not improve from 0.89133\n",
            "Epoch 88/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0733 - accuracy: 0.9904 - val_loss: 0.8642 - val_accuracy: 0.8852\n",
            "\n",
            "Epoch 00088: val_accuracy did not improve from 0.89133\n",
            "Epoch 89/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0719 - accuracy: 0.9908 - val_loss: 0.9480 - val_accuracy: 0.8852\n",
            "\n",
            "Epoch 00089: val_accuracy did not improve from 0.89133\n",
            "Epoch 90/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0734 - accuracy: 0.9915 - val_loss: 1.0380 - val_accuracy: 0.8868\n",
            "\n",
            "Epoch 00090: val_accuracy did not improve from 0.89133\n",
            "Epoch 91/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0708 - accuracy: 0.9917 - val_loss: 0.9186 - val_accuracy: 0.8856\n",
            "\n",
            "Epoch 00091: val_accuracy did not improve from 0.89133\n",
            "Epoch 92/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0730 - accuracy: 0.9913 - val_loss: 1.0821 - val_accuracy: 0.8773\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.89133\n",
            "Epoch 93/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0723 - accuracy: 0.9912 - val_loss: 0.9456 - val_accuracy: 0.8853\n",
            "\n",
            "Epoch 00093: val_accuracy did not improve from 0.89133\n",
            "Epoch 94/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0713 - accuracy: 0.9914 - val_loss: 1.1113 - val_accuracy: 0.8721\n",
            "\n",
            "Epoch 00094: val_accuracy did not improve from 0.89133\n",
            "Epoch 95/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0694 - accuracy: 0.9923 - val_loss: 0.9533 - val_accuracy: 0.8831\n",
            "\n",
            "Epoch 00095: val_accuracy did not improve from 0.89133\n",
            "Epoch 96/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0714 - accuracy: 0.9911 - val_loss: 0.9878 - val_accuracy: 0.8871\n",
            "\n",
            "Epoch 00096: val_accuracy did not improve from 0.89133\n",
            "Epoch 97/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0716 - accuracy: 0.9918 - val_loss: 0.9940 - val_accuracy: 0.8863\n",
            "\n",
            "Epoch 00097: val_accuracy did not improve from 0.89133\n",
            "Epoch 98/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0721 - accuracy: 0.9916 - val_loss: 1.2867 - val_accuracy: 0.8649\n",
            "\n",
            "Epoch 00098: val_accuracy did not improve from 0.89133\n",
            "Epoch 99/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0700 - accuracy: 0.9916 - val_loss: 0.9419 - val_accuracy: 0.8802\n",
            "\n",
            "Epoch 00099: val_accuracy did not improve from 0.89133\n",
            "Epoch 100/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0708 - accuracy: 0.9916 - val_loss: 1.1902 - val_accuracy: 0.8713\n",
            "\n",
            "Epoch 00100: val_accuracy did not improve from 0.89133\n",
            "Epoch 101/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0698 - accuracy: 0.9920 - val_loss: 0.9357 - val_accuracy: 0.8904\n",
            "\n",
            "Epoch 00101: val_accuracy did not improve from 0.89133\n",
            "Epoch 102/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0694 - accuracy: 0.9919 - val_loss: 1.0839 - val_accuracy: 0.8794\n",
            "\n",
            "Epoch 00102: val_accuracy did not improve from 0.89133\n",
            "Epoch 103/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0703 - accuracy: 0.9915 - val_loss: 1.1552 - val_accuracy: 0.8753\n",
            "\n",
            "Epoch 00103: val_accuracy did not improve from 0.89133\n",
            "Epoch 104/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0702 - accuracy: 0.9922 - val_loss: 0.9891 - val_accuracy: 0.8832\n",
            "\n",
            "Epoch 00104: val_accuracy did not improve from 0.89133\n",
            "Epoch 105/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0687 - accuracy: 0.9924 - val_loss: 0.9940 - val_accuracy: 0.8826\n",
            "\n",
            "Epoch 00105: val_accuracy did not improve from 0.89133\n",
            "Epoch 106/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0704 - accuracy: 0.9921 - val_loss: 1.0112 - val_accuracy: 0.8845\n",
            "\n",
            "Epoch 00106: val_accuracy did not improve from 0.89133\n",
            "Epoch 107/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0747 - accuracy: 0.9905 - val_loss: 0.9888 - val_accuracy: 0.8779\n",
            "\n",
            "Epoch 00107: val_accuracy did not improve from 0.89133\n",
            "Epoch 108/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0743 - accuracy: 0.9909 - val_loss: 1.1012 - val_accuracy: 0.8737\n",
            "\n",
            "Epoch 00108: val_accuracy did not improve from 0.89133\n",
            "Epoch 109/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0688 - accuracy: 0.9917 - val_loss: 1.2069 - val_accuracy: 0.8665\n",
            "\n",
            "Epoch 00109: val_accuracy did not improve from 0.89133\n",
            "Epoch 110/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0719 - accuracy: 0.9912 - val_loss: 0.9486 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00110: val_accuracy did not improve from 0.89133\n",
            "Epoch 111/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0678 - accuracy: 0.9925 - val_loss: 1.1186 - val_accuracy: 0.8774\n",
            "\n",
            "Epoch 00111: val_accuracy did not improve from 0.89133\n",
            "Epoch 112/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0736 - accuracy: 0.9916 - val_loss: 1.0476 - val_accuracy: 0.8843\n",
            "\n",
            "Epoch 00112: val_accuracy did not improve from 0.89133\n",
            "Epoch 113/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0710 - accuracy: 0.9915 - val_loss: 1.1401 - val_accuracy: 0.8725\n",
            "\n",
            "Epoch 00113: val_accuracy did not improve from 0.89133\n",
            "Epoch 114/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0685 - accuracy: 0.9930 - val_loss: 1.1476 - val_accuracy: 0.8699\n",
            "\n",
            "Epoch 00114: val_accuracy did not improve from 0.89133\n",
            "Epoch 115/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0719 - accuracy: 0.9910 - val_loss: 0.8963 - val_accuracy: 0.8880\n",
            "\n",
            "Epoch 00115: val_accuracy did not improve from 0.89133\n",
            "Epoch 116/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0699 - accuracy: 0.9924 - val_loss: 1.0469 - val_accuracy: 0.8888\n",
            "\n",
            "Epoch 00116: val_accuracy did not improve from 0.89133\n",
            "Epoch 117/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0696 - accuracy: 0.9925 - val_loss: 1.0092 - val_accuracy: 0.8741\n",
            "\n",
            "Epoch 00117: val_accuracy did not improve from 0.89133\n",
            "Epoch 118/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0684 - accuracy: 0.9926 - val_loss: 1.0073 - val_accuracy: 0.8801\n",
            "\n",
            "Epoch 00118: val_accuracy did not improve from 0.89133\n",
            "Epoch 119/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0705 - accuracy: 0.9915 - val_loss: 1.1065 - val_accuracy: 0.8732\n",
            "\n",
            "Epoch 00119: val_accuracy did not improve from 0.89133\n",
            "Epoch 120/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0707 - accuracy: 0.9911 - val_loss: 1.0346 - val_accuracy: 0.8837\n",
            "\n",
            "Epoch 00120: val_accuracy did not improve from 0.89133\n",
            "Epoch 121/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0677 - accuracy: 0.9919 - val_loss: 1.1690 - val_accuracy: 0.8763\n",
            "\n",
            "Epoch 00121: val_accuracy did not improve from 0.89133\n",
            "Epoch 122/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0725 - accuracy: 0.9917 - val_loss: 1.1495 - val_accuracy: 0.8776\n",
            "\n",
            "Epoch 00122: val_accuracy did not improve from 0.89133\n",
            "Epoch 123/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0720 - accuracy: 0.9917 - val_loss: 1.0812 - val_accuracy: 0.8758\n",
            "\n",
            "Epoch 00123: val_accuracy did not improve from 0.89133\n",
            "Epoch 124/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0702 - accuracy: 0.9920 - val_loss: 0.9382 - val_accuracy: 0.8848\n",
            "\n",
            "Epoch 00124: val_accuracy did not improve from 0.89133\n",
            "Epoch 125/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0684 - accuracy: 0.9926 - val_loss: 1.0672 - val_accuracy: 0.8849\n",
            "\n",
            "Epoch 00125: val_accuracy did not improve from 0.89133\n",
            "Epoch 126/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0666 - accuracy: 0.9929 - val_loss: 1.0400 - val_accuracy: 0.8853\n",
            "\n",
            "Epoch 00126: val_accuracy did not improve from 0.89133\n",
            "Epoch 127/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0710 - accuracy: 0.9915 - val_loss: 0.9786 - val_accuracy: 0.8809\n",
            "\n",
            "Epoch 00127: val_accuracy did not improve from 0.89133\n",
            "Epoch 128/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0675 - accuracy: 0.9924 - val_loss: 1.1590 - val_accuracy: 0.8781\n",
            "\n",
            "Epoch 00128: val_accuracy did not improve from 0.89133\n",
            "Epoch 129/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0656 - accuracy: 0.9931 - val_loss: 0.9914 - val_accuracy: 0.8887\n",
            "\n",
            "Epoch 00129: val_accuracy did not improve from 0.89133\n",
            "Epoch 130/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0656 - accuracy: 0.9934 - val_loss: 1.0720 - val_accuracy: 0.8839\n",
            "\n",
            "Epoch 00130: val_accuracy did not improve from 0.89133\n",
            "Epoch 131/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0669 - accuracy: 0.9920 - val_loss: 1.0894 - val_accuracy: 0.8836\n",
            "\n",
            "Epoch 00131: val_accuracy did not improve from 0.89133\n",
            "Epoch 132/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0694 - accuracy: 0.9919 - val_loss: 0.9285 - val_accuracy: 0.8898\n",
            "\n",
            "Epoch 00132: val_accuracy did not improve from 0.89133\n",
            "Epoch 133/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0661 - accuracy: 0.9933 - val_loss: 1.0191 - val_accuracy: 0.8859\n",
            "\n",
            "Epoch 00133: val_accuracy did not improve from 0.89133\n",
            "Epoch 134/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0694 - accuracy: 0.9919 - val_loss: 1.0070 - val_accuracy: 0.8834\n",
            "\n",
            "Epoch 00134: val_accuracy did not improve from 0.89133\n",
            "Epoch 135/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0673 - accuracy: 0.9921 - val_loss: 1.0056 - val_accuracy: 0.8881\n",
            "\n",
            "Epoch 00135: val_accuracy did not improve from 0.89133\n",
            "Epoch 136/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0685 - accuracy: 0.9917 - val_loss: 1.3443 - val_accuracy: 0.8706\n",
            "\n",
            "Epoch 00136: val_accuracy did not improve from 0.89133\n",
            "Epoch 137/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0703 - accuracy: 0.9922 - val_loss: 1.2015 - val_accuracy: 0.8837\n",
            "\n",
            "Epoch 00137: val_accuracy did not improve from 0.89133\n",
            "Epoch 138/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0703 - accuracy: 0.9917 - val_loss: 1.1176 - val_accuracy: 0.8801\n",
            "\n",
            "Epoch 00138: val_accuracy did not improve from 0.89133\n",
            "Epoch 139/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0674 - accuracy: 0.9926 - val_loss: 1.0252 - val_accuracy: 0.8840\n",
            "\n",
            "Epoch 00139: val_accuracy did not improve from 0.89133\n",
            "Epoch 140/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0667 - accuracy: 0.9931 - val_loss: 1.2501 - val_accuracy: 0.8746\n",
            "\n",
            "Epoch 00140: val_accuracy did not improve from 0.89133\n",
            "Epoch 141/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0668 - accuracy: 0.9927 - val_loss: 1.2466 - val_accuracy: 0.8675\n",
            "\n",
            "Epoch 00141: val_accuracy did not improve from 0.89133\n",
            "Epoch 142/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0698 - accuracy: 0.9919 - val_loss: 1.0182 - val_accuracy: 0.8893\n",
            "\n",
            "Epoch 00142: val_accuracy did not improve from 0.89133\n",
            "Epoch 143/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0675 - accuracy: 0.9921 - val_loss: 1.0575 - val_accuracy: 0.8716\n",
            "\n",
            "Epoch 00143: val_accuracy did not improve from 0.89133\n",
            "Epoch 144/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0720 - accuracy: 0.9917 - val_loss: 1.0817 - val_accuracy: 0.8798\n",
            "\n",
            "Epoch 00144: val_accuracy did not improve from 0.89133\n",
            "Epoch 145/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0711 - accuracy: 0.9912 - val_loss: 1.3460 - val_accuracy: 0.8767\n",
            "\n",
            "Epoch 00145: val_accuracy did not improve from 0.89133\n",
            "Epoch 146/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0720 - accuracy: 0.9916 - val_loss: 0.9274 - val_accuracy: 0.8863\n",
            "\n",
            "Epoch 00146: val_accuracy did not improve from 0.89133\n",
            "Epoch 147/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0692 - accuracy: 0.9923 - val_loss: 1.0974 - val_accuracy: 0.8810\n",
            "\n",
            "Epoch 00147: val_accuracy did not improve from 0.89133\n",
            "Epoch 148/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0687 - accuracy: 0.9922 - val_loss: 1.1055 - val_accuracy: 0.8812\n",
            "\n",
            "Epoch 00148: val_accuracy did not improve from 0.89133\n",
            "Epoch 149/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0670 - accuracy: 0.9923 - val_loss: 1.1021 - val_accuracy: 0.8832\n",
            "\n",
            "Epoch 00149: val_accuracy did not improve from 0.89133\n",
            "Epoch 150/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0672 - accuracy: 0.9933 - val_loss: 1.1739 - val_accuracy: 0.8757\n",
            "\n",
            "Epoch 00150: val_accuracy did not improve from 0.89133\n",
            "Epoch 151/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0645 - accuracy: 0.9929 - val_loss: 1.0339 - val_accuracy: 0.8775\n",
            "\n",
            "Epoch 00151: val_accuracy did not improve from 0.89133\n",
            "Epoch 152/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0703 - accuracy: 0.9917 - val_loss: 1.0889 - val_accuracy: 0.8831\n",
            "\n",
            "Epoch 00152: val_accuracy did not improve from 0.89133\n",
            "Epoch 153/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0696 - accuracy: 0.9922 - val_loss: 1.0133 - val_accuracy: 0.8832\n",
            "\n",
            "Epoch 00153: val_accuracy did not improve from 0.89133\n",
            "Epoch 154/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0690 - accuracy: 0.9920 - val_loss: 1.0469 - val_accuracy: 0.8869\n",
            "\n",
            "Epoch 00154: val_accuracy did not improve from 0.89133\n",
            "Epoch 155/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0685 - accuracy: 0.9935 - val_loss: 1.1198 - val_accuracy: 0.8815\n",
            "\n",
            "Epoch 00155: val_accuracy did not improve from 0.89133\n",
            "Epoch 156/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0670 - accuracy: 0.9935 - val_loss: 1.0988 - val_accuracy: 0.8821\n",
            "\n",
            "Epoch 00156: val_accuracy did not improve from 0.89133\n",
            "Epoch 157/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0672 - accuracy: 0.9926 - val_loss: 1.1010 - val_accuracy: 0.8823\n",
            "\n",
            "Epoch 00157: val_accuracy did not improve from 0.89133\n",
            "Epoch 00157: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "kB7leuyJ-2gG",
        "outputId": "950c03c7-ba49-413b-d1a1-b64649b1e8c4"
      },
      "source": [
        "epochs = range(1, len(Hist_L2_BN.history['loss'])+1)\n",
        "\n",
        "plt.figure(figsize=(9,6))\n",
        "plt.plot(epochs, Hist_L2_BN.history['loss'])\n",
        "plt.plot(epochs, Hist_L2_BN.history['val_loss'])\n",
        "plt.legend(['Train loss','Test loss'])\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 648x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCMVqV9t-6Ui",
        "outputId": "09d0a4ac-0bf4-4a52-cd46-652d617f18f8"
      },
      "source": [
        "loss_L2_BN, accuracy_L2_BN = L2_BN.evaluate(X_test, y_test)\n",
        "print('Loss = {:.5f}'.format(loss_L2_BN))\n",
        "print('Accuracy = {:.5f}'.format (accuracy_L2_BN))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 1.1495 - accuracy: 0.8781\n",
            "Loss = 1.14954\n",
            "Accuracy = 0.87810\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKDUHyLOtvr6"
      },
      "source": [
        "# Dropout + Batch Normal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4Jfsag7_D8N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e53afb4f-ee39-46dc-8698-15bae3a6e2bc"
      },
      "source": [
        "DR_BN = models.Sequential()\n",
        "\n",
        "DR_BN.add(layers.Dense(512, input_shape=(28*28,)))\n",
        "DR_BN.add(layers.BatchNormalization())\n",
        "DR_BN.add(layers.Activation('relu'))\n",
        "DR_BN.add(layers.Dropout(0.4))\n",
        "DR_BN.add(layers.Dense(256))\n",
        "DR_BN.add(layers.BatchNormalization())\n",
        "\n",
        "DR_BN.add(layers.Activation('relu'))\n",
        "DR_BN.add(layers.Dropout(0.4))\n",
        "DR_BN.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "DR_BN.summary()\n",
        "\n",
        "DR_BN.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_9 (Dense)              (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 538,890\n",
            "Trainable params: 537,354\n",
            "Non-trainable params: 1,536\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpP5xF0gQ2Hx",
        "outputId": "0dd1a51e-e05f-4530-9e73-41c3712fa762"
      },
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "es = EarlyStopping(monitor='val_accuracy', mode='max', patience=100, verbose = 1)\n",
        "mc = ModelCheckpoint('best-DR_BN.h5', monitor='val_accuracy', mode='max', save_best_only=True, verbose=1)\n",
        "\n",
        "%time\n",
        "Hist_DR_BN = DR_BN.fit(X_train, y_train, epochs=500, batch_size=128, validation_split=0.2, callbacks=[es, mc], verbose=1)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
            "Wall time: 6.44 µs\n",
            "Epoch 1/500\n",
            "375/375 [==============================] - 4s 7ms/step - loss: 0.6970 - accuracy: 0.7589 - val_loss: 0.4778 - val_accuracy: 0.8348\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.83483, saving model to best-DR_BN.h5\n",
            "Epoch 2/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.4238 - accuracy: 0.8473 - val_loss: 0.4133 - val_accuracy: 0.8481\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.83483 to 0.84808, saving model to best-DR_BN.h5\n",
            "Epoch 3/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.3814 - accuracy: 0.8605 - val_loss: 0.4075 - val_accuracy: 0.8597\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.84808 to 0.85975, saving model to best-DR_BN.h5\n",
            "Epoch 4/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.3532 - accuracy: 0.8711 - val_loss: 0.3583 - val_accuracy: 0.8735\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.85975 to 0.87350, saving model to best-DR_BN.h5\n",
            "Epoch 5/500\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.3319 - accuracy: 0.8778 - val_loss: 0.4152 - val_accuracy: 0.8612\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.87350\n",
            "Epoch 6/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.3270 - accuracy: 0.8800 - val_loss: 0.3844 - val_accuracy: 0.8657\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.87350\n",
            "Epoch 7/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.3124 - accuracy: 0.8886 - val_loss: 0.3262 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.87350 to 0.88417, saving model to best-DR_BN.h5\n",
            "Epoch 8/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.3069 - accuracy: 0.8894 - val_loss: 0.3495 - val_accuracy: 0.8818\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.88417\n",
            "Epoch 9/500\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.2957 - accuracy: 0.8941 - val_loss: 0.3392 - val_accuracy: 0.8875\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.88417 to 0.88750, saving model to best-DR_BN.h5\n",
            "Epoch 10/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.2897 - accuracy: 0.8951 - val_loss: 0.3608 - val_accuracy: 0.8801\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.88750\n",
            "Epoch 11/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.2864 - accuracy: 0.8969 - val_loss: 0.3376 - val_accuracy: 0.8863\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.88750\n",
            "Epoch 12/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.2698 - accuracy: 0.9008 - val_loss: 0.3655 - val_accuracy: 0.8812\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.88750\n",
            "Epoch 13/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.2709 - accuracy: 0.9006 - val_loss: 0.3974 - val_accuracy: 0.8833\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.88750\n",
            "Epoch 14/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.2548 - accuracy: 0.9081 - val_loss: 0.3246 - val_accuracy: 0.8892\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.88750 to 0.88925, saving model to best-DR_BN.h5\n",
            "Epoch 15/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.2577 - accuracy: 0.9074 - val_loss: 0.3711 - val_accuracy: 0.8792\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.88925\n",
            "Epoch 16/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.2507 - accuracy: 0.9100 - val_loss: 0.3758 - val_accuracy: 0.8833\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.88925\n",
            "Epoch 17/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.2494 - accuracy: 0.9085 - val_loss: 0.3571 - val_accuracy: 0.8856\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.88925\n",
            "Epoch 18/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.2336 - accuracy: 0.9135 - val_loss: 0.3240 - val_accuracy: 0.8946\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.88925 to 0.89458, saving model to best-DR_BN.h5\n",
            "Epoch 19/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.2332 - accuracy: 0.9139 - val_loss: 0.3718 - val_accuracy: 0.8873\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.89458\n",
            "Epoch 20/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.2323 - accuracy: 0.9157 - val_loss: 0.3513 - val_accuracy: 0.8888\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.89458\n",
            "Epoch 21/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.2282 - accuracy: 0.9176 - val_loss: 0.3715 - val_accuracy: 0.8907\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.89458\n",
            "Epoch 22/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.2266 - accuracy: 0.9193 - val_loss: 0.3621 - val_accuracy: 0.8875\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.89458\n",
            "Epoch 23/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.2181 - accuracy: 0.9203 - val_loss: 0.3647 - val_accuracy: 0.8907\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.89458\n",
            "Epoch 24/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.2159 - accuracy: 0.9242 - val_loss: 0.3871 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.89458\n",
            "Epoch 25/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.2129 - accuracy: 0.9241 - val_loss: 0.3942 - val_accuracy: 0.8913\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.89458\n",
            "Epoch 26/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.2168 - accuracy: 0.9234 - val_loss: 0.3510 - val_accuracy: 0.8915\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.89458\n",
            "Epoch 27/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.2051 - accuracy: 0.9269 - val_loss: 0.3711 - val_accuracy: 0.8918\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.89458\n",
            "Epoch 28/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.2057 - accuracy: 0.9266 - val_loss: 0.3589 - val_accuracy: 0.8911\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.89458\n",
            "Epoch 29/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1979 - accuracy: 0.9293 - val_loss: 0.4132 - val_accuracy: 0.8903\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.89458\n",
            "Epoch 30/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.2022 - accuracy: 0.9269 - val_loss: 0.3829 - val_accuracy: 0.8925\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.89458\n",
            "Epoch 31/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1959 - accuracy: 0.9295 - val_loss: 0.3656 - val_accuracy: 0.8952\n",
            "\n",
            "Epoch 00031: val_accuracy improved from 0.89458 to 0.89517, saving model to best-DR_BN.h5\n",
            "Epoch 32/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1882 - accuracy: 0.9324 - val_loss: 0.4313 - val_accuracy: 0.8798\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.89517\n",
            "Epoch 33/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1925 - accuracy: 0.9300 - val_loss: 0.4128 - val_accuracy: 0.8882\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.89517\n",
            "Epoch 34/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1888 - accuracy: 0.9320 - val_loss: 0.4134 - val_accuracy: 0.8892\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.89517\n",
            "Epoch 35/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1870 - accuracy: 0.9316 - val_loss: 0.4084 - val_accuracy: 0.8971\n",
            "\n",
            "Epoch 00035: val_accuracy improved from 0.89517 to 0.89708, saving model to best-DR_BN.h5\n",
            "Epoch 36/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1849 - accuracy: 0.9334 - val_loss: 0.4156 - val_accuracy: 0.8903\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.89708\n",
            "Epoch 37/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1850 - accuracy: 0.9330 - val_loss: 0.4311 - val_accuracy: 0.8915\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.89708\n",
            "Epoch 38/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1746 - accuracy: 0.9385 - val_loss: 0.3856 - val_accuracy: 0.8947\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.89708\n",
            "Epoch 39/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1761 - accuracy: 0.9356 - val_loss: 0.3931 - val_accuracy: 0.8958\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.89708\n",
            "Epoch 40/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1783 - accuracy: 0.9366 - val_loss: 0.4577 - val_accuracy: 0.8844\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.89708\n",
            "Epoch 41/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1738 - accuracy: 0.9379 - val_loss: 0.4446 - val_accuracy: 0.8911\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.89708\n",
            "Epoch 42/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1764 - accuracy: 0.9365 - val_loss: 0.4649 - val_accuracy: 0.8917\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.89708\n",
            "Epoch 43/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1658 - accuracy: 0.9407 - val_loss: 0.3959 - val_accuracy: 0.8954\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.89708\n",
            "Epoch 44/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1706 - accuracy: 0.9401 - val_loss: 0.3791 - val_accuracy: 0.8947\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.89708\n",
            "Epoch 45/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1678 - accuracy: 0.9394 - val_loss: 0.4482 - val_accuracy: 0.8873\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.89708\n",
            "Epoch 46/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1621 - accuracy: 0.9423 - val_loss: 0.4816 - val_accuracy: 0.8854\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.89708\n",
            "Epoch 47/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1690 - accuracy: 0.9409 - val_loss: 0.4912 - val_accuracy: 0.8788\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.89708\n",
            "Epoch 48/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1658 - accuracy: 0.9405 - val_loss: 0.3991 - val_accuracy: 0.8971\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.89708\n",
            "Epoch 49/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1653 - accuracy: 0.9425 - val_loss: 0.4103 - val_accuracy: 0.8953\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.89708\n",
            "Epoch 50/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1567 - accuracy: 0.9427 - val_loss: 0.4144 - val_accuracy: 0.8960\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.89708\n",
            "Epoch 51/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1571 - accuracy: 0.9428 - val_loss: 0.4306 - val_accuracy: 0.8959\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.89708\n",
            "Epoch 52/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1566 - accuracy: 0.9443 - val_loss: 0.5123 - val_accuracy: 0.8944\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.89708\n",
            "Epoch 53/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1561 - accuracy: 0.9441 - val_loss: 0.4407 - val_accuracy: 0.8929\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.89708\n",
            "Epoch 54/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1534 - accuracy: 0.9462 - val_loss: 0.4827 - val_accuracy: 0.8980\n",
            "\n",
            "Epoch 00054: val_accuracy improved from 0.89708 to 0.89800, saving model to best-DR_BN.h5\n",
            "Epoch 55/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1520 - accuracy: 0.9475 - val_loss: 0.5057 - val_accuracy: 0.8950\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.89800\n",
            "Epoch 56/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1532 - accuracy: 0.9459 - val_loss: 0.4517 - val_accuracy: 0.8958\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.89800\n",
            "Epoch 57/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1524 - accuracy: 0.9465 - val_loss: 0.4561 - val_accuracy: 0.8944\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.89800\n",
            "Epoch 58/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1449 - accuracy: 0.9490 - val_loss: 0.4753 - val_accuracy: 0.8898\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.89800\n",
            "Epoch 59/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1469 - accuracy: 0.9499 - val_loss: 0.4919 - val_accuracy: 0.8928\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.89800\n",
            "Epoch 60/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1439 - accuracy: 0.9495 - val_loss: 0.4875 - val_accuracy: 0.8852\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.89800\n",
            "Epoch 61/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1450 - accuracy: 0.9495 - val_loss: 0.4484 - val_accuracy: 0.8943\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.89800\n",
            "Epoch 62/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1472 - accuracy: 0.9477 - val_loss: 0.4855 - val_accuracy: 0.8957\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.89800\n",
            "Epoch 63/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1424 - accuracy: 0.9508 - val_loss: 0.5507 - val_accuracy: 0.8913\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.89800\n",
            "Epoch 64/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1393 - accuracy: 0.9504 - val_loss: 0.4353 - val_accuracy: 0.8946\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.89800\n",
            "Epoch 65/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1373 - accuracy: 0.9529 - val_loss: 0.5047 - val_accuracy: 0.8950\n",
            "\n",
            "Epoch 00065: val_accuracy did not improve from 0.89800\n",
            "Epoch 66/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1384 - accuracy: 0.9511 - val_loss: 0.5120 - val_accuracy: 0.8957\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.89800\n",
            "Epoch 67/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1343 - accuracy: 0.9531 - val_loss: 0.4893 - val_accuracy: 0.8939\n",
            "\n",
            "Epoch 00067: val_accuracy did not improve from 0.89800\n",
            "Epoch 68/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1393 - accuracy: 0.9514 - val_loss: 0.5189 - val_accuracy: 0.8962\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.89800\n",
            "Epoch 69/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1372 - accuracy: 0.9525 - val_loss: 0.5037 - val_accuracy: 0.8878\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.89800\n",
            "Epoch 70/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1372 - accuracy: 0.9534 - val_loss: 0.4834 - val_accuracy: 0.8955\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.89800\n",
            "Epoch 71/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1317 - accuracy: 0.9549 - val_loss: 0.5750 - val_accuracy: 0.8866\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.89800\n",
            "Epoch 72/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1330 - accuracy: 0.9538 - val_loss: 0.4345 - val_accuracy: 0.8955\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.89800\n",
            "Epoch 73/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1235 - accuracy: 0.9562 - val_loss: 0.5267 - val_accuracy: 0.8988\n",
            "\n",
            "Epoch 00073: val_accuracy improved from 0.89800 to 0.89883, saving model to best-DR_BN.h5\n",
            "Epoch 74/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1245 - accuracy: 0.9566 - val_loss: 0.5321 - val_accuracy: 0.8950\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.89883\n",
            "Epoch 75/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1228 - accuracy: 0.9569 - val_loss: 0.5687 - val_accuracy: 0.8897\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.89883\n",
            "Epoch 76/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1284 - accuracy: 0.9573 - val_loss: 0.5584 - val_accuracy: 0.8968\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.89883\n",
            "Epoch 77/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1261 - accuracy: 0.9571 - val_loss: 0.5005 - val_accuracy: 0.8955\n",
            "\n",
            "Epoch 00077: val_accuracy did not improve from 0.89883\n",
            "Epoch 78/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1277 - accuracy: 0.9556 - val_loss: 0.5391 - val_accuracy: 0.8923\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.89883\n",
            "Epoch 79/500\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.1279 - accuracy: 0.9556 - val_loss: 0.5253 - val_accuracy: 0.8932\n",
            "\n",
            "Epoch 00079: val_accuracy did not improve from 0.89883\n",
            "Epoch 80/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1270 - accuracy: 0.9561 - val_loss: 0.4785 - val_accuracy: 0.8974\n",
            "\n",
            "Epoch 00080: val_accuracy did not improve from 0.89883\n",
            "Epoch 81/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1306 - accuracy: 0.9559 - val_loss: 0.5448 - val_accuracy: 0.8916\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.89883\n",
            "Epoch 82/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1202 - accuracy: 0.9578 - val_loss: 0.5812 - val_accuracy: 0.8942\n",
            "\n",
            "Epoch 00082: val_accuracy did not improve from 0.89883\n",
            "Epoch 83/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1196 - accuracy: 0.9593 - val_loss: 0.5780 - val_accuracy: 0.8967\n",
            "\n",
            "Epoch 00083: val_accuracy did not improve from 0.89883\n",
            "Epoch 84/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1249 - accuracy: 0.9557 - val_loss: 0.5208 - val_accuracy: 0.8931\n",
            "\n",
            "Epoch 00084: val_accuracy did not improve from 0.89883\n",
            "Epoch 85/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1221 - accuracy: 0.9587 - val_loss: 0.4970 - val_accuracy: 0.8979\n",
            "\n",
            "Epoch 00085: val_accuracy did not improve from 0.89883\n",
            "Epoch 86/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1167 - accuracy: 0.9598 - val_loss: 0.5543 - val_accuracy: 0.8923\n",
            "\n",
            "Epoch 00086: val_accuracy did not improve from 0.89883\n",
            "Epoch 87/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1125 - accuracy: 0.9621 - val_loss: 0.5236 - val_accuracy: 0.8983\n",
            "\n",
            "Epoch 00087: val_accuracy did not improve from 0.89883\n",
            "Epoch 88/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1175 - accuracy: 0.9599 - val_loss: 0.5426 - val_accuracy: 0.8946\n",
            "\n",
            "Epoch 00088: val_accuracy did not improve from 0.89883\n",
            "Epoch 89/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1136 - accuracy: 0.9616 - val_loss: 0.5340 - val_accuracy: 0.8953\n",
            "\n",
            "Epoch 00089: val_accuracy did not improve from 0.89883\n",
            "Epoch 90/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1201 - accuracy: 0.9602 - val_loss: 0.5051 - val_accuracy: 0.8933\n",
            "\n",
            "Epoch 00090: val_accuracy did not improve from 0.89883\n",
            "Epoch 91/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1143 - accuracy: 0.9612 - val_loss: 0.5242 - val_accuracy: 0.8964\n",
            "\n",
            "Epoch 00091: val_accuracy did not improve from 0.89883\n",
            "Epoch 92/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1199 - accuracy: 0.9605 - val_loss: 0.5901 - val_accuracy: 0.8964\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.89883\n",
            "Epoch 93/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1172 - accuracy: 0.9601 - val_loss: 0.5026 - val_accuracy: 0.8934\n",
            "\n",
            "Epoch 00093: val_accuracy did not improve from 0.89883\n",
            "Epoch 94/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1210 - accuracy: 0.9604 - val_loss: 0.5639 - val_accuracy: 0.8959\n",
            "\n",
            "Epoch 00094: val_accuracy did not improve from 0.89883\n",
            "Epoch 95/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1127 - accuracy: 0.9605 - val_loss: 0.5926 - val_accuracy: 0.8924\n",
            "\n",
            "Epoch 00095: val_accuracy did not improve from 0.89883\n",
            "Epoch 96/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1123 - accuracy: 0.9613 - val_loss: 0.5087 - val_accuracy: 0.8888\n",
            "\n",
            "Epoch 00096: val_accuracy did not improve from 0.89883\n",
            "Epoch 97/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1115 - accuracy: 0.9624 - val_loss: 0.5250 - val_accuracy: 0.8944\n",
            "\n",
            "Epoch 00097: val_accuracy did not improve from 0.89883\n",
            "Epoch 98/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1084 - accuracy: 0.9632 - val_loss: 0.5867 - val_accuracy: 0.8954\n",
            "\n",
            "Epoch 00098: val_accuracy did not improve from 0.89883\n",
            "Epoch 99/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1174 - accuracy: 0.9612 - val_loss: 0.5341 - val_accuracy: 0.8967\n",
            "\n",
            "Epoch 00099: val_accuracy did not improve from 0.89883\n",
            "Epoch 100/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1109 - accuracy: 0.9624 - val_loss: 0.5196 - val_accuracy: 0.8988\n",
            "\n",
            "Epoch 00100: val_accuracy did not improve from 0.89883\n",
            "Epoch 101/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1056 - accuracy: 0.9642 - val_loss: 0.6201 - val_accuracy: 0.8981\n",
            "\n",
            "Epoch 00101: val_accuracy did not improve from 0.89883\n",
            "Epoch 102/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1036 - accuracy: 0.9642 - val_loss: 0.5755 - val_accuracy: 0.8961\n",
            "\n",
            "Epoch 00102: val_accuracy did not improve from 0.89883\n",
            "Epoch 103/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1143 - accuracy: 0.9613 - val_loss: 0.6142 - val_accuracy: 0.8932\n",
            "\n",
            "Epoch 00103: val_accuracy did not improve from 0.89883\n",
            "Epoch 104/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1136 - accuracy: 0.9627 - val_loss: 0.4977 - val_accuracy: 0.8956\n",
            "\n",
            "Epoch 00104: val_accuracy did not improve from 0.89883\n",
            "Epoch 105/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1028 - accuracy: 0.9631 - val_loss: 0.5406 - val_accuracy: 0.8969\n",
            "\n",
            "Epoch 00105: val_accuracy did not improve from 0.89883\n",
            "Epoch 106/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1085 - accuracy: 0.9641 - val_loss: 0.5492 - val_accuracy: 0.8992\n",
            "\n",
            "Epoch 00106: val_accuracy improved from 0.89883 to 0.89917, saving model to best-DR_BN.h5\n",
            "Epoch 107/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1073 - accuracy: 0.9645 - val_loss: 0.6069 - val_accuracy: 0.8968\n",
            "\n",
            "Epoch 00107: val_accuracy did not improve from 0.89917\n",
            "Epoch 108/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1060 - accuracy: 0.9625 - val_loss: 0.6064 - val_accuracy: 0.8959\n",
            "\n",
            "Epoch 00108: val_accuracy did not improve from 0.89917\n",
            "Epoch 109/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1033 - accuracy: 0.9647 - val_loss: 0.5600 - val_accuracy: 0.8974\n",
            "\n",
            "Epoch 00109: val_accuracy did not improve from 0.89917\n",
            "Epoch 110/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1037 - accuracy: 0.9640 - val_loss: 0.6512 - val_accuracy: 0.8936\n",
            "\n",
            "Epoch 00110: val_accuracy did not improve from 0.89917\n",
            "Epoch 111/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1106 - accuracy: 0.9641 - val_loss: 0.5405 - val_accuracy: 0.8958\n",
            "\n",
            "Epoch 00111: val_accuracy did not improve from 0.89917\n",
            "Epoch 112/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0996 - accuracy: 0.9663 - val_loss: 0.6255 - val_accuracy: 0.8989\n",
            "\n",
            "Epoch 00112: val_accuracy did not improve from 0.89917\n",
            "Epoch 113/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0989 - accuracy: 0.9671 - val_loss: 0.5266 - val_accuracy: 0.8948\n",
            "\n",
            "Epoch 00113: val_accuracy did not improve from 0.89917\n",
            "Epoch 114/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1041 - accuracy: 0.9664 - val_loss: 0.6431 - val_accuracy: 0.8903\n",
            "\n",
            "Epoch 00114: val_accuracy did not improve from 0.89917\n",
            "Epoch 115/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0997 - accuracy: 0.9666 - val_loss: 0.5202 - val_accuracy: 0.8953\n",
            "\n",
            "Epoch 00115: val_accuracy did not improve from 0.89917\n",
            "Epoch 116/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1034 - accuracy: 0.9659 - val_loss: 0.5673 - val_accuracy: 0.8996\n",
            "\n",
            "Epoch 00116: val_accuracy improved from 0.89917 to 0.89958, saving model to best-DR_BN.h5\n",
            "Epoch 117/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0943 - accuracy: 0.9675 - val_loss: 0.6190 - val_accuracy: 0.8934\n",
            "\n",
            "Epoch 00117: val_accuracy did not improve from 0.89958\n",
            "Epoch 118/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0956 - accuracy: 0.9686 - val_loss: 0.5557 - val_accuracy: 0.8957\n",
            "\n",
            "Epoch 00118: val_accuracy did not improve from 0.89958\n",
            "Epoch 119/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1007 - accuracy: 0.9657 - val_loss: 0.6137 - val_accuracy: 0.8978\n",
            "\n",
            "Epoch 00119: val_accuracy did not improve from 0.89958\n",
            "Epoch 120/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0948 - accuracy: 0.9696 - val_loss: 0.6701 - val_accuracy: 0.8940\n",
            "\n",
            "Epoch 00120: val_accuracy did not improve from 0.89958\n",
            "Epoch 121/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0992 - accuracy: 0.9667 - val_loss: 0.5441 - val_accuracy: 0.8943\n",
            "\n",
            "Epoch 00121: val_accuracy did not improve from 0.89958\n",
            "Epoch 122/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0967 - accuracy: 0.9690 - val_loss: 0.5969 - val_accuracy: 0.8954\n",
            "\n",
            "Epoch 00122: val_accuracy did not improve from 0.89958\n",
            "Epoch 123/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0995 - accuracy: 0.9675 - val_loss: 0.6001 - val_accuracy: 0.8946\n",
            "\n",
            "Epoch 00123: val_accuracy did not improve from 0.89958\n",
            "Epoch 124/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0978 - accuracy: 0.9670 - val_loss: 0.6218 - val_accuracy: 0.8954\n",
            "\n",
            "Epoch 00124: val_accuracy did not improve from 0.89958\n",
            "Epoch 125/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0956 - accuracy: 0.9674 - val_loss: 0.5741 - val_accuracy: 0.8983\n",
            "\n",
            "Epoch 00125: val_accuracy did not improve from 0.89958\n",
            "Epoch 126/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0925 - accuracy: 0.9681 - val_loss: 0.5905 - val_accuracy: 0.8936\n",
            "\n",
            "Epoch 00126: val_accuracy did not improve from 0.89958\n",
            "Epoch 127/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0939 - accuracy: 0.9693 - val_loss: 0.5788 - val_accuracy: 0.8973\n",
            "\n",
            "Epoch 00127: val_accuracy did not improve from 0.89958\n",
            "Epoch 128/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0986 - accuracy: 0.9692 - val_loss: 0.5764 - val_accuracy: 0.8963\n",
            "\n",
            "Epoch 00128: val_accuracy did not improve from 0.89958\n",
            "Epoch 129/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0903 - accuracy: 0.9712 - val_loss: 0.7066 - val_accuracy: 0.8914\n",
            "\n",
            "Epoch 00129: val_accuracy did not improve from 0.89958\n",
            "Epoch 130/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0917 - accuracy: 0.9686 - val_loss: 0.7430 - val_accuracy: 0.8851\n",
            "\n",
            "Epoch 00130: val_accuracy did not improve from 0.89958\n",
            "Epoch 131/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0907 - accuracy: 0.9687 - val_loss: 0.7734 - val_accuracy: 0.8875\n",
            "\n",
            "Epoch 00131: val_accuracy did not improve from 0.89958\n",
            "Epoch 132/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0927 - accuracy: 0.9693 - val_loss: 0.6670 - val_accuracy: 0.8938\n",
            "\n",
            "Epoch 00132: val_accuracy did not improve from 0.89958\n",
            "Epoch 133/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0930 - accuracy: 0.9698 - val_loss: 0.5982 - val_accuracy: 0.8985\n",
            "\n",
            "Epoch 00133: val_accuracy did not improve from 0.89958\n",
            "Epoch 134/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0904 - accuracy: 0.9705 - val_loss: 0.6319 - val_accuracy: 0.8947\n",
            "\n",
            "Epoch 00134: val_accuracy did not improve from 0.89958\n",
            "Epoch 135/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0883 - accuracy: 0.9702 - val_loss: 0.6296 - val_accuracy: 0.9007\n",
            "\n",
            "Epoch 00135: val_accuracy improved from 0.89958 to 0.90067, saving model to best-DR_BN.h5\n",
            "Epoch 136/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0886 - accuracy: 0.9701 - val_loss: 0.6230 - val_accuracy: 0.8967\n",
            "\n",
            "Epoch 00136: val_accuracy did not improve from 0.90067\n",
            "Epoch 137/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0924 - accuracy: 0.9694 - val_loss: 0.6906 - val_accuracy: 0.8855\n",
            "\n",
            "Epoch 00137: val_accuracy did not improve from 0.90067\n",
            "Epoch 138/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0890 - accuracy: 0.9695 - val_loss: 0.6490 - val_accuracy: 0.8998\n",
            "\n",
            "Epoch 00138: val_accuracy did not improve from 0.90067\n",
            "Epoch 139/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0952 - accuracy: 0.9695 - val_loss: 0.6746 - val_accuracy: 0.8979\n",
            "\n",
            "Epoch 00139: val_accuracy did not improve from 0.90067\n",
            "Epoch 140/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0879 - accuracy: 0.9705 - val_loss: 0.6673 - val_accuracy: 0.8967\n",
            "\n",
            "Epoch 00140: val_accuracy did not improve from 0.90067\n",
            "Epoch 141/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0948 - accuracy: 0.9693 - val_loss: 0.5991 - val_accuracy: 0.8991\n",
            "\n",
            "Epoch 00141: val_accuracy did not improve from 0.90067\n",
            "Epoch 142/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0911 - accuracy: 0.9714 - val_loss: 0.6088 - val_accuracy: 0.8973\n",
            "\n",
            "Epoch 00142: val_accuracy did not improve from 0.90067\n",
            "Epoch 143/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0841 - accuracy: 0.9721 - val_loss: 0.7065 - val_accuracy: 0.8953\n",
            "\n",
            "Epoch 00143: val_accuracy did not improve from 0.90067\n",
            "Epoch 144/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0851 - accuracy: 0.9721 - val_loss: 0.6655 - val_accuracy: 0.8973\n",
            "\n",
            "Epoch 00144: val_accuracy did not improve from 0.90067\n",
            "Epoch 145/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0852 - accuracy: 0.9718 - val_loss: 0.7576 - val_accuracy: 0.8938\n",
            "\n",
            "Epoch 00145: val_accuracy did not improve from 0.90067\n",
            "Epoch 146/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0895 - accuracy: 0.9702 - val_loss: 0.6625 - val_accuracy: 0.8930\n",
            "\n",
            "Epoch 00146: val_accuracy did not improve from 0.90067\n",
            "Epoch 147/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0902 - accuracy: 0.9714 - val_loss: 0.6329 - val_accuracy: 0.8977\n",
            "\n",
            "Epoch 00147: val_accuracy did not improve from 0.90067\n",
            "Epoch 148/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0837 - accuracy: 0.9716 - val_loss: 0.6811 - val_accuracy: 0.8963\n",
            "\n",
            "Epoch 00148: val_accuracy did not improve from 0.90067\n",
            "Epoch 149/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0893 - accuracy: 0.9711 - val_loss: 0.6372 - val_accuracy: 0.8962\n",
            "\n",
            "Epoch 00149: val_accuracy did not improve from 0.90067\n",
            "Epoch 150/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0880 - accuracy: 0.9706 - val_loss: 0.6617 - val_accuracy: 0.9006\n",
            "\n",
            "Epoch 00150: val_accuracy did not improve from 0.90067\n",
            "Epoch 151/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0816 - accuracy: 0.9724 - val_loss: 0.6417 - val_accuracy: 0.8972\n",
            "\n",
            "Epoch 00151: val_accuracy did not improve from 0.90067\n",
            "Epoch 152/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0859 - accuracy: 0.9714 - val_loss: 0.6675 - val_accuracy: 0.8968\n",
            "\n",
            "Epoch 00152: val_accuracy did not improve from 0.90067\n",
            "Epoch 153/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0848 - accuracy: 0.9729 - val_loss: 0.6921 - val_accuracy: 0.8955\n",
            "\n",
            "Epoch 00153: val_accuracy did not improve from 0.90067\n",
            "Epoch 154/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0825 - accuracy: 0.9734 - val_loss: 0.7201 - val_accuracy: 0.8963\n",
            "\n",
            "Epoch 00154: val_accuracy did not improve from 0.90067\n",
            "Epoch 155/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0844 - accuracy: 0.9725 - val_loss: 0.6500 - val_accuracy: 0.8983\n",
            "\n",
            "Epoch 00155: val_accuracy did not improve from 0.90067\n",
            "Epoch 156/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0876 - accuracy: 0.9715 - val_loss: 0.7027 - val_accuracy: 0.9000\n",
            "\n",
            "Epoch 00156: val_accuracy did not improve from 0.90067\n",
            "Epoch 157/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0825 - accuracy: 0.9724 - val_loss: 0.6108 - val_accuracy: 0.8984\n",
            "\n",
            "Epoch 00157: val_accuracy did not improve from 0.90067\n",
            "Epoch 158/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0875 - accuracy: 0.9717 - val_loss: 0.6871 - val_accuracy: 0.8967\n",
            "\n",
            "Epoch 00158: val_accuracy did not improve from 0.90067\n",
            "Epoch 159/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0847 - accuracy: 0.9729 - val_loss: 0.6489 - val_accuracy: 0.8965\n",
            "\n",
            "Epoch 00159: val_accuracy did not improve from 0.90067\n",
            "Epoch 160/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0772 - accuracy: 0.9746 - val_loss: 0.6397 - val_accuracy: 0.8967\n",
            "\n",
            "Epoch 00160: val_accuracy did not improve from 0.90067\n",
            "Epoch 161/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0863 - accuracy: 0.9724 - val_loss: 0.6756 - val_accuracy: 0.8958\n",
            "\n",
            "Epoch 00161: val_accuracy did not improve from 0.90067\n",
            "Epoch 162/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0813 - accuracy: 0.9730 - val_loss: 0.6254 - val_accuracy: 0.9006\n",
            "\n",
            "Epoch 00162: val_accuracy did not improve from 0.90067\n",
            "Epoch 163/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0838 - accuracy: 0.9721 - val_loss: 0.7305 - val_accuracy: 0.8969\n",
            "\n",
            "Epoch 00163: val_accuracy did not improve from 0.90067\n",
            "Epoch 164/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0886 - accuracy: 0.9713 - val_loss: 0.6094 - val_accuracy: 0.8973\n",
            "\n",
            "Epoch 00164: val_accuracy did not improve from 0.90067\n",
            "Epoch 165/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0797 - accuracy: 0.9738 - val_loss: 0.7337 - val_accuracy: 0.8959\n",
            "\n",
            "Epoch 00165: val_accuracy did not improve from 0.90067\n",
            "Epoch 166/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0856 - accuracy: 0.9727 - val_loss: 0.6490 - val_accuracy: 0.8959\n",
            "\n",
            "Epoch 00166: val_accuracy did not improve from 0.90067\n",
            "Epoch 167/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0794 - accuracy: 0.9739 - val_loss: 0.6635 - val_accuracy: 0.8943\n",
            "\n",
            "Epoch 00167: val_accuracy did not improve from 0.90067\n",
            "Epoch 168/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0833 - accuracy: 0.9748 - val_loss: 0.6785 - val_accuracy: 0.9006\n",
            "\n",
            "Epoch 00168: val_accuracy did not improve from 0.90067\n",
            "Epoch 169/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0797 - accuracy: 0.9743 - val_loss: 0.6936 - val_accuracy: 0.8997\n",
            "\n",
            "Epoch 00169: val_accuracy did not improve from 0.90067\n",
            "Epoch 170/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0749 - accuracy: 0.9761 - val_loss: 0.6714 - val_accuracy: 0.8994\n",
            "\n",
            "Epoch 00170: val_accuracy did not improve from 0.90067\n",
            "Epoch 171/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0770 - accuracy: 0.9753 - val_loss: 0.8018 - val_accuracy: 0.8967\n",
            "\n",
            "Epoch 00171: val_accuracy did not improve from 0.90067\n",
            "Epoch 172/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0770 - accuracy: 0.9755 - val_loss: 0.6907 - val_accuracy: 0.8962\n",
            "\n",
            "Epoch 00172: val_accuracy did not improve from 0.90067\n",
            "Epoch 173/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0821 - accuracy: 0.9748 - val_loss: 0.6552 - val_accuracy: 0.8972\n",
            "\n",
            "Epoch 00173: val_accuracy did not improve from 0.90067\n",
            "Epoch 174/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0834 - accuracy: 0.9744 - val_loss: 0.6791 - val_accuracy: 0.8983\n",
            "\n",
            "Epoch 00174: val_accuracy did not improve from 0.90067\n",
            "Epoch 175/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0835 - accuracy: 0.9735 - val_loss: 0.6732 - val_accuracy: 0.8934\n",
            "\n",
            "Epoch 00175: val_accuracy did not improve from 0.90067\n",
            "Epoch 176/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0727 - accuracy: 0.9758 - val_loss: 0.6467 - val_accuracy: 0.8978\n",
            "\n",
            "Epoch 00176: val_accuracy did not improve from 0.90067\n",
            "Epoch 177/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0747 - accuracy: 0.9751 - val_loss: 0.7866 - val_accuracy: 0.8954\n",
            "\n",
            "Epoch 00177: val_accuracy did not improve from 0.90067\n",
            "Epoch 178/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0770 - accuracy: 0.9754 - val_loss: 0.6638 - val_accuracy: 0.8972\n",
            "\n",
            "Epoch 00178: val_accuracy did not improve from 0.90067\n",
            "Epoch 179/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0729 - accuracy: 0.9751 - val_loss: 0.7457 - val_accuracy: 0.8967\n",
            "\n",
            "Epoch 00179: val_accuracy did not improve from 0.90067\n",
            "Epoch 180/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0779 - accuracy: 0.9752 - val_loss: 0.7516 - val_accuracy: 0.8938\n",
            "\n",
            "Epoch 00180: val_accuracy did not improve from 0.90067\n",
            "Epoch 181/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0765 - accuracy: 0.9752 - val_loss: 0.6776 - val_accuracy: 0.8928\n",
            "\n",
            "Epoch 00181: val_accuracy did not improve from 0.90067\n",
            "Epoch 182/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0766 - accuracy: 0.9772 - val_loss: 0.6867 - val_accuracy: 0.8989\n",
            "\n",
            "Epoch 00182: val_accuracy did not improve from 0.90067\n",
            "Epoch 183/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0764 - accuracy: 0.9756 - val_loss: 0.7054 - val_accuracy: 0.8995\n",
            "\n",
            "Epoch 00183: val_accuracy did not improve from 0.90067\n",
            "Epoch 184/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0743 - accuracy: 0.9762 - val_loss: 0.7373 - val_accuracy: 0.8982\n",
            "\n",
            "Epoch 00184: val_accuracy did not improve from 0.90067\n",
            "Epoch 185/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0761 - accuracy: 0.9752 - val_loss: 0.6481 - val_accuracy: 0.9007\n",
            "\n",
            "Epoch 00185: val_accuracy improved from 0.90067 to 0.90075, saving model to best-DR_BN.h5\n",
            "Epoch 186/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0755 - accuracy: 0.9766 - val_loss: 0.7363 - val_accuracy: 0.8968\n",
            "\n",
            "Epoch 00186: val_accuracy did not improve from 0.90075\n",
            "Epoch 187/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0761 - accuracy: 0.9765 - val_loss: 0.6511 - val_accuracy: 0.8998\n",
            "\n",
            "Epoch 00187: val_accuracy did not improve from 0.90075\n",
            "Epoch 188/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0738 - accuracy: 0.9766 - val_loss: 0.7484 - val_accuracy: 0.8956\n",
            "\n",
            "Epoch 00188: val_accuracy did not improve from 0.90075\n",
            "Epoch 189/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0728 - accuracy: 0.9770 - val_loss: 0.7008 - val_accuracy: 0.8942\n",
            "\n",
            "Epoch 00189: val_accuracy did not improve from 0.90075\n",
            "Epoch 190/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0707 - accuracy: 0.9784 - val_loss: 0.7215 - val_accuracy: 0.8987\n",
            "\n",
            "Epoch 00190: val_accuracy did not improve from 0.90075\n",
            "Epoch 191/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0798 - accuracy: 0.9746 - val_loss: 0.7633 - val_accuracy: 0.8938\n",
            "\n",
            "Epoch 00191: val_accuracy did not improve from 0.90075\n",
            "Epoch 192/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0742 - accuracy: 0.9767 - val_loss: 0.7221 - val_accuracy: 0.8996\n",
            "\n",
            "Epoch 00192: val_accuracy did not improve from 0.90075\n",
            "Epoch 193/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0764 - accuracy: 0.9768 - val_loss: 0.7197 - val_accuracy: 0.8983\n",
            "\n",
            "Epoch 00193: val_accuracy did not improve from 0.90075\n",
            "Epoch 194/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0696 - accuracy: 0.9759 - val_loss: 0.7421 - val_accuracy: 0.8947\n",
            "\n",
            "Epoch 00194: val_accuracy did not improve from 0.90075\n",
            "Epoch 195/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0774 - accuracy: 0.9759 - val_loss: 0.6962 - val_accuracy: 0.8928\n",
            "\n",
            "Epoch 00195: val_accuracy did not improve from 0.90075\n",
            "Epoch 196/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0764 - accuracy: 0.9768 - val_loss: 0.7288 - val_accuracy: 0.8977\n",
            "\n",
            "Epoch 00196: val_accuracy did not improve from 0.90075\n",
            "Epoch 197/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0756 - accuracy: 0.9752 - val_loss: 0.7095 - val_accuracy: 0.8972\n",
            "\n",
            "Epoch 00197: val_accuracy did not improve from 0.90075\n",
            "Epoch 198/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0733 - accuracy: 0.9764 - val_loss: 0.7105 - val_accuracy: 0.8985\n",
            "\n",
            "Epoch 00198: val_accuracy did not improve from 0.90075\n",
            "Epoch 199/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0758 - accuracy: 0.9788 - val_loss: 0.7514 - val_accuracy: 0.8935\n",
            "\n",
            "Epoch 00199: val_accuracy did not improve from 0.90075\n",
            "Epoch 200/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0683 - accuracy: 0.9778 - val_loss: 0.7443 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00200: val_accuracy did not improve from 0.90075\n",
            "Epoch 201/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0745 - accuracy: 0.9762 - val_loss: 0.6697 - val_accuracy: 0.8984\n",
            "\n",
            "Epoch 00201: val_accuracy did not improve from 0.90075\n",
            "Epoch 202/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0713 - accuracy: 0.9770 - val_loss: 0.7175 - val_accuracy: 0.8973\n",
            "\n",
            "Epoch 00202: val_accuracy did not improve from 0.90075\n",
            "Epoch 203/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0734 - accuracy: 0.9765 - val_loss: 0.6266 - val_accuracy: 0.8984\n",
            "\n",
            "Epoch 00203: val_accuracy did not improve from 0.90075\n",
            "Epoch 204/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0718 - accuracy: 0.9775 - val_loss: 0.7177 - val_accuracy: 0.8983\n",
            "\n",
            "Epoch 00204: val_accuracy did not improve from 0.90075\n",
            "Epoch 205/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0717 - accuracy: 0.9765 - val_loss: 0.7701 - val_accuracy: 0.8957\n",
            "\n",
            "Epoch 00205: val_accuracy did not improve from 0.90075\n",
            "Epoch 206/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0689 - accuracy: 0.9782 - val_loss: 0.7164 - val_accuracy: 0.8979\n",
            "\n",
            "Epoch 00206: val_accuracy did not improve from 0.90075\n",
            "Epoch 207/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0728 - accuracy: 0.9773 - val_loss: 0.7165 - val_accuracy: 0.8938\n",
            "\n",
            "Epoch 00207: val_accuracy did not improve from 0.90075\n",
            "Epoch 208/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0684 - accuracy: 0.9786 - val_loss: 0.7645 - val_accuracy: 0.8969\n",
            "\n",
            "Epoch 00208: val_accuracy did not improve from 0.90075\n",
            "Epoch 209/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0739 - accuracy: 0.9773 - val_loss: 0.7748 - val_accuracy: 0.8947\n",
            "\n",
            "Epoch 00209: val_accuracy did not improve from 0.90075\n",
            "Epoch 210/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0720 - accuracy: 0.9772 - val_loss: 0.7505 - val_accuracy: 0.8968\n",
            "\n",
            "Epoch 00210: val_accuracy did not improve from 0.90075\n",
            "Epoch 211/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0663 - accuracy: 0.9792 - val_loss: 0.7132 - val_accuracy: 0.9005\n",
            "\n",
            "Epoch 00211: val_accuracy did not improve from 0.90075\n",
            "Epoch 212/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0666 - accuracy: 0.9793 - val_loss: 0.7426 - val_accuracy: 0.8957\n",
            "\n",
            "Epoch 00212: val_accuracy did not improve from 0.90075\n",
            "Epoch 213/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0687 - accuracy: 0.9780 - val_loss: 0.7059 - val_accuracy: 0.8923\n",
            "\n",
            "Epoch 00213: val_accuracy did not improve from 0.90075\n",
            "Epoch 214/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0700 - accuracy: 0.9781 - val_loss: 0.7874 - val_accuracy: 0.8993\n",
            "\n",
            "Epoch 00214: val_accuracy did not improve from 0.90075\n",
            "Epoch 215/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0727 - accuracy: 0.9789 - val_loss: 0.6773 - val_accuracy: 0.8967\n",
            "\n",
            "Epoch 00215: val_accuracy did not improve from 0.90075\n",
            "Epoch 216/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0687 - accuracy: 0.9787 - val_loss: 0.8160 - val_accuracy: 0.8957\n",
            "\n",
            "Epoch 00216: val_accuracy did not improve from 0.90075\n",
            "Epoch 217/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0720 - accuracy: 0.9797 - val_loss: 0.7533 - val_accuracy: 0.8962\n",
            "\n",
            "Epoch 00217: val_accuracy did not improve from 0.90075\n",
            "Epoch 218/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0665 - accuracy: 0.9790 - val_loss: 0.8640 - val_accuracy: 0.8967\n",
            "\n",
            "Epoch 00218: val_accuracy did not improve from 0.90075\n",
            "Epoch 219/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0637 - accuracy: 0.9790 - val_loss: 0.7355 - val_accuracy: 0.8978\n",
            "\n",
            "Epoch 00219: val_accuracy did not improve from 0.90075\n",
            "Epoch 220/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0692 - accuracy: 0.9790 - val_loss: 0.7436 - val_accuracy: 0.8942\n",
            "\n",
            "Epoch 00220: val_accuracy did not improve from 0.90075\n",
            "Epoch 221/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0660 - accuracy: 0.9781 - val_loss: 0.8399 - val_accuracy: 0.8980\n",
            "\n",
            "Epoch 00221: val_accuracy did not improve from 0.90075\n",
            "Epoch 222/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0662 - accuracy: 0.9796 - val_loss: 0.7611 - val_accuracy: 0.8943\n",
            "\n",
            "Epoch 00222: val_accuracy did not improve from 0.90075\n",
            "Epoch 223/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0661 - accuracy: 0.9800 - val_loss: 0.7967 - val_accuracy: 0.8971\n",
            "\n",
            "Epoch 00223: val_accuracy did not improve from 0.90075\n",
            "Epoch 224/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0679 - accuracy: 0.9795 - val_loss: 0.7644 - val_accuracy: 0.8950\n",
            "\n",
            "Epoch 00224: val_accuracy did not improve from 0.90075\n",
            "Epoch 225/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0658 - accuracy: 0.9789 - val_loss: 0.7675 - val_accuracy: 0.9006\n",
            "\n",
            "Epoch 00225: val_accuracy did not improve from 0.90075\n",
            "Epoch 226/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0655 - accuracy: 0.9796 - val_loss: 0.7868 - val_accuracy: 0.8935\n",
            "\n",
            "Epoch 00226: val_accuracy did not improve from 0.90075\n",
            "Epoch 227/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0684 - accuracy: 0.9783 - val_loss: 0.7955 - val_accuracy: 0.8972\n",
            "\n",
            "Epoch 00227: val_accuracy did not improve from 0.90075\n",
            "Epoch 228/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0682 - accuracy: 0.9788 - val_loss: 0.7031 - val_accuracy: 0.8986\n",
            "\n",
            "Epoch 00228: val_accuracy did not improve from 0.90075\n",
            "Epoch 229/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0711 - accuracy: 0.9783 - val_loss: 0.7256 - val_accuracy: 0.8968\n",
            "\n",
            "Epoch 00229: val_accuracy did not improve from 0.90075\n",
            "Epoch 230/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0727 - accuracy: 0.9785 - val_loss: 0.7731 - val_accuracy: 0.8962\n",
            "\n",
            "Epoch 00230: val_accuracy did not improve from 0.90075\n",
            "Epoch 231/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0657 - accuracy: 0.9799 - val_loss: 0.7755 - val_accuracy: 0.8983\n",
            "\n",
            "Epoch 00231: val_accuracy did not improve from 0.90075\n",
            "Epoch 232/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0650 - accuracy: 0.9790 - val_loss: 0.8567 - val_accuracy: 0.8967\n",
            "\n",
            "Epoch 00232: val_accuracy did not improve from 0.90075\n",
            "Epoch 233/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0641 - accuracy: 0.9803 - val_loss: 0.7691 - val_accuracy: 0.8931\n",
            "\n",
            "Epoch 00233: val_accuracy did not improve from 0.90075\n",
            "Epoch 234/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0643 - accuracy: 0.9797 - val_loss: 0.7724 - val_accuracy: 0.8957\n",
            "\n",
            "Epoch 00234: val_accuracy did not improve from 0.90075\n",
            "Epoch 235/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0626 - accuracy: 0.9804 - val_loss: 0.8090 - val_accuracy: 0.8971\n",
            "\n",
            "Epoch 00235: val_accuracy did not improve from 0.90075\n",
            "Epoch 236/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0630 - accuracy: 0.9799 - val_loss: 0.7954 - val_accuracy: 0.8978\n",
            "\n",
            "Epoch 00236: val_accuracy did not improve from 0.90075\n",
            "Epoch 237/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0679 - accuracy: 0.9786 - val_loss: 0.7718 - val_accuracy: 0.8978\n",
            "\n",
            "Epoch 00237: val_accuracy did not improve from 0.90075\n",
            "Epoch 238/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0635 - accuracy: 0.9811 - val_loss: 0.7792 - val_accuracy: 0.8992\n",
            "\n",
            "Epoch 00238: val_accuracy did not improve from 0.90075\n",
            "Epoch 239/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0657 - accuracy: 0.9796 - val_loss: 0.7819 - val_accuracy: 0.8947\n",
            "\n",
            "Epoch 00239: val_accuracy did not improve from 0.90075\n",
            "Epoch 240/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0680 - accuracy: 0.9800 - val_loss: 0.7878 - val_accuracy: 0.8958\n",
            "\n",
            "Epoch 00240: val_accuracy did not improve from 0.90075\n",
            "Epoch 241/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0640 - accuracy: 0.9808 - val_loss: 0.7993 - val_accuracy: 0.8982\n",
            "\n",
            "Epoch 00241: val_accuracy did not improve from 0.90075\n",
            "Epoch 242/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0644 - accuracy: 0.9811 - val_loss: 0.8140 - val_accuracy: 0.8969\n",
            "\n",
            "Epoch 00242: val_accuracy did not improve from 0.90075\n",
            "Epoch 243/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0608 - accuracy: 0.9809 - val_loss: 0.7651 - val_accuracy: 0.8978\n",
            "\n",
            "Epoch 00243: val_accuracy did not improve from 0.90075\n",
            "Epoch 244/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0641 - accuracy: 0.9802 - val_loss: 0.8460 - val_accuracy: 0.8961\n",
            "\n",
            "Epoch 00244: val_accuracy did not improve from 0.90075\n",
            "Epoch 245/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0594 - accuracy: 0.9808 - val_loss: 0.7995 - val_accuracy: 0.8976\n",
            "\n",
            "Epoch 00245: val_accuracy did not improve from 0.90075\n",
            "Epoch 246/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0608 - accuracy: 0.9810 - val_loss: 0.8522 - val_accuracy: 0.8979\n",
            "\n",
            "Epoch 00246: val_accuracy did not improve from 0.90075\n",
            "Epoch 247/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0661 - accuracy: 0.9800 - val_loss: 0.7850 - val_accuracy: 0.8962\n",
            "\n",
            "Epoch 00247: val_accuracy did not improve from 0.90075\n",
            "Epoch 248/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0630 - accuracy: 0.9795 - val_loss: 0.8035 - val_accuracy: 0.8979\n",
            "\n",
            "Epoch 00248: val_accuracy did not improve from 0.90075\n",
            "Epoch 249/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0679 - accuracy: 0.9792 - val_loss: 0.8055 - val_accuracy: 0.8980\n",
            "\n",
            "Epoch 00249: val_accuracy did not improve from 0.90075\n",
            "Epoch 250/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0632 - accuracy: 0.9815 - val_loss: 0.7750 - val_accuracy: 0.8997\n",
            "\n",
            "Epoch 00250: val_accuracy did not improve from 0.90075\n",
            "Epoch 251/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0605 - accuracy: 0.9810 - val_loss: 0.7667 - val_accuracy: 0.8987\n",
            "\n",
            "Epoch 00251: val_accuracy did not improve from 0.90075\n",
            "Epoch 252/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0604 - accuracy: 0.9811 - val_loss: 0.7607 - val_accuracy: 0.8986\n",
            "\n",
            "Epoch 00252: val_accuracy did not improve from 0.90075\n",
            "Epoch 253/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0679 - accuracy: 0.9809 - val_loss: 0.7571 - val_accuracy: 0.8986\n",
            "\n",
            "Epoch 00253: val_accuracy did not improve from 0.90075\n",
            "Epoch 254/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0602 - accuracy: 0.9815 - val_loss: 0.7968 - val_accuracy: 0.8982\n",
            "\n",
            "Epoch 00254: val_accuracy did not improve from 0.90075\n",
            "Epoch 255/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0625 - accuracy: 0.9812 - val_loss: 0.7800 - val_accuracy: 0.8977\n",
            "\n",
            "Epoch 00255: val_accuracy did not improve from 0.90075\n",
            "Epoch 256/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0559 - accuracy: 0.9821 - val_loss: 0.7480 - val_accuracy: 0.8979\n",
            "\n",
            "Epoch 00256: val_accuracy did not improve from 0.90075\n",
            "Epoch 257/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0604 - accuracy: 0.9818 - val_loss: 0.8223 - val_accuracy: 0.8967\n",
            "\n",
            "Epoch 00257: val_accuracy did not improve from 0.90075\n",
            "Epoch 258/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0616 - accuracy: 0.9814 - val_loss: 0.7758 - val_accuracy: 0.8978\n",
            "\n",
            "Epoch 00258: val_accuracy did not improve from 0.90075\n",
            "Epoch 259/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0575 - accuracy: 0.9817 - val_loss: 0.7815 - val_accuracy: 0.8982\n",
            "\n",
            "Epoch 00259: val_accuracy did not improve from 0.90075\n",
            "Epoch 260/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0619 - accuracy: 0.9827 - val_loss: 0.8243 - val_accuracy: 0.8940\n",
            "\n",
            "Epoch 00260: val_accuracy did not improve from 0.90075\n",
            "Epoch 261/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0603 - accuracy: 0.9813 - val_loss: 0.7447 - val_accuracy: 0.8968\n",
            "\n",
            "Epoch 00261: val_accuracy did not improve from 0.90075\n",
            "Epoch 262/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0620 - accuracy: 0.9806 - val_loss: 0.8438 - val_accuracy: 0.8964\n",
            "\n",
            "Epoch 00262: val_accuracy did not improve from 0.90075\n",
            "Epoch 263/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0625 - accuracy: 0.9809 - val_loss: 0.8323 - val_accuracy: 0.8928\n",
            "\n",
            "Epoch 00263: val_accuracy did not improve from 0.90075\n",
            "Epoch 264/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0588 - accuracy: 0.9820 - val_loss: 0.7748 - val_accuracy: 0.8989\n",
            "\n",
            "Epoch 00264: val_accuracy did not improve from 0.90075\n",
            "Epoch 265/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0616 - accuracy: 0.9817 - val_loss: 0.7589 - val_accuracy: 0.8978\n",
            "\n",
            "Epoch 00265: val_accuracy did not improve from 0.90075\n",
            "Epoch 266/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0590 - accuracy: 0.9814 - val_loss: 0.8601 - val_accuracy: 0.8997\n",
            "\n",
            "Epoch 00266: val_accuracy did not improve from 0.90075\n",
            "Epoch 267/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0611 - accuracy: 0.9811 - val_loss: 0.8257 - val_accuracy: 0.8987\n",
            "\n",
            "Epoch 00267: val_accuracy did not improve from 0.90075\n",
            "Epoch 268/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0636 - accuracy: 0.9810 - val_loss: 0.6984 - val_accuracy: 0.8965\n",
            "\n",
            "Epoch 00268: val_accuracy did not improve from 0.90075\n",
            "Epoch 269/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0619 - accuracy: 0.9815 - val_loss: 0.8494 - val_accuracy: 0.8927\n",
            "\n",
            "Epoch 00269: val_accuracy did not improve from 0.90075\n",
            "Epoch 270/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0630 - accuracy: 0.9812 - val_loss: 0.8037 - val_accuracy: 0.8996\n",
            "\n",
            "Epoch 00270: val_accuracy did not improve from 0.90075\n",
            "Epoch 271/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0588 - accuracy: 0.9816 - val_loss: 0.7999 - val_accuracy: 0.8995\n",
            "\n",
            "Epoch 00271: val_accuracy did not improve from 0.90075\n",
            "Epoch 272/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0596 - accuracy: 0.9821 - val_loss: 0.8117 - val_accuracy: 0.8967\n",
            "\n",
            "Epoch 00272: val_accuracy did not improve from 0.90075\n",
            "Epoch 273/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0576 - accuracy: 0.9819 - val_loss: 0.7558 - val_accuracy: 0.8968\n",
            "\n",
            "Epoch 00273: val_accuracy did not improve from 0.90075\n",
            "Epoch 274/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0605 - accuracy: 0.9822 - val_loss: 0.7828 - val_accuracy: 0.8988\n",
            "\n",
            "Epoch 00274: val_accuracy did not improve from 0.90075\n",
            "Epoch 275/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0618 - accuracy: 0.9821 - val_loss: 0.8102 - val_accuracy: 0.8955\n",
            "\n",
            "Epoch 00275: val_accuracy did not improve from 0.90075\n",
            "Epoch 276/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0582 - accuracy: 0.9811 - val_loss: 0.8281 - val_accuracy: 0.8972\n",
            "\n",
            "Epoch 00276: val_accuracy did not improve from 0.90075\n",
            "Epoch 277/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0633 - accuracy: 0.9804 - val_loss: 0.8062 - val_accuracy: 0.9000\n",
            "\n",
            "Epoch 00277: val_accuracy did not improve from 0.90075\n",
            "Epoch 278/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0580 - accuracy: 0.9830 - val_loss: 0.7450 - val_accuracy: 0.8980\n",
            "\n",
            "Epoch 00278: val_accuracy did not improve from 0.90075\n",
            "Epoch 279/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0627 - accuracy: 0.9808 - val_loss: 0.7786 - val_accuracy: 0.9008\n",
            "\n",
            "Epoch 00279: val_accuracy improved from 0.90075 to 0.90083, saving model to best-DR_BN.h5\n",
            "Epoch 280/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0578 - accuracy: 0.9823 - val_loss: 0.7922 - val_accuracy: 0.8960\n",
            "\n",
            "Epoch 00280: val_accuracy did not improve from 0.90083\n",
            "Epoch 281/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0537 - accuracy: 0.9836 - val_loss: 0.8846 - val_accuracy: 0.8935\n",
            "\n",
            "Epoch 00281: val_accuracy did not improve from 0.90083\n",
            "Epoch 282/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0534 - accuracy: 0.9838 - val_loss: 0.8110 - val_accuracy: 0.8979\n",
            "\n",
            "Epoch 00282: val_accuracy did not improve from 0.90083\n",
            "Epoch 283/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0551 - accuracy: 0.9836 - val_loss: 0.8278 - val_accuracy: 0.8961\n",
            "\n",
            "Epoch 00283: val_accuracy did not improve from 0.90083\n",
            "Epoch 284/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0535 - accuracy: 0.9840 - val_loss: 0.7998 - val_accuracy: 0.8952\n",
            "\n",
            "Epoch 00284: val_accuracy did not improve from 0.90083\n",
            "Epoch 285/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0586 - accuracy: 0.9827 - val_loss: 0.8185 - val_accuracy: 0.8987\n",
            "\n",
            "Epoch 00285: val_accuracy did not improve from 0.90083\n",
            "Epoch 286/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0557 - accuracy: 0.9836 - val_loss: 0.8199 - val_accuracy: 0.8973\n",
            "\n",
            "Epoch 00286: val_accuracy did not improve from 0.90083\n",
            "Epoch 287/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0534 - accuracy: 0.9836 - val_loss: 0.8529 - val_accuracy: 0.8946\n",
            "\n",
            "Epoch 00287: val_accuracy did not improve from 0.90083\n",
            "Epoch 288/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0616 - accuracy: 0.9819 - val_loss: 0.8333 - val_accuracy: 0.9005\n",
            "\n",
            "Epoch 00288: val_accuracy did not improve from 0.90083\n",
            "Epoch 289/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0581 - accuracy: 0.9834 - val_loss: 0.8188 - val_accuracy: 0.8987\n",
            "\n",
            "Epoch 00289: val_accuracy did not improve from 0.90083\n",
            "Epoch 290/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0588 - accuracy: 0.9824 - val_loss: 0.8045 - val_accuracy: 0.8982\n",
            "\n",
            "Epoch 00290: val_accuracy did not improve from 0.90083\n",
            "Epoch 291/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0599 - accuracy: 0.9814 - val_loss: 0.7640 - val_accuracy: 0.8963\n",
            "\n",
            "Epoch 00291: val_accuracy did not improve from 0.90083\n",
            "Epoch 292/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0531 - accuracy: 0.9838 - val_loss: 0.8395 - val_accuracy: 0.8995\n",
            "\n",
            "Epoch 00292: val_accuracy did not improve from 0.90083\n",
            "Epoch 293/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0529 - accuracy: 0.9825 - val_loss: 0.7989 - val_accuracy: 0.8988\n",
            "\n",
            "Epoch 00293: val_accuracy did not improve from 0.90083\n",
            "Epoch 294/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0620 - accuracy: 0.9817 - val_loss: 0.7280 - val_accuracy: 0.8938\n",
            "\n",
            "Epoch 00294: val_accuracy did not improve from 0.90083\n",
            "Epoch 295/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0582 - accuracy: 0.9828 - val_loss: 0.7856 - val_accuracy: 0.8976\n",
            "\n",
            "Epoch 00295: val_accuracy did not improve from 0.90083\n",
            "Epoch 296/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0483 - accuracy: 0.9845 - val_loss: 0.7742 - val_accuracy: 0.8996\n",
            "\n",
            "Epoch 00296: val_accuracy did not improve from 0.90083\n",
            "Epoch 297/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0551 - accuracy: 0.9840 - val_loss: 0.8089 - val_accuracy: 0.8971\n",
            "\n",
            "Epoch 00297: val_accuracy did not improve from 0.90083\n",
            "Epoch 298/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0586 - accuracy: 0.9839 - val_loss: 0.8222 - val_accuracy: 0.8982\n",
            "\n",
            "Epoch 00298: val_accuracy did not improve from 0.90083\n",
            "Epoch 299/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0614 - accuracy: 0.9829 - val_loss: 0.8430 - val_accuracy: 0.8981\n",
            "\n",
            "Epoch 00299: val_accuracy did not improve from 0.90083\n",
            "Epoch 300/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0552 - accuracy: 0.9829 - val_loss: 0.8817 - val_accuracy: 0.8977\n",
            "\n",
            "Epoch 00300: val_accuracy did not improve from 0.90083\n",
            "Epoch 301/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0559 - accuracy: 0.9843 - val_loss: 0.7997 - val_accuracy: 0.8972\n",
            "\n",
            "Epoch 00301: val_accuracy did not improve from 0.90083\n",
            "Epoch 302/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0534 - accuracy: 0.9835 - val_loss: 0.8288 - val_accuracy: 0.8981\n",
            "\n",
            "Epoch 00302: val_accuracy did not improve from 0.90083\n",
            "Epoch 303/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0542 - accuracy: 0.9846 - val_loss: 0.8294 - val_accuracy: 0.8957\n",
            "\n",
            "Epoch 00303: val_accuracy did not improve from 0.90083\n",
            "Epoch 304/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0531 - accuracy: 0.9847 - val_loss: 0.7981 - val_accuracy: 0.8933\n",
            "\n",
            "Epoch 00304: val_accuracy did not improve from 0.90083\n",
            "Epoch 305/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0566 - accuracy: 0.9814 - val_loss: 0.8598 - val_accuracy: 0.8927\n",
            "\n",
            "Epoch 00305: val_accuracy did not improve from 0.90083\n",
            "Epoch 306/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0593 - accuracy: 0.9828 - val_loss: 0.8493 - val_accuracy: 0.8978\n",
            "\n",
            "Epoch 00306: val_accuracy did not improve from 0.90083\n",
            "Epoch 307/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0563 - accuracy: 0.9829 - val_loss: 0.7613 - val_accuracy: 0.8987\n",
            "\n",
            "Epoch 00307: val_accuracy did not improve from 0.90083\n",
            "Epoch 308/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0572 - accuracy: 0.9833 - val_loss: 0.7809 - val_accuracy: 0.8973\n",
            "\n",
            "Epoch 00308: val_accuracy did not improve from 0.90083\n",
            "Epoch 309/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0563 - accuracy: 0.9832 - val_loss: 0.8004 - val_accuracy: 0.8941\n",
            "\n",
            "Epoch 00309: val_accuracy did not improve from 0.90083\n",
            "Epoch 310/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0537 - accuracy: 0.9842 - val_loss: 0.9079 - val_accuracy: 0.8956\n",
            "\n",
            "Epoch 00310: val_accuracy did not improve from 0.90083\n",
            "Epoch 311/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0536 - accuracy: 0.9829 - val_loss: 0.8846 - val_accuracy: 0.8969\n",
            "\n",
            "Epoch 00311: val_accuracy did not improve from 0.90083\n",
            "Epoch 312/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0530 - accuracy: 0.9841 - val_loss: 0.8020 - val_accuracy: 0.8982\n",
            "\n",
            "Epoch 00312: val_accuracy did not improve from 0.90083\n",
            "Epoch 313/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0565 - accuracy: 0.9827 - val_loss: 0.8196 - val_accuracy: 0.8957\n",
            "\n",
            "Epoch 00313: val_accuracy did not improve from 0.90083\n",
            "Epoch 314/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0573 - accuracy: 0.9833 - val_loss: 0.7888 - val_accuracy: 0.8968\n",
            "\n",
            "Epoch 00314: val_accuracy did not improve from 0.90083\n",
            "Epoch 315/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0598 - accuracy: 0.9826 - val_loss: 0.7851 - val_accuracy: 0.8963\n",
            "\n",
            "Epoch 00315: val_accuracy did not improve from 0.90083\n",
            "Epoch 316/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0593 - accuracy: 0.9831 - val_loss: 0.7954 - val_accuracy: 0.8967\n",
            "\n",
            "Epoch 00316: val_accuracy did not improve from 0.90083\n",
            "Epoch 317/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0581 - accuracy: 0.9829 - val_loss: 0.8018 - val_accuracy: 0.8950\n",
            "\n",
            "Epoch 00317: val_accuracy did not improve from 0.90083\n",
            "Epoch 318/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0515 - accuracy: 0.9851 - val_loss: 0.8767 - val_accuracy: 0.8976\n",
            "\n",
            "Epoch 00318: val_accuracy did not improve from 0.90083\n",
            "Epoch 319/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0553 - accuracy: 0.9841 - val_loss: 0.8756 - val_accuracy: 0.8987\n",
            "\n",
            "Epoch 00319: val_accuracy did not improve from 0.90083\n",
            "Epoch 320/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0573 - accuracy: 0.9836 - val_loss: 0.9033 - val_accuracy: 0.8974\n",
            "\n",
            "Epoch 00320: val_accuracy did not improve from 0.90083\n",
            "Epoch 321/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0538 - accuracy: 0.9839 - val_loss: 0.8720 - val_accuracy: 0.8951\n",
            "\n",
            "Epoch 00321: val_accuracy did not improve from 0.90083\n",
            "Epoch 322/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0525 - accuracy: 0.9839 - val_loss: 0.8607 - val_accuracy: 0.8972\n",
            "\n",
            "Epoch 00322: val_accuracy did not improve from 0.90083\n",
            "Epoch 323/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0541 - accuracy: 0.9839 - val_loss: 0.7841 - val_accuracy: 0.8956\n",
            "\n",
            "Epoch 00323: val_accuracy did not improve from 0.90083\n",
            "Epoch 324/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0561 - accuracy: 0.9839 - val_loss: 0.8074 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00324: val_accuracy did not improve from 0.90083\n",
            "Epoch 325/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0541 - accuracy: 0.9839 - val_loss: 0.7781 - val_accuracy: 0.9013\n",
            "\n",
            "Epoch 00325: val_accuracy improved from 0.90083 to 0.90133, saving model to best-DR_BN.h5\n",
            "Epoch 326/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0541 - accuracy: 0.9836 - val_loss: 0.8995 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00326: val_accuracy did not improve from 0.90133\n",
            "Epoch 327/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0559 - accuracy: 0.9834 - val_loss: 0.8373 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00327: val_accuracy did not improve from 0.90133\n",
            "Epoch 328/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0531 - accuracy: 0.9847 - val_loss: 0.8671 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00328: val_accuracy did not improve from 0.90133\n",
            "Epoch 329/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0544 - accuracy: 0.9841 - val_loss: 0.7952 - val_accuracy: 0.9001\n",
            "\n",
            "Epoch 00329: val_accuracy did not improve from 0.90133\n",
            "Epoch 330/500\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 0.0560 - accuracy: 0.9839 - val_loss: 0.8761 - val_accuracy: 0.8994\n",
            "\n",
            "Epoch 00330: val_accuracy did not improve from 0.90133\n",
            "Epoch 331/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0554 - accuracy: 0.9844 - val_loss: 0.8158 - val_accuracy: 0.8991\n",
            "\n",
            "Epoch 00331: val_accuracy did not improve from 0.90133\n",
            "Epoch 332/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0539 - accuracy: 0.9853 - val_loss: 0.8054 - val_accuracy: 0.8984\n",
            "\n",
            "Epoch 00332: val_accuracy did not improve from 0.90133\n",
            "Epoch 333/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0518 - accuracy: 0.9848 - val_loss: 0.9224 - val_accuracy: 0.8954\n",
            "\n",
            "Epoch 00333: val_accuracy did not improve from 0.90133\n",
            "Epoch 334/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0552 - accuracy: 0.9835 - val_loss: 0.8393 - val_accuracy: 0.8986\n",
            "\n",
            "Epoch 00334: val_accuracy did not improve from 0.90133\n",
            "Epoch 335/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0512 - accuracy: 0.9851 - val_loss: 0.8087 - val_accuracy: 0.8961\n",
            "\n",
            "Epoch 00335: val_accuracy did not improve from 0.90133\n",
            "Epoch 336/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0506 - accuracy: 0.9838 - val_loss: 0.7772 - val_accuracy: 0.8964\n",
            "\n",
            "Epoch 00336: val_accuracy did not improve from 0.90133\n",
            "Epoch 337/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0476 - accuracy: 0.9851 - val_loss: 0.8213 - val_accuracy: 0.8994\n",
            "\n",
            "Epoch 00337: val_accuracy did not improve from 0.90133\n",
            "Epoch 338/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0506 - accuracy: 0.9848 - val_loss: 0.8509 - val_accuracy: 0.8982\n",
            "\n",
            "Epoch 00338: val_accuracy did not improve from 0.90133\n",
            "Epoch 339/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0517 - accuracy: 0.9843 - val_loss: 0.9294 - val_accuracy: 0.8971\n",
            "\n",
            "Epoch 00339: val_accuracy did not improve from 0.90133\n",
            "Epoch 340/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0506 - accuracy: 0.9845 - val_loss: 0.8240 - val_accuracy: 0.8949\n",
            "\n",
            "Epoch 00340: val_accuracy did not improve from 0.90133\n",
            "Epoch 341/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0576 - accuracy: 0.9842 - val_loss: 0.8761 - val_accuracy: 0.8979\n",
            "\n",
            "Epoch 00341: val_accuracy did not improve from 0.90133\n",
            "Epoch 342/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0540 - accuracy: 0.9841 - val_loss: 0.8783 - val_accuracy: 0.8994\n",
            "\n",
            "Epoch 00342: val_accuracy did not improve from 0.90133\n",
            "Epoch 343/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0523 - accuracy: 0.9845 - val_loss: 0.9031 - val_accuracy: 0.8977\n",
            "\n",
            "Epoch 00343: val_accuracy did not improve from 0.90133\n",
            "Epoch 344/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0538 - accuracy: 0.9840 - val_loss: 0.9491 - val_accuracy: 0.8983\n",
            "\n",
            "Epoch 00344: val_accuracy did not improve from 0.90133\n",
            "Epoch 345/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0501 - accuracy: 0.9846 - val_loss: 0.8647 - val_accuracy: 0.8964\n",
            "\n",
            "Epoch 00345: val_accuracy did not improve from 0.90133\n",
            "Epoch 346/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0503 - accuracy: 0.9851 - val_loss: 0.8623 - val_accuracy: 0.8988\n",
            "\n",
            "Epoch 00346: val_accuracy did not improve from 0.90133\n",
            "Epoch 347/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0539 - accuracy: 0.9840 - val_loss: 0.8734 - val_accuracy: 0.8988\n",
            "\n",
            "Epoch 00347: val_accuracy did not improve from 0.90133\n",
            "Epoch 348/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0563 - accuracy: 0.9845 - val_loss: 0.8423 - val_accuracy: 0.9011\n",
            "\n",
            "Epoch 00348: val_accuracy did not improve from 0.90133\n",
            "Epoch 349/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0465 - accuracy: 0.9856 - val_loss: 0.9224 - val_accuracy: 0.8954\n",
            "\n",
            "Epoch 00349: val_accuracy did not improve from 0.90133\n",
            "Epoch 350/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0509 - accuracy: 0.9848 - val_loss: 0.7816 - val_accuracy: 0.8997\n",
            "\n",
            "Epoch 00350: val_accuracy did not improve from 0.90133\n",
            "Epoch 351/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0523 - accuracy: 0.9845 - val_loss: 0.9160 - val_accuracy: 0.8978\n",
            "\n",
            "Epoch 00351: val_accuracy did not improve from 0.90133\n",
            "Epoch 352/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0461 - accuracy: 0.9858 - val_loss: 0.8316 - val_accuracy: 0.8996\n",
            "\n",
            "Epoch 00352: val_accuracy did not improve from 0.90133\n",
            "Epoch 353/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0466 - accuracy: 0.9858 - val_loss: 0.8948 - val_accuracy: 0.8984\n",
            "\n",
            "Epoch 00353: val_accuracy did not improve from 0.90133\n",
            "Epoch 354/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0504 - accuracy: 0.9853 - val_loss: 0.8550 - val_accuracy: 0.8943\n",
            "\n",
            "Epoch 00354: val_accuracy did not improve from 0.90133\n",
            "Epoch 355/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0511 - accuracy: 0.9851 - val_loss: 0.9175 - val_accuracy: 0.8958\n",
            "\n",
            "Epoch 00355: val_accuracy did not improve from 0.90133\n",
            "Epoch 356/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0520 - accuracy: 0.9843 - val_loss: 0.8510 - val_accuracy: 0.8993\n",
            "\n",
            "Epoch 00356: val_accuracy did not improve from 0.90133\n",
            "Epoch 357/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0510 - accuracy: 0.9855 - val_loss: 0.8263 - val_accuracy: 0.9007\n",
            "\n",
            "Epoch 00357: val_accuracy did not improve from 0.90133\n",
            "Epoch 358/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0493 - accuracy: 0.9861 - val_loss: 0.8943 - val_accuracy: 0.8996\n",
            "\n",
            "Epoch 00358: val_accuracy did not improve from 0.90133\n",
            "Epoch 359/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0541 - accuracy: 0.9838 - val_loss: 0.8554 - val_accuracy: 0.8984\n",
            "\n",
            "Epoch 00359: val_accuracy did not improve from 0.90133\n",
            "Epoch 360/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0520 - accuracy: 0.9847 - val_loss: 0.8073 - val_accuracy: 0.8950\n",
            "\n",
            "Epoch 00360: val_accuracy did not improve from 0.90133\n",
            "Epoch 361/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0527 - accuracy: 0.9851 - val_loss: 0.9350 - val_accuracy: 0.8970\n",
            "\n",
            "Epoch 00361: val_accuracy did not improve from 0.90133\n",
            "Epoch 362/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0483 - accuracy: 0.9858 - val_loss: 0.8917 - val_accuracy: 0.8976\n",
            "\n",
            "Epoch 00362: val_accuracy did not improve from 0.90133\n",
            "Epoch 363/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0523 - accuracy: 0.9844 - val_loss: 0.8887 - val_accuracy: 0.9012\n",
            "\n",
            "Epoch 00363: val_accuracy did not improve from 0.90133\n",
            "Epoch 364/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0468 - accuracy: 0.9867 - val_loss: 0.9065 - val_accuracy: 0.8997\n",
            "\n",
            "Epoch 00364: val_accuracy did not improve from 0.90133\n",
            "Epoch 365/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0499 - accuracy: 0.9861 - val_loss: 0.8758 - val_accuracy: 0.8984\n",
            "\n",
            "Epoch 00365: val_accuracy did not improve from 0.90133\n",
            "Epoch 366/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0493 - accuracy: 0.9856 - val_loss: 0.8748 - val_accuracy: 0.9007\n",
            "\n",
            "Epoch 00366: val_accuracy did not improve from 0.90133\n",
            "Epoch 367/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0501 - accuracy: 0.9850 - val_loss: 0.9192 - val_accuracy: 0.8994\n",
            "\n",
            "Epoch 00367: val_accuracy did not improve from 0.90133\n",
            "Epoch 368/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0526 - accuracy: 0.9838 - val_loss: 0.8219 - val_accuracy: 0.8977\n",
            "\n",
            "Epoch 00368: val_accuracy did not improve from 0.90133\n",
            "Epoch 369/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0466 - accuracy: 0.9856 - val_loss: 0.9109 - val_accuracy: 0.8937\n",
            "\n",
            "Epoch 00369: val_accuracy did not improve from 0.90133\n",
            "Epoch 370/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0525 - accuracy: 0.9848 - val_loss: 0.8332 - val_accuracy: 0.8991\n",
            "\n",
            "Epoch 00370: val_accuracy did not improve from 0.90133\n",
            "Epoch 371/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0443 - accuracy: 0.9865 - val_loss: 0.8874 - val_accuracy: 0.8995\n",
            "\n",
            "Epoch 00371: val_accuracy did not improve from 0.90133\n",
            "Epoch 372/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0442 - accuracy: 0.9865 - val_loss: 0.8055 - val_accuracy: 0.8999\n",
            "\n",
            "Epoch 00372: val_accuracy did not improve from 0.90133\n",
            "Epoch 373/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0485 - accuracy: 0.9852 - val_loss: 0.8913 - val_accuracy: 0.8992\n",
            "\n",
            "Epoch 00373: val_accuracy did not improve from 0.90133\n",
            "Epoch 374/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0535 - accuracy: 0.9853 - val_loss: 0.8677 - val_accuracy: 0.8955\n",
            "\n",
            "Epoch 00374: val_accuracy did not improve from 0.90133\n",
            "Epoch 375/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0471 - accuracy: 0.9860 - val_loss: 0.7784 - val_accuracy: 0.8965\n",
            "\n",
            "Epoch 00375: val_accuracy did not improve from 0.90133\n",
            "Epoch 376/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0483 - accuracy: 0.9861 - val_loss: 0.8613 - val_accuracy: 0.8980\n",
            "\n",
            "Epoch 00376: val_accuracy did not improve from 0.90133\n",
            "Epoch 377/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0476 - accuracy: 0.9867 - val_loss: 0.8868 - val_accuracy: 0.8981\n",
            "\n",
            "Epoch 00377: val_accuracy did not improve from 0.90133\n",
            "Epoch 378/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0488 - accuracy: 0.9856 - val_loss: 0.8598 - val_accuracy: 0.9006\n",
            "\n",
            "Epoch 00378: val_accuracy did not improve from 0.90133\n",
            "Epoch 379/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0502 - accuracy: 0.9851 - val_loss: 0.8847 - val_accuracy: 0.8976\n",
            "\n",
            "Epoch 00379: val_accuracy did not improve from 0.90133\n",
            "Epoch 380/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0482 - accuracy: 0.9856 - val_loss: 0.8882 - val_accuracy: 0.8981\n",
            "\n",
            "Epoch 00380: val_accuracy did not improve from 0.90133\n",
            "Epoch 381/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0473 - accuracy: 0.9856 - val_loss: 0.9297 - val_accuracy: 0.8970\n",
            "\n",
            "Epoch 00381: val_accuracy did not improve from 0.90133\n",
            "Epoch 382/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0479 - accuracy: 0.9859 - val_loss: 0.8662 - val_accuracy: 0.8991\n",
            "\n",
            "Epoch 00382: val_accuracy did not improve from 0.90133\n",
            "Epoch 383/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0492 - accuracy: 0.9861 - val_loss: 0.8498 - val_accuracy: 0.8995\n",
            "\n",
            "Epoch 00383: val_accuracy did not improve from 0.90133\n",
            "Epoch 384/500\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 0.0464 - accuracy: 0.9867 - val_loss: 0.8257 - val_accuracy: 0.9003\n",
            "\n",
            "Epoch 00384: val_accuracy did not improve from 0.90133\n",
            "Epoch 385/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0505 - accuracy: 0.9861 - val_loss: 0.8360 - val_accuracy: 0.8992\n",
            "\n",
            "Epoch 00385: val_accuracy did not improve from 0.90133\n",
            "Epoch 386/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0480 - accuracy: 0.9853 - val_loss: 0.8591 - val_accuracy: 0.8996\n",
            "\n",
            "Epoch 00386: val_accuracy did not improve from 0.90133\n",
            "Epoch 387/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0489 - accuracy: 0.9867 - val_loss: 0.9070 - val_accuracy: 0.8958\n",
            "\n",
            "Epoch 00387: val_accuracy did not improve from 0.90133\n",
            "Epoch 388/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0465 - accuracy: 0.9864 - val_loss: 0.7952 - val_accuracy: 0.9007\n",
            "\n",
            "Epoch 00388: val_accuracy did not improve from 0.90133\n",
            "Epoch 389/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0529 - accuracy: 0.9850 - val_loss: 0.8220 - val_accuracy: 0.9004\n",
            "\n",
            "Epoch 00389: val_accuracy did not improve from 0.90133\n",
            "Epoch 390/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0487 - accuracy: 0.9859 - val_loss: 0.9120 - val_accuracy: 0.8965\n",
            "\n",
            "Epoch 00390: val_accuracy did not improve from 0.90133\n",
            "Epoch 391/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0439 - accuracy: 0.9867 - val_loss: 0.9222 - val_accuracy: 0.8962\n",
            "\n",
            "Epoch 00391: val_accuracy did not improve from 0.90133\n",
            "Epoch 392/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0501 - accuracy: 0.9861 - val_loss: 0.8736 - val_accuracy: 0.8972\n",
            "\n",
            "Epoch 00392: val_accuracy did not improve from 0.90133\n",
            "Epoch 393/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0479 - accuracy: 0.9868 - val_loss: 0.8842 - val_accuracy: 0.8984\n",
            "\n",
            "Epoch 00393: val_accuracy did not improve from 0.90133\n",
            "Epoch 394/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0410 - accuracy: 0.9875 - val_loss: 0.8735 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00394: val_accuracy did not improve from 0.90133\n",
            "Epoch 395/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0446 - accuracy: 0.9871 - val_loss: 0.8844 - val_accuracy: 0.9002\n",
            "\n",
            "Epoch 00395: val_accuracy did not improve from 0.90133\n",
            "Epoch 396/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0464 - accuracy: 0.9857 - val_loss: 0.8757 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00396: val_accuracy did not improve from 0.90133\n",
            "Epoch 397/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0444 - accuracy: 0.9865 - val_loss: 0.8446 - val_accuracy: 0.8978\n",
            "\n",
            "Epoch 00397: val_accuracy did not improve from 0.90133\n",
            "Epoch 398/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0411 - accuracy: 0.9876 - val_loss: 0.8355 - val_accuracy: 0.9011\n",
            "\n",
            "Epoch 00398: val_accuracy did not improve from 0.90133\n",
            "Epoch 399/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0525 - accuracy: 0.9849 - val_loss: 0.9153 - val_accuracy: 0.8969\n",
            "\n",
            "Epoch 00399: val_accuracy did not improve from 0.90133\n",
            "Epoch 400/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0444 - accuracy: 0.9866 - val_loss: 0.8484 - val_accuracy: 0.8983\n",
            "\n",
            "Epoch 00400: val_accuracy did not improve from 0.90133\n",
            "Epoch 401/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0466 - accuracy: 0.9870 - val_loss: 0.8949 - val_accuracy: 0.8999\n",
            "\n",
            "Epoch 00401: val_accuracy did not improve from 0.90133\n",
            "Epoch 402/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0469 - accuracy: 0.9867 - val_loss: 0.9031 - val_accuracy: 0.8954\n",
            "\n",
            "Epoch 00402: val_accuracy did not improve from 0.90133\n",
            "Epoch 403/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0462 - accuracy: 0.9859 - val_loss: 0.8611 - val_accuracy: 0.8968\n",
            "\n",
            "Epoch 00403: val_accuracy did not improve from 0.90133\n",
            "Epoch 404/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0453 - accuracy: 0.9864 - val_loss: 0.9285 - val_accuracy: 0.8996\n",
            "\n",
            "Epoch 00404: val_accuracy did not improve from 0.90133\n",
            "Epoch 405/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0475 - accuracy: 0.9869 - val_loss: 0.7934 - val_accuracy: 0.8979\n",
            "\n",
            "Epoch 00405: val_accuracy did not improve from 0.90133\n",
            "Epoch 406/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0479 - accuracy: 0.9854 - val_loss: 0.8799 - val_accuracy: 0.8963\n",
            "\n",
            "Epoch 00406: val_accuracy did not improve from 0.90133\n",
            "Epoch 407/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0474 - accuracy: 0.9860 - val_loss: 0.8930 - val_accuracy: 0.8997\n",
            "\n",
            "Epoch 00407: val_accuracy did not improve from 0.90133\n",
            "Epoch 408/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0449 - accuracy: 0.9868 - val_loss: 0.8050 - val_accuracy: 0.9009\n",
            "\n",
            "Epoch 00408: val_accuracy did not improve from 0.90133\n",
            "Epoch 409/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0468 - accuracy: 0.9880 - val_loss: 0.8731 - val_accuracy: 0.8948\n",
            "\n",
            "Epoch 00409: val_accuracy did not improve from 0.90133\n",
            "Epoch 410/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0474 - accuracy: 0.9863 - val_loss: 0.9224 - val_accuracy: 0.8986\n",
            "\n",
            "Epoch 00410: val_accuracy did not improve from 0.90133\n",
            "Epoch 411/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0451 - accuracy: 0.9862 - val_loss: 0.8820 - val_accuracy: 0.8982\n",
            "\n",
            "Epoch 00411: val_accuracy did not improve from 0.90133\n",
            "Epoch 412/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0428 - accuracy: 0.9871 - val_loss: 0.8826 - val_accuracy: 0.8985\n",
            "\n",
            "Epoch 00412: val_accuracy did not improve from 0.90133\n",
            "Epoch 413/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0482 - accuracy: 0.9862 - val_loss: 0.9127 - val_accuracy: 0.8991\n",
            "\n",
            "Epoch 00413: val_accuracy did not improve from 0.90133\n",
            "Epoch 414/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0413 - accuracy: 0.9871 - val_loss: 0.9523 - val_accuracy: 0.8947\n",
            "\n",
            "Epoch 00414: val_accuracy did not improve from 0.90133\n",
            "Epoch 415/500\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 0.0431 - accuracy: 0.9868 - val_loss: 0.8594 - val_accuracy: 0.8982\n",
            "\n",
            "Epoch 00415: val_accuracy did not improve from 0.90133\n",
            "Epoch 416/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0451 - accuracy: 0.9870 - val_loss: 0.9793 - val_accuracy: 0.8984\n",
            "\n",
            "Epoch 00416: val_accuracy did not improve from 0.90133\n",
            "Epoch 417/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0501 - accuracy: 0.9868 - val_loss: 0.8919 - val_accuracy: 0.8969\n",
            "\n",
            "Epoch 00417: val_accuracy did not improve from 0.90133\n",
            "Epoch 418/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0435 - accuracy: 0.9874 - val_loss: 0.8634 - val_accuracy: 0.8991\n",
            "\n",
            "Epoch 00418: val_accuracy did not improve from 0.90133\n",
            "Epoch 419/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0488 - accuracy: 0.9863 - val_loss: 0.8441 - val_accuracy: 0.8967\n",
            "\n",
            "Epoch 00419: val_accuracy did not improve from 0.90133\n",
            "Epoch 420/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0451 - accuracy: 0.9867 - val_loss: 0.8952 - val_accuracy: 0.9003\n",
            "\n",
            "Epoch 00420: val_accuracy did not improve from 0.90133\n",
            "Epoch 421/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0488 - accuracy: 0.9857 - val_loss: 0.8695 - val_accuracy: 0.8973\n",
            "\n",
            "Epoch 00421: val_accuracy did not improve from 0.90133\n",
            "Epoch 422/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0447 - accuracy: 0.9874 - val_loss: 0.9060 - val_accuracy: 0.8997\n",
            "\n",
            "Epoch 00422: val_accuracy did not improve from 0.90133\n",
            "Epoch 423/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0443 - accuracy: 0.9876 - val_loss: 0.8319 - val_accuracy: 0.8978\n",
            "\n",
            "Epoch 00423: val_accuracy did not improve from 0.90133\n",
            "Epoch 424/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0435 - accuracy: 0.9880 - val_loss: 0.9214 - val_accuracy: 0.8984\n",
            "\n",
            "Epoch 00424: val_accuracy did not improve from 0.90133\n",
            "Epoch 425/500\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0451 - accuracy: 0.9869 - val_loss: 0.9096 - val_accuracy: 0.8998\n",
            "\n",
            "Epoch 00425: val_accuracy did not improve from 0.90133\n",
            "Epoch 00425: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCkWn3kU_D8P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "b2a46354-2aa1-4b76-9336-8cb894a7e17f"
      },
      "source": [
        "epochs = range(1, len(Hist_DR_BN.history['loss'])+1)\n",
        "\n",
        "plt.figure(figsize=(9,6))\n",
        "plt.plot(epochs, Hist_DR_BN.history['loss'])\n",
        "plt.plot(epochs, Hist_DR_BN.history['val_loss'])\n",
        "plt.legend(['Train loss','Test loss'])\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 648x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQz6UWU0_D8R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19f07f2e-2a66-4f2b-d8b8-e62d30123d87"
      },
      "source": [
        "loss_DR_BN, accuracy_DR_BN = DR_BN.evaluate(X_test, y_test)\n",
        "print('Loss = {:.5f}'.format(loss_DR_BN))\n",
        "print('Accuracy = {:.5f}'.format (accuracy_DR_BN))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.9164 - accuracy: 0.8959\n",
            "Loss = 0.91640\n",
            "Accuracy = 0.89590\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIo0PLU2Yz8x"
      },
      "source": [
        "# 비교"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yp0bWKaQYzti",
        "outputId": "b48c5b35-ab40-47f0-f129-10a9ef9f35d2"
      },
      "source": [
        "print('L2 Loss = {:.5f}, L2 Accuracy = {:.5f}'.format(loss_L2, accuracy_L2))\n",
        "print('L2 + BatchNormal Loss = {:.5f}, L2 + BatchNormal Accuracy = {:.5f}'.format(loss_L2_BN, accuracy_L2_BN ))\n",
        "print('Dropout + BatchNormal Loss = {:.5f}, Dropout + BatchNormal Accuracy = {:.5f}'.format(loss_DR_BN, accuracy_DR_BN))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "L2 Loss = 0.99603, L2 Accuracy = 0.88630\n",
            "L2 + BatchNormal Loss = 1.14954, L2 + BatchNormal Accuracy = 0.87810\n",
            "Dropout + BatchNormal Loss = 0.91640, Dropout + BatchNormal Accuracy = 0.89590\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}