{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_preprocessing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SavlRT_4M7-",
        "outputId": "870bf086-ea2a-4e7c-9570-c52375e1c2d8"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ve5v9SuR4UTk",
        "outputId": "23d9aa9f-3b73-4caf-a3f6-5bfe45ba2e1a"
      },
      "source": [
        "from nltk import sent_tokenize\n",
        "\n",
        "sentences = 'The X-Files is an American science fiction drama television series created by Chris Carter. The original television series aired from September 10, 1993 to May 19, 2002 on Fox. The program spanned nine seasons, with 202 episodes.'\n",
        "\n",
        "sent_tokenize(sentences)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The X-Files is an American science fiction drama television series created by Chris Carter.',\n",
              " 'The original television series aired from September 10, 1993 to May 19, 2002 on Fox.',\n",
              " 'The program spanned nine seasons, with 202 episodes.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hM5vGmHM4xFw",
        "outputId": "1479bbca-6754-4794-fc5e-b94d14c7b026"
      },
      "source": [
        "from nltk.tokenize import word_tokenize # 단어 토큰화\n",
        "\n",
        "text = 'The truth is out there'\n",
        "\n",
        "word_tokenize(text)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'truth', 'is', 'out', 'there']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRp6b2ol48nv",
        "outputId": "dd5485b0-9634-4b16-bea8-927fcb010614"
      },
      "source": [
        "from nltk.tag import pos_tag # 단어품사 태깅\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71FbwiaL5G6Y",
        "outputId": "c461cd04-521c-4c33-c510-31835ece094d"
      },
      "source": [
        "x = word_tokenize(text)\n",
        "pos_tag(x)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DT'), ('truth', 'NN'), ('is', 'VBZ'), ('out', 'RP'), ('there', 'RB')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6YEZh2t5Llg",
        "outputId": "10056cda-85d4-446e-a49e-35dd41942fc6"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDCoBTqZ5y7G",
        "outputId": "e26f0450-ba0c-460a-e935-7c1cc94a94da"
      },
      "source": [
        "print('English stop words :',len(nltk.corpus.stopwords.words('english')))\n",
        "print(nltk.corpus.stopwords.words('english')[:20])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English stop words : 179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10gke_fk53Tl"
      },
      "source": [
        "from nltk import word_tokenize, sent_tokenize\n",
        "def tokenize_text(doc):\n",
        "  sentences = sent_tokenize(doc)\n",
        "  word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
        "  return word_tokens"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0g_b-Rh6TUL",
        "outputId": "75d6a18c-60ce-424d-a31d-a668c0b68c84"
      },
      "source": [
        "word_tokens = tokenize_text(sentences)\n",
        "print(type(word_tokens),len(word_tokens))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'> 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbQrM7hZ6d3s",
        "outputId": "3e6478fb-ac89-40c1-c821-f92ec34717fe"
      },
      "source": [
        "print(word_tokens)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['The', 'X-Files', 'is', 'an', 'American', 'science', 'fiction', 'drama', 'television', 'series', 'created', 'by', 'Chris', 'Carter', '.'], ['The', 'original', 'television', 'series', 'aired', 'from', 'September', '10', ',', '1993', 'to', 'May', '19', ',', '2002', 'on', 'Fox', '.'], ['The', 'program', 'spanned', 'nine', 'seasons', ',', 'with', '202', 'episodes', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_rBytpm7mmx",
        "outputId": "0b579146-191e-4603-f17c-6e408e5210d9"
      },
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')  # corpus = 말뭉치topwords = nltk.corpus.stopwords.words('english')  # corpus = 말뭉치\n",
        "all_tokens = []\n",
        "\n",
        "for sentence in word_tokens:\n",
        "  filtered_words = []\n",
        "  print(sentence)\n",
        "  for word in sentence:\n",
        "    word = word.lower()\n",
        "    print(word)\n",
        "    if word not in stopwords:\n",
        "      filtered_words.append(word)\n",
        "  print('='*50)\n",
        "  all_tokens.append(filtered_words)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'X-Files', 'is', 'an', 'American', 'science', 'fiction', 'drama', 'television', 'series', 'created', 'by', 'Chris', 'Carter', '.']\n",
            "the\n",
            "x-files\n",
            "is\n",
            "an\n",
            "american\n",
            "science\n",
            "fiction\n",
            "drama\n",
            "television\n",
            "series\n",
            "created\n",
            "by\n",
            "chris\n",
            "carter\n",
            ".\n",
            "==================================================\n",
            "['The', 'original', 'television', 'series', 'aired', 'from', 'September', '10', ',', '1993', 'to', 'May', '19', ',', '2002', 'on', 'Fox', '.']\n",
            "the\n",
            "original\n",
            "television\n",
            "series\n",
            "aired\n",
            "from\n",
            "september\n",
            "10\n",
            ",\n",
            "1993\n",
            "to\n",
            "may\n",
            "19\n",
            ",\n",
            "2002\n",
            "on\n",
            "fox\n",
            ".\n",
            "==================================================\n",
            "['The', 'program', 'spanned', 'nine', 'seasons', ',', 'with', '202', 'episodes', '.']\n",
            "the\n",
            "program\n",
            "spanned\n",
            "nine\n",
            "seasons\n",
            ",\n",
            "with\n",
            "202\n",
            "episodes\n",
            ".\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOmRDqw8GGDw",
        "outputId": "11398a7e-9e12-43f5-b364-4d07b4ab0e06"
      },
      "source": [
        "print(all_tokens)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['x-files', 'american', 'science', 'fiction', 'drama', 'television', 'series', 'created', 'chris', 'carter', '.'], ['original', 'television', 'series', 'aired', 'september', '10', ',', '1993', 'may', '19', ',', '2002', 'fox', '.'], ['program', 'spanned', 'nine', 'seasons', ',', '202', 'episodes', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvW1jsUGG_Fc",
        "outputId": "574daaca-e8e1-4cd2-8852-877fa9d8fb2d"
      },
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "print(stemmer.stem('working'),stemmer.stem('works'), stemmer.stem('worked'))\n",
        "print(stemmer.stem('amusing'),stemmer.stem('amuses'),stemmer.stem('amused'))\n",
        "print(stemmer.stem('happier'),stemmer.stem('happiest'))\n",
        "print(stemmer.stem('fancier'),stemmer.stem('fanciest'))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "work work work\n",
            "amus amus amus\n",
            "happy happiest\n",
            "fant fanciest\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZy9yQ4PIWAC",
        "outputId": "120202b7-3539-4417-f779-dffb41cae661"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYB6BZFjIdek",
        "outputId": "158bab11-cfe2-4307-bb4a-b9fe99e09ee4"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "print(lemma.lemmatize('amusing','v'), lemma.lemmatize('amuses','v'), lemma.lemmatize('amused','v'))\n",
        "print(lemma.lemmatize('happier', 'a'), lemma.lemmatize('happiest', 'a'))\n",
        "print(lemma.lemmatize('fancier', 'a'), lemma.lemmatize('fanciest', 'a'))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "amuse amuse amuse\n",
            "happy happy\n",
            "fancy fancy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cNx9pMTIyBu",
        "outputId": "3a6f5a46-2f4e-4f78-fca7-6614824ffac1"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 1.3MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 8.4MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/a5/9781e2ef4ca92d09912c4794642c1653aea7607f473e156cf4d423a881a1/JPype1-1.2.1-cp37-cp37m-manylinux2010_x86_64.whl (457kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 37.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Installing collected packages: colorama, beautifulsoup4, JPype1, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.2.1 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjw1Pql1I1HU",
        "outputId": "1bc38df7-673e-451a-92fc-c49bb20f68d0"
      },
      "source": [
        "from konlpy.tag import Okt\n",
        "\n",
        "okt = Okt()\n",
        "print(okt.morphs('지난 몇 달간 전 세계 모든 사람은 코로나19로 인해 \\\n",
        "전례 없는 고통을 겪으며 다양한 방식으로 심각하게 피해를 겪었습니다.'))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['지난', '몇', '달', '간', '전', '세계', '모든', '사람', '은', '코로나', '19', '로', '인해', '전례', '없는', '고통', '을', '겪으며', '다양한', '방식', '으로', '심각하게', '피해', '를', '겪었습니다', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsc9B_KEI_6t",
        "outputId": "3d535b17-6200-45c8-86cb-ccd32ce8e727"
      },
      "source": [
        "print(okt.pos('지난 몇 달간 전 세계 모든 사람은 코로나19로 인해 \\\n",
        "전례 없는 고통을 겪으며 다양한 방식으로 심각하게 피해를 겪었습니다.'))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('지난', 'Noun'), ('몇', 'Noun'), ('달', 'Noun'), ('간', 'Suffix'), ('전', 'Noun'), ('세계', 'Noun'), ('모든', 'Noun'), ('사람', 'Noun'), ('은', 'Josa'), ('코로나', 'Noun'), ('19', 'Number'), ('로', 'Noun'), ('인해', 'Adjective'), ('전례', 'Noun'), ('없는', 'Adjective'), ('고통', 'Noun'), ('을', 'Josa'), ('겪으며', 'Verb'), ('다양한', 'Adjective'), ('방식', 'Noun'), ('으로', 'Josa'), ('심각하게', 'Adjective'), ('피해', 'Noun'), ('를', 'Josa'), ('겪었습니다', 'Verb'), ('.', 'Punctuation')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJ-5WTRiJFYc",
        "outputId": "c3415be8-36b2-463b-ef3e-84e64f3610cc"
      },
      "source": [
        "print(okt.nouns('지난 몇 달간 전 세계 모든 사람은 코로나19로 인해 \\\n",
        "전례 없는 고통을 겪으며 다양한 방식으로 심각하게 피해를 겪었습니다.'))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['지난', '몇', '달', '전', '세계', '모든', '사람', '코로나', '로', '전례', '고통', '방식', '피해']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82dK2hgMJLaV",
        "outputId": "b304ccb6-863d-4512-ab85-04d9ed45ab92"
      },
      "source": [
        "from konlpy.tag import Kkma\n",
        "\n",
        "kkma = Kkma()\n",
        "print(kkma.morphs('지난 몇 달간 전 세계 모든 사람은 코로나19로 인해 \\\n",
        "전례 없는 고통을 겪으며 다양한 방식으로 심각하게 피해를 겪었습니다.'))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['지나', 'ㄴ', '몇', '달', '간', '전', '세계', '모든', '사람', '은', '코로나', '19', '로', '인하', '어', '전례', '없', '는', '고통', '을', '겪', '으며', '다양', '하', 'ㄴ', '방식', '으로', '심각', '하', '게', '피해', '를', '겪', '었', '습니다', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2vcIKlQJQi2",
        "outputId": "27ff7724-f8a0-44b4-b89b-833fa139bb03"
      },
      "source": [
        "print(kkma.pos('지난 몇 달간 전 세계 모든 사람은 코로나19로 인해 \\\n",
        "전례 없는 고통을 겪으며 다양한 방식으로 심각하게 피해를 겪었습니다.'))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('지나', 'VV'), ('ㄴ', 'ETD'), ('몇', 'MDT'), ('달', 'NNG'), ('간', 'NNB'), ('전', 'NNG'), ('세계', 'NNG'), ('모든', 'MDT'), ('사람', 'NNG'), ('은', 'JX'), ('코로나', 'NNG'), ('19', 'NR'), ('로', 'JKM'), ('인하', 'VV'), ('어', 'ECS'), ('전례', 'NNG'), ('없', 'VA'), ('는', 'ETD'), ('고통', 'NNG'), ('을', 'JKO'), ('겪', 'VV'), ('으며', 'ECE'), ('다양', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('방식', 'NNG'), ('으로', 'JKM'), ('심각', 'XR'), ('하', 'XSA'), ('게', 'ECD'), ('피해', 'NNG'), ('를', 'JKO'), ('겪', 'VV'), ('었', 'EPT'), ('습니다', 'EFN'), ('.', 'SF')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9PgEplBJSEg",
        "outputId": "f0f4f306-af7b-4aec-d9d5-3c7e13553433"
      },
      "source": [
        "print(kkma.nouns('지난 몇 달간 전 세계 모든 사람은 코로나19로 인해 \\\n",
        "전례 없는 고통을 겪으며 다양한 방식으로 심각하게 피해를 겪었습니다.'))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['달', '달간', '간', '전', '세계', '사람', '코로나', '코로나19', '19', '전례', '고통', '다양', '방식', '피해']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmkpF0z3JfIY"
      },
      "source": [
        "# Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRTMpYlWJgho"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "sentence = '가지마라 가지마라 그녀는 위험해 매력이 너무 넘치는 Girl \\\n",
        "하지마라 하지마라 사랑은 위험해 \\\n",
        "내가 내가 내가 먼저 네게 네게 네게 빠져 빠져 빠져 버려 baby'"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B251mL7WJju0",
        "outputId": "9dbf812c-4097-4ff9-bfcb-453ca536d2c3"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tknz = Tokenizer()\n",
        "tknz.fit_on_texts([sentence])\n",
        "print(tknz.word_index)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'내가': 1, '네게': 2, '빠져': 3, '가지마라': 4, '위험해': 5, '하지마라': 6, '그녀는': 7, '매력이': 8, '너무': 9, '넘치는': 10, 'girl': 11, '사랑은': 12, '먼저': 13, '버려': 14, 'baby': 15}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPDl_n2fJtVI",
        "outputId": "305e97d4-5b89-4502-b16e-235de960c7f8"
      },
      "source": [
        "LBE = tknz.texts_to_sequences([sentence])\n",
        "print(LBE)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4, 4, 7, 5, 8, 9, 10, 11, 6, 6, 12, 5, 1, 1, 1, 13, 2, 2, 2, 3, 3, 3, 14, 15]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCKwKRxTKBnb",
        "outputId": "48356344-f7c4-4c3e-ebf1-4ce11224bad2"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "OHE = to_categorical(LBE)\n",
        "print(OHE)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOBqGXrHKC8u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}