{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ig1rdYbPRN4",
        "outputId": "3739a324-4867-444d-bfff-c36194a87e6b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yj1VWWa3P_NV",
        "outputId": "535180e8-5bba-48ca-c0d4-938373aeacec"
      },
      "source": [
        "!ls -l /content/drive/My\\ Drive/Colab\\ Notebooks/datasets/IMDB.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw------- 1 root root 60711700 Apr 28 11:33 '/content/drive/My Drive/Colab Notebooks/datasets/IMDB.zip'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGgzAM0PQEdb"
      },
      "source": [
        "!unzip /content/drive/My\\ Drive/Colab\\ Notebooks/datasets/IMDB.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsdAyNgcQg5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc18e0f9-5bde-4075-ac24-d387af75d6ab"
      },
      "source": [
        "import os\n",
        "imdb_dir = 'aclImdb'\n",
        "train_dir = os.path.join(imdb_dir, 'train')\n",
        "print('train_dir:',train_dir)\n",
        "labels=[]\n",
        "texts=[]\n",
        "\n",
        "for label_type in ['neg','pos']:\n",
        "  dir_name = os.path.join(train_dir, label_type)\n",
        "  print('label_type:',label_type)\n",
        "  print(dir_name)\n",
        "  for fname in os.listdir(dir_name):\n",
        "    if fname[-4:] == '.txt':\n",
        "      f = open(os.path.join(dir_name, fname), encoding='utf8')\n",
        "      texts.append(f.read())\n",
        "      print('texts:',texts)\n",
        "      f.close()\n",
        "      if label_type == 'neg':\n",
        "        labels.append(0)\n",
        "      else:\n",
        "        labels.append(1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykHPyY1HFZJ2"
      },
      "source": [
        "import os\n",
        "imdb_dir = 'aclImdb'\n",
        "train_dir = os.path.join(imdb_dir, 'train')\n",
        "print('train_dir:',train_dir)\n",
        "labels=[]\n",
        "texts=[]\n",
        "\n",
        "for label_type in ['neg','pos']:\n",
        "  dir_name = os.path.join(train_dir, label_type)\n",
        "  print('label_type:',label_type)\n",
        "  print(dir_name)\n",
        "  for fname in os.listdir(dir_name):\n",
        "    if fname[-4:] == '.txt':\n",
        "      f = open(os.path.join(dir_name, fname), encoding='utf8')\n",
        "      texts.append(f.read())\n",
        "      f.close()\n",
        "      if label_type == 'neg':\n",
        "        labels.append(0)\n",
        "      else:\n",
        "        labels.append(1)\n",
        "  print('fname:',fname)\n",
        "  print('texts:', texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_XRPb7JS74S"
      },
      "source": [
        "len(texts), len(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_BglrXuS9iK"
      },
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "maxlen = 2000\n",
        "max_words = 10000\n",
        "\n",
        "tokenizer = Tokenizer(num_words = max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('%s개의 고유한 토큰을 찾았습니다.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen = maxlen)\n",
        "labels = np.asarray(labels)\n",
        "print('데이터 텐서의 크기:', data.shape)\n",
        "print('레이블 텐서의 크기:', labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}